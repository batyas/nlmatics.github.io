
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about/",
    "title": "About",
    "body": "This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb. com You can find the source code for Minima at GitHub:jekyll /minima You can find the source code for Jekyll at GitHub:jekyll /jekyll "
    }, {
    "id": 2,
    "url": "http://localhost:4000/",
    "title": "",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/Assets/js/lunrsearchengine.js",
    "title": "",
    "body": "{% assign counter = 0 %}var documents = [{% for page in site. pages %}{% if page. url contains ‘. xml’ or page. url contains ‘assets’ or page. url contains ‘category’ or page. url contains ‘tag’ %}{% else %}{  “id”: {{ counter }},  “url”: “{{ site. url }}{{site. baseurl}}{{ page. url }}”,  “title”: “{{ page. title }}”,  “body”: “{{ page. content | markdownify | replace: ‘. ’, ‘. ‘ | replace: ‘&lt;/h2&gt;’, ‘: ‘ | replace: ‘&lt;/h3&gt;’, ‘: ‘ | replace: ‘&lt;/h4&gt;’, ‘: ‘ | replace: ‘&lt;/p&gt;’, ‘ ‘ | strip_html | strip_newlines | replace: ‘ ‘, ‘ ‘ | replace: ‘”’, ‘ ‘ }}”{% assign counter = counter | plus: 1 %}  }, {% endif %}{% endfor %}{% for page in site. without-plugin %}{  “id”: {{ counter }},  “url”: “{{ site. url }}{{site. baseurl}}{{ page. url }}”,  “title”: “{{ page. title }}”,  “body”: “{{ page. content | markdownify | replace: ‘. ’, ‘. ‘ | replace: ‘&lt;/h2&gt;’, ‘: ‘ | replace: ‘&lt;/h3&gt;’, ‘: ‘ | replace: ‘&lt;/h4&gt;’, ‘: ‘ | replace: ‘&lt;/p&gt;’, ‘ ‘ | strip_html | strip_newlines | replace: ‘ ‘, ‘ ‘ | replace: ‘”’, ‘ ‘ }}”{% assign counter = counter | plus: 1 %}  }, {% endfor %}{% for page in site. posts %}{  “id”: {{ counter }},  “url”: “{{ site. url }}{{site. baseurl}}{{ page. url }}”,  “title”: “{{ page. title }}”,  “body”: “{{ page. date | date: “%Y/%m/%d” }} - {{ page. content | markdownify | replace: ‘. ’, ‘. ‘ | replace: ‘&lt;/h2&gt;’, ‘: ‘ | replace: ‘&lt;/h3&gt;’, ‘: ‘ | replace: ‘&lt;/h4&gt;’, ‘: ‘ | replace: ‘&lt;/p&gt;’, ‘ ‘ | strip_html | strip_newlines | replace: ‘ ‘, ‘ ‘ | replace: ‘”’, ‘ ‘ }}”{% assign counter = counter | plus: 1 %}  }{% if forloop. last %}{% else %}, {% endif %}{% endfor %}]; var idx = lunr(function () {  this. ref(‘id’)  this. field(‘title’)  this. field(‘body’) documents. forEach(function (doc) {  this. add(doc)}, this) }); function lunr_search(term) {document. getElementById('lunrsearchresults'). innerHTML = '&lt;ul&gt;&lt;/ul&gt;';if(term) {  document. getElementById('lunrsearchresults'). innerHTML =  &lt;p&gt;Search results for '  + term +  '&lt;/p&gt;  + document. getElementById('lunrsearchresults'). innerHTML;  //put results on the screen.   var results = idx. search(term);  if(results. length&gt;0){    //console. log(idx. search(term));    //if results    for (var i = 0; i &lt; results. length; i++) {      // more statements      var ref = results[i]['ref'];      var url = documents[ref]['url'];      var title = documents[ref]['title'];      var body = documents[ref]['body']. substring(0,160)+'. . . ';      document. querySelectorAll('#lunrsearchresults ul')[0]. innerHTML = document. querySelectorAll('#lunrsearchresults ul')[0]. innerHTML +  &lt;li class='lunrsearchresult'&gt;&lt;a href='  + url +  '&gt;&lt;span class='title'&gt;  + title +  &lt;/span&gt;&lt;br /&gt;&lt;span class='body'&gt; + body + &lt;/span&gt;&lt;br /&gt;&lt;span class='url'&gt; + url + &lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ;    }  } else {    document. querySelectorAll('#lunrsearchresults ul')[0]. innerHTML =  &lt;li class='lunrsearchresult'&gt;No results found. . . &lt;/li&gt; ;  }}return false; }function lunr_search(term) {  $(‘#lunrsearchresults’). show( 400 );  $( “body” ). addClass( “modal-open” ); document. getElementById('lunrsearchresults'). innerHTML = '&lt;div id= resultsmodal  class= modal fade show d-block  tabindex= -1  role= dialog  aria-labelledby= resultsmodal &gt; &lt;div class= modal-dialog shadow-lg  role= document &gt; &lt;div class= modal-content &gt; &lt;div class= modal-header  id= modtit &gt; &lt;button type= button  class= close  id= btnx  data-dismiss= modal  aria-label= Close &gt; &amp;times; &lt;/button&gt; &lt;/div&gt; &lt;div class= modal-body &gt; &lt;ul class= mb-0 &gt; &lt;/ul&gt;  &lt;/div&gt; &lt;div class= modal-footer &gt;&lt;button id= btnx  type= button  class= btn btn-danger btn-sm  data-dismiss= modal &gt;Close&lt;/button&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;';if(term) {  document. getElementById('modtit'). innerHTML =  &lt;h5 class='modal-title'&gt;Search results for '  + term +  '&lt;/h5&gt;  + document. getElementById('modtit'). innerHTML;  //put results on the screen.   var results = idx. search(term);  if(results. length&gt;0){    //console. log(idx. search(term));    //if results    for (var i = 0; i &lt; results. length; i++) {      // more statements      var ref = results[i]['ref'];      var url = documents[ref]['url'];      var title = documents[ref]['title'];      var body = documents[ref]['body']. substring(0,160)+'. . . ';      document. querySelectorAll('#lunrsearchresults ul')[0]. innerHTML = document. querySelectorAll('#lunrsearchresults ul')[0]. innerHTML +  &lt;li class='lunrsearchresult'&gt;&lt;a href='  + url +  '&gt;&lt;span class='title'&gt;  + title +  &lt;/span&gt;&lt;br /&gt;&lt;small&gt;&lt;span class='body'&gt; + body + &lt;/span&gt;&lt;br /&gt;&lt;span class='url'&gt; + url + &lt;/span&gt;&lt;/small&gt;&lt;/a&gt;&lt;/li&gt; ;    }  } else {    document. querySelectorAll('#lunrsearchresults ul')[0]. innerHTML =  &lt;li class='lunrsearchresult'&gt;Sorry, no results found. Close &amp; try a different search!&lt;/li&gt; ;  }}return false; }$(function() {  $(“#lunrsearchresults”). on(‘click’, ‘#btnx’, function () {    $(‘#lunrsearchresults’). hide( 5 );    $( “body” ). removeClass( “modal-open” );  });}); "
    }, {
    "id": 4,
    "url": "http://localhost:4000/Assets/main.css",
    "title": "",
    "body": "@import “minima”; "
    }, {
    "id": 5,
    "url": "http://localhost:4000/Assets/css/main.css",
    "title": "",
    "body": "/* We need to add display:inline in order to align the ‘»’ of the ‘read more’ link */. post-excerpt p {	display:inline;} // Import partials from sass_dir (defaults to _sass)@import	“syntax”,  “starsnonscss”; "
    }, {
    "id": 6,
    "url": "http://localhost:4000/nlp/sentence-embeddings/2020/08/07/Smooth-Inverse-Frequency-Frequency-(SIF)-Embeddings-in-Golang.html",
    "title": "Smooth Inverse Frequency Frequency (SIF) Embeddings in Golang",
    "body": "2020/08/07 - Smooth Inverse Frequency (SIF) Embeddings in Golang Daniel Ye Table of Contents    Introduction     Motivation     Why GoLang?     GRPC Improvements     Post-Processing Improvements     Productionalizing With Docker     Conclusion  **1. Introduction ** I am a rising junior majoring in computer science and minoring in operations research and information engineering at Cornell Engineering. This summer, I interned at NLMatics, and one of the projects I worked on was implementing a Smooth Inverse Frequency model using Golang. This is able to calculate sentence embeddings from sequences of words in the form of vectors, which mathematically represent the meaning of the sentence. We use it to encode documents and queries into embeddings which are then processed further using other natural language processing models to get search results. However, our original Python implementation was fairly slow at calculating these embeddings, and it scaled poorly with increasing document sizes or concurrent requests, so we needed to find a way to speed up the service. Over the course of about four weeks, I worked on combating this issue by developing a Golang implementation of the model and switching from HTTP 1. 0 protocol to gRPC protocol for the server. This increased the amount of concurrent processing we were able to utilize, and reduced overhead for connecting and sending requests to the server, speeding up the service greatly. Ultimately, I was able to build a system that generated more accurate sentence embeddings at much faster speeds. 2. Motivation Word embeddings are one of the most important developments in the field of modern Natural Language Processing. Translating the meaning behind words and the semantic relationships between them into measurable quantities is a crucial step in processing language. Many words, such as “cat” and “dog” or “Mozart” and “Beethoven” have almost no physical characteristics that would reveal their similarities. Instead, modern algorithms like Google’s Word2Vec developed in 2013 or Stanford’s GloVe essentially count the cooccurrences of words with other words, and condense these values into dense, relatively low-dimensional vectors. Their models train on massive corpora of English text such as all of Wikipedia, and embed words as vectors based on which other words they appear in proximity to. So, if “cat” and “dog” are found together in many sentences or documents, they will have very similar vector values. This method is able to capture not only semantic similarity, but also analogies (woman is to man as king is to __) and the effects of prefixes or suffixes.  semantic relationships represented by Word2Vec and GloVe A natural next step in the field was the development of sentence embeddings, or being able to extract meaning from a sequence of words. Early methods include:    TF-ID**F **     **Paragram Phrase (PP**)     **Recurrent Neural Network (RNN**)     **Long Short-Term Memory Networks (LSTM**)     **Deep Averaging Network (DAN**)  In 2017, Arora et. al proposed SIF, or Smooth Inverse Frequency, a weighting scheme to improve performance of sentence embeddings. When encoding a sentence, it is important to identify which words in the sentence are more significant. For example, if calculating the embedding of the sentence “who was Mozart?” the word “was” doesn’t add much meaning; looking for sentences or documents relating to the word “was” will not yield any useful results for the original question. It’s clear that “Mozart” holds the most meaning in the question from a human standpoint, but how do you program a machine to identify that? SIF operates under the assumption that the most important words tend to also be used less frequently. If you counted all the words in Wikipedia, the word “was” would most likely appear much more frequently than in “Mozart”. Weights of a word w are computed by a/(a + p(w)) where a is a parameter and p(w) is the word frequency of w, which can be estimated by scraping a large corpus. * The hyperparameter *a adjusts which words are quantitatively “common” and “uncommon. ” Here is the formal algorithm: Arora et. al found that despite its simplicity, SIF worked surprisingly well on semantic text similarity (STS), entailment, and sentiment tasks. STS tasks involve scoring pairs of sentences from 0-5 based on how similar their meanings are, which are then checked against a golden standard of human generated scores. For example, “The bird is bathing in the sink” and “Birdie is washing itself in the water basin” should receive a 5. Entailment tasks involve identifying if one sentence entails that another one is true. For example, if you read the sentence “There is a soccer game with multiple males playing,” you could infer that the sentence “Several men are playing a sport” is true. Thus the first sentence, commonly referred to as the text entails the following, also known as the hypothesis.  Tables detailing SIF performance on various semantic tasks. “GloVe + WR”, “PSL + WR”, and “Ours” correspond to SIF systems. Due to its effectiveness and simplicity, SIF is an incredibly practical method of embedding sentences for commercial or enterprise products that rely on both accurate and fast results while consuming low amounts of resources. 2. Why Golang? The original code corresponding to the paper describing SIF is implemented in Python, which by design has a Global Interpreter Lock (GIL), a mutex that prevents multi threaded processes from utilizing multiple cores of a processor. We hosted our Cython implementation of the SIF embedding model on a cloud service, which provided us with multiple cores of processing power. However, the GIL meant that we could not make use of the full processing power we were paying for. GoLang however, has no such restrictions and also provides built-in structures for concurrency through the form of Goroutines, a form of very lightweight threads.  They are organized by channels, which allow goroutines to either block on awaiting input or signal that they have completed.    func main() {  jobs := make(chan int, 100)  results := make(chan int, 100)  for i := 0; i &lt; 4; i++ {    go worker(jobs, results)  }  for i := 0; i &lt; 100; i++ {    jobs &lt;- i  }  close(jobs)  for j := 0; j &lt; 100; j++ {    fmt. Println(&lt;-results)  }}func worker(jobs &lt;-chan int, results chan&lt;- int) {  for n := range jobs {    results &lt;- fib(n)  }}func fib(n int) int {  if n &lt;= 1 {    return n  }  return fib(n-1) + fib(n-2)} Basic architecture of a worker pool using goroutines It is important to note that this structure does not enforce the order in which workers complete their jobs or the order in which their results are sent to the results channel. Executing this code won’t necessarily return the fibonacci numbers in their correct order. Our API allowed for clients to send multiple sentences at a time to be encoded by the SIF, often many at a time, and implementing the SIF in Golang allowed us to leverage powerful concurrency to speed up our calculations. My first iteration of the SIF server used Golang’s http package to host the model on an HTTP 1. 0 server. I compared it with our old system, as well as the original Python implementation. I used three different independent variables to benchmark how the performance scaled up: total words per ‘sentence’ to be embedded, total number of calls with fixed number of concurrent requests, and number of concurrent requests with fixed total number of calls. These benchmarks were made using Apache Benchmark, a software that allows you to make many concurrent requests to a server and reports significant timing data.   The results were impressive, to say the least. At a sentence level, around 10-100 words, the new implementation outperforms the old by up to 30x and at a document level, around 1000-10000+ words, it is still 10x** **faster in almost every scenario. The improved speed for sentence level embeddings is really useful, since it means that we can embed at lower levels of granularity much more easily. For example, if you were searching “who was Mozart” and only had embeddings for each document, your system might flag a document about Joseph Haydn, friend and mentor of Mozart, as relevant. As you descend levels of granularity of embeddings however, you can much more easily locate relevant information about your query. Perhaps it allows you to find a paragraph detailing their time in Vienna together, or a specific sentence about the pieces of music they worked on together. I found that Go works incredibly well for creating a web service capable of handling many concurrent requests efficiently. However, Go’s build in functionality does have a significant limitation of restricting HTTP requests to sizes of 1MB or less, which was not ideal for our use cases where we had to embed large amounts of text in a single request. For example, a legal company looking to process Google’s 2019 environmental and sustainability report would need about 11 MB of payload. Or a stressed-out undergraduate trying to find tips in Cracking the Coding Interview would require around 90 MB of allowance. Additionally, every HTTP request requires a new connection to be made between the server and client, which adds a significant amount of overhead to our typical use case which often requires embedding many documents at once and sending many requests. 3. GRPC Improvements Developed by Google, gRPC is an open source Remote Procedure Call framework and is what I turned to in order to hopefully remove the size limit and connection overhead problems with Go’s http package. gRPC processes payloads from requests using buffers, so it removes the 1MB size cap on requests. It also maintains connections between individual clients, so a single user can make multiple requests without having to create a connection more than once. It has its own Interface Definition Language called protocol buffers that also serve as a mechanism for serializing structured data and uses HTTP/2 to transport data. gRPC services are defined in . proto files:    syntax =  proto3 option go_package = “greeter”package main// The greeter service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {}}// The request message containing the user's name. message HelloRequest { string name = 1;}// The response message containing the greetingsmessage HelloReply { string message = 1;} You can then use the protoc command to compile your service into any of gRPC’s supported languages.    // Code generated by protoc-gen-go. DO NOT EDIT. // versions:// protoc-gen-go v1. 23. 0// protoc    v3. 6. 1// source: greeter. protopackage greeterimport (  proto  github. com/golang/protobuf/proto   protoreflect  google. golang. org/protobuf/reflect/protoreflect   protoimpl  google. golang. org/protobuf/runtime/protoimpl   reflect  reflect   sync  sync )const (  // Verify that this generated code is sufficiently up-to-date.   _ = protoimpl. EnforceVersion(20 - protoimpl. MinVersion)  // Verify that runtime/protoimpl is sufficiently up-to-date.   _ = protoimpl. EnforceVersion(protoimpl. MaxVersion - 20))// This is a compile-time assertion that a sufficiently up-to-date version// of the legacy proto package is being used. const _ = proto. ProtoPackageIsVersion4// The request message containing the user's name. type HelloRequest struct {  state     protoimpl. MessageState  sizeCache   protoimpl. SizeCache  unknownFields protoimpl. UnknownFields  Name string `protobuf: bytes,1,opt,name=name,proto3  json: name,omitempty `}//. . . excess code has been left out Due to how gRPC buffers data being received and sent, we no longer had to worry about size limits on requests to our server. gRPC servers also have a very helpful feature in that connections between client and server are maintained across multiple requests, whereas with HTTP requests, a new connection has to be established for every POST. As a result, I saw more improvements in performance as this overhead was removed from the new system: The largest improvements are seen when requests have very small payloads, so the majority of time is spent on overhead from connecting to the server. However, once you get to larger payloads, the times converge to become about equal. **4. Post-Processing Improvements ** A coworker sent me this paper about post-processing word vectors in order to improve their representation of meaning. The algorithm essentially takes a list of pre-computed word vectors and performs principal component analysis on them, and then removes the top N components from every vector via Gram-Schmidt. Here is their formal algorithm: Python’s Numpy and sklearn packages have all the built-in tools needed to implement this algorithm, which you can find the code for here:    import numpy as npfrom sklearn. decomposition import PCAimport argparseparser = argparse. ArgumentParser(description='postprocess word embeddings')parser. add_argument( file , help= file containing embeddings to be processed )args = parser. parse_args()N = 2embedding_file = args. fileembs = []#map indexes of word vectors in matrix to their corresponding wordsidx_to_word = dict()dimension = 0#append each vector to a 2-D matrix and calculate average vectorwith open(embedding_file, 'rb') as f:  first_line = []  for line in f:     first_line = line. rstrip(). split()    dimension = len(first_line) - 1    if dimension &lt; 100 :      continue    print( dimension:  , dimension)        break  avg_vec = [0] * dimension  vocab_size = 0  word = str(first_line[0]. decode( utf-8 ))  word = word. split( _ )[0]  # print(word)  idx_to_word[vocab_size] = word  vec = [float(x) for x in first_line[1:]]  avg_vec = [vec[i] + avg_vec[i] for i in range(len(vec))]  vocab_size += 1  embs. append(vec)  for line in f:    line = line. rstrip(). split()    word = str(line[0]. decode( utf-8 ))    word = word. split( _ )[0]    idx_to_word[vocab_size] = word    vec = [float(x) for x in line[1:]]    avg_vec = [vec[i] + avg_vec[i] for i in range(len(vec))]    vocab_size += 1    embs. append(vec)  avg_vec = [x / vocab_size for x in avg_vec]# convert to numpy arrayembs = np. array(embs)#subtract average vector from each vectorfor i in range(len(embs)):  new_vec = [embs[i][j] - avg_vec[j] for j in range(len(avg_vec))]  embs[i] = np. array(new_vec)#principal component analysis using sklearnpca = PCA()pca. fit(embs)#remove the top N components from each vectorfor i in range(len(embs)):  preprocess_sum = [0] * dimension  for j in range(N):    princip = np. array(pca. components_[j])    preprocess = princip. dot(embs[i])    preprocess_vec = [princip[k] * preprocess for k in range(len(princip))]    preprocess_sum = [preprocess_sum[k] + preprocess_vec[k] for k in range(len(preprocess_sum))]  embs[i] = np. array([embs[i][j] - preprocess_sum[j] for j in range(len(preprocess_sum))])file = open( postprocessed_embeddings. txt ,  w+ , encoding= utf-8 )#write back new word vector fileidx = 0for vec in embs:  file. write(idx_to_word[idx])  file. write(   )  for num in vec:    file. write(str(num))    file. write(   )  file. write( \n )  idx+=1file. close()print( Wrote:  , len(embs),  word embeddings ) Using the new word embedding file, I saw meaningful improvements in semantic similarity tasks, similar to the results they found in their paper:    Dataset  k  Original  Post-Processed Vectors    MSR Paraphrase  k = 1  76. 43  78. 34    MS MARCO, 10000  k = 1  18. 2002  20. 4    MSR Paraphrase  k = 3  87. 122  88. 84    MS MARCO, 10000  k = 3  26. 915  29. 37    MSR Paraphrase  k = 5  89. 37944567  90. 92    MS MARCO, 10000  k = 5  33. 05634374  36. 2 I benchmarked the system on semantic text similarities tests using the MSR Paraphrase dataset, as well as the first 10000 entries of the MS MARCO dataset using p@k with k = 1, 3, and 5. These tasks involved identifying a sentence from a large pool of candidates as being semantically equivalent to another. p@k testing introduces some leeway into the testing, where the program chooses the top K candidates from the pool as potential matches instead of only getting one attempt at the correct answer. This kind of testing is more representative of a search engine, where you care not only about getting the single best result, but also a reasonable amount of relevant information. The post-processed vectors outperformed the original in every case. I also benchmarked our system using a word frequency file generated by fellow intern Connie Xu using a June snapshot of all of Wikipedia. It had a far more comprehensive vocabulary than our current word frequency file, with over 50x as many entries, but performance results were inconclusive. The results do indicate, however, that post-processed word vectors also increase performance of sentence embedding systems. 5. Productionalizing With Docker After finishing the code for our new SIF, my final step was to prepare it for production, which meant an inevitable encounter with Docker. Here is my Dockerfile, which I got from this tutorial:    # Start from the latest golang base imageFROM golang:latest# Add Maintainer InfoLABEL maintainer= Daniel Ye &lt;daniel. ye@nlmatics. com&gt; # Set the Current Working Directory inside the containerWORKDIR /app# Copy go mod and sum filesCOPY go. mod go. sum . /# Download all dependencies. Dependencies will be cached if the go. mod and go. sum files are not changedRUN go mod download# Copy the source from the current directory to the Working Directory inside the containerCOPY . . # Build the Go appRUN go build -o main . # Expose port 8080 to the outside worldEXPOSE 8080# Command to run the executableCMD [ . /main ] Most components are fairly self-explanatory. It does require that you use Go modules to manage your dependencies, which you can read about here. The steps I took to compile my code into a Docker image were as follows:    Create a go mod file in the root directory of your Golang project using go mod init . Your project name can be anything you want, it does not have to correspond to any file or package names, although it probably should.     Populate your go mod file with dependencies using go build. This will automatically detect the packages used by your project, and write them to your go mod file     Create a Dockerfile in the root directory of your project     Build your image using docker build -t . Include the period!     Run your project with docker run -d -p 8080:8080  6. Conclusion Improving the SIF implementation was a really interesting project. There were a lot of fun challenges involved like solving the ordering of goroutines and dealing with concurrent writes to map. It was incredibly satisfying to run my own benchmarks and see quantitative improvements in performance go as high as 30x the original speed. Of course, more improvements can still be made. This paper details how SIF embeddings of documents can be improved by producing and then combining topic vectors. Other models for embedding sentences such as Universal Sentence Encoder or Sentence-BERT have been developed in recent years as well and are able to outperform SIF in certain categories of NLP tasks. "
    }, {
    "id": 7,
    "url": "http://localhost:4000/category-1/category-2/2020/08/06/A-Comprehensive-Guide-to-Training-a-Machine-Learning-Model-for-Question-Answering_.html",
    "title": "A Comprehensive Guide to Training a Machine Learning Model for Question Answering",
    "body": "2020/08/06 -  A Comprehensive Guide to Training a Machine Learning Model for Question Answering: Fine-tuning ALBERT on Google Natural Questions : By Nicholas Greenspan and Batya Stein Contents::  Setting up the EC2 Instance     Choosing and launching EC2 Instance   Preparing instance for training   Setting up Wandb logging    Downloading and Formatting Data     About the datasets   Downloading the data   Processing from zip file   Reformatting Google Natural Questions to SQuAD format    Training the Model     Model comparisons   Understanding run_squad. py parameters   Common errors encountered during training   Restoring from a checkpoint   Understanding model outputs   Downloading your model and uploading to an S3 bucket   Intro:: Feel like a machine learning question answering system could give your business a boost? Just interested in trying out revolutionary AI tech? You’ve come to the right place. Training your first deep learning model can feel like navigating a labyrinth with no clear start or finish, so we’ve made a highly detailed guide to save you time, energy, and a lot of stackoverflow searches. This is a comprehensive guide to training a ML question answering model that will walk you through every step of the process from cloud computing to model training. We will use AWS for our cloud computing necessities, and Huggingface’s transformers repository to access our ALBERT model and run_squad. py script for training. Why do I care? What AI can do for you: Extracting information from long text documents can be a time consuming, monotonous endeavour. A revolutionary cutting edge technology, NLP Question Answering systems can automate this process, freeing you up to use that information to make decisions to further your business. Given a piece of text and a question, a Question Answering system can tell you where the answer lies in the text, and even tell you if the answer isn’t present at all! If this technology interests you, and you want to apply it at a business-level scale, you should definitely check out NLMatics’ product at https://www. nlmatics. com/.  We ran the model training described here during our summer engineering internship at NLMatics. Currently, one of the most popular Question Answering transformer models is trained on the SQuAD dataset, and we were curious to see if training on Google Natural Questions could give us more effective results. (As of this post, there are no ALBERT models pretrained on Google Natural Questions available for use online. ) Along the way, we learned a lot about the ins and outs of model training, memory management, and data-logging.  Huggingface’s transformers github repository, an open source resource for training language models, greatly simplifies the training process and puts many essential materials all in one place. Huggingface provides scripts for training models on different datasets, which saves you the labor of writing your own. In order to train on a new dataset while still making use of Huggingface’s run_squad. py script and functions for evaluating SQuAD training metrics, we converted the Google Natural Questions dataset into the same format as SQuAD. For this reason, this guide can help you whether you want to use Google Natural Questions, SQuAD, or any other question answering dataset to train a transformer model. To learn more about machine learning or language models, see the links below. \Machine Learning\Transformer architecture (BERT) To learn more about the datasets we worked with, see the links above or our blog post comparing the characteristics of both sets. PART 1: SETTING UP THE EC2 INSTANCE: Choosing And Launching An EC2 Instance:: To start our model training, we have to choose the right platform (one with a bit more computing power than a personal computer)                List of instances in EC2 dashboard   What is AWS EC2? Amazon Web Services’ EC2 service allows users to utilize cloud computing power from their local machines. An On-Demand EC2 Instance provides a virtual server that charges per hour that instance is running. Instance types and costs To choose an instance for your use case, consider its GPU and vCPU capacities, storage space, and cost. For our training, we considered P and G- type instances, since both classes come with GPU power. P2/P3 instances have up to 16 GPUs (P3 is the latest generation and comes at a slightly higher cost). G4 instances have up to 4 GPUs, but are more cost effective for larger amounts of memory. Note that if you’re using CUDA, a platform for computing with GPU power often used in machine learning applications, the maximum number of GPUs it will run on is 8, due to limitations in the “peer-to-peer” system. We chose the p2. 8xl instance, which has 8 NVIDIA K80 GPUs and 96GB of storage.       Instance   GPUs   GPU Type and total GPU memory   vCPUs   RAM   Linux Pricing (USD/HR)         p2. xlarge   1   NVIDIA K80 GPU (12GiB)   4   61 GiB   0. 9       p2. 8xlarge   8   NVIDIA K80 GPU(96GiB)   32   488 GiB   7. 2       p2. 16xlarge   16   NVIDIA K80 GPU (192GiB)   64   732 GiB   14. 4       p3. 2xlarge   1   NVIDIA Tesla V100 GPU (16GiB)   8   61   3. 06       p3. 8xlarge   4   NVIDIA Tesla V100 GPU (64GiB)   32   244   12. 24       p3. 16xlarge   8   NVIDIA Tesla V100 GPU (128GiB)   64   488   24. 48       g4dn. xlarge   1   NVIDIA T4 Tensor Core GPU (16GiB)   4   16   0. 526       g4dn. 8xlarge   1   NVIDIA T4 Tensor Core GPU (16GiB)   32   128   2. 176       g4dn. 12xlarge   4   NVIDIA T4 Tensor Core GPU (64GiB)   48   192   3. 912       g4dn. 16xlarge   1   NVIDIA T4 Tensor Core GPU (16GiB)   64   256   4. 352   Account limits Before launching an instance, be aware that each type has a specific vCPU capacity that it needs to run, which can be found in the chart above. However, all AWS accounts have automatic limitations on the number of vCPUs that can be allocated per instance class. Current account limits can be viewed in the EC2 section of the “Service Quotas” tab in the AWS console. Make sure you are in the same region you plan on starting your instance in, then check the “applied quota value” for “Running On-Demand P instances” (or instance class of your choice). If the current vCPU limit isn’t large enough to launch your instance, use the “Request Limit Increase” button to request the necessary number of vCPUs. In our experience, requests can take up to 48 hours to be filled, so request increases in advance of when you plan to start your training. Once the limit is increased, you can launch your instance from the AWS EC2 console. Choose a platform with deep learning capabilities - we needed CUDA and a PyTorch environment, so we chose the Deep Learning AMI (Amazon Linux) Version 30. 0. Next, choose instance type, and add additional storage if desired. Then configure the security group to allow from incoming connections from your IP address to the instance, by adding a rule of Type “SSH” with Source “My IP Address”, which automatically fills in your IP for you. You can also add additional rules to the security group later if needed to give others access to the instance. Do so by choosing yourinstance from the console instance list. Under its description, click its security group, then from the pulldown menu in the security groups list choose actions -&gt; edit inbound rules -&gt; add SSH rule, with IP of the person who needs access as the source. Key-pair security file To secure connections to your instance, create a key-pair file. In order to work, the key must not have public security permissions, so once you’ve downloaded the file, go to your terminal, navigate to the directory that the file is stored in, and run chmod 400 keypair-file-name. You can then connect to the instance from your terminal/ command-line by selecting the instance in the console list and running the example command given by the connect button, which will look like ssh -i”xxx. pem” EC2-user@EC2-xx-xx-xxx-xxx. us-region. compute. amazonaws. com. (When logging onto the server for the first time, you may get a warning that “The authenticity of host… can’t be established” but this should not be a security issue since you are instantiating the connection directly from AWS. ) IAM roles (sharing AWS resource access) If you want to give someone else access to the instance without giving them your account login information, you can create an IAM role in the IAM console. In the Users menu, add a User with AWS Management Console access, give user all EC2 permissions and then share the role’s login information. Once you start using your instance, you can monitor the costs or credits used, with the AWS cost explorer service. Make sure to stop the instance when not in use to avoid extra charges! Preparing The Instance For Training: Before we can get started with training, we’ll need to load all the necessary libraries and data onto our instance. Upload training data Prepare for training by connecting to your EC2 instance from the terminal of your choice and creating a directory inside it to store the data files in. (See section II, on downloading and reformatting data, to learn how to get your data into the right format before you start working with it. ) Upload your training and dev sets into the directory (in json format). Upload the training script, which can be found in the Transformers github repo, onto the instance. Lastly, create a new directory that will be used to store the evaluations and model checkpoints from the script’s output. File transferring To upload and transfer files between an EC2 instance and local computer you can use the following terminal commands. (Run these commands while the instance is running, but in a separate terminal window from where the instance is logged in. ) From local -&gt; EC2 – run the commandscp -i /directory/to/abc. pem /your/local/file/to/copy user@EC2-xx-xx-xxx-xxx. compute-1. amazonaws. com:path/to/file From EC2 -&gt; local (for downloading your model checkpoints, etc. ) runscp -i /directory/to/abc. pem user@EC2-xx-xx-xxx-xxx. compute-1. amazonaws. com:path/to/file /your/local/directory/files/to/download Another option is to use FileZilla software for transferring files. This post explains how to set up a connection to the EC2 instance through Filezilla with your pem key. Library installations Prepare your environment by downloading the libraries needed for the run_squad script. Run the following commands in the terminal of your choice, while logged into your instance: source activate env-name  This command activates a virtual environment. Choose one of the environments that come pre-loaded onto the instance, listed when you first log into the instance from your terminal. Since run_squad. py relies on PyTorch, we used the environment “pytorch_latest_p36”. PyTorch is a python machine learning library that can carry out accelerated tensor computations using GPU power. pip install transformers  Transformers is the Huggingface library that provides transformer model architectures that can be used with PyTorch or tensorflow. The transformers library also has processors for training on SQuAD data, which are used in the run_squad. py script. pip install wandb  Wandb is web app that allows for easy visualization of training progress and evaluation graphs, lets you see logging output from terminal, keeps track of different runs within you project, and allows you to share results with others (We’ll talk more about how to use Wandb to view your results in the next section!)pip install tensorboardX  The run_squad. py script uses the tensorboard library to graph evaluations that are run as training progresses. We’ll sync wandb with tensorboard so the graphs can be seen directly from the wandb dashboard. git clone https://github. com/NVIDIA/apexcd apexpip install -v --no-cache-dir . /  Apex is a pytorch extension that allows scripts to run with mixed-floating point precision (using 16 bit floats instead of 32 bit in order to decrease RAM usage)Setting Up Wandb Logging: You’re going to want to keep track of your training as it happens - here’s where we’ll set that up. Wandb (Weights &amp; Biases) provides a web-based platform for visualizing training loss and evaluation metrics as your training runs. Checking up on metrics periodically throughout training lets you visualize your progress, and course-correct if your loss suddenly swings upwards, or your metrics take a dramatic turn down. First you’ll want to create a free account on wandb’s website. Then, to connect your wandb account to your EC2 instance, go to the wandb website settings section and copy your api key. Log into your EC2 instance from your terminal and run the command wandb login your-api-key. Next, we’ll edit the run_squad. py script to integrate wandb logging. (You can edit it on your computer before uploading to the instance in the terminal editor of your choice after uploading it. )  Add import wandb to the library imports.  Before line 742 (sending model to cuda), add wandb. init(project= full_data_gnq_run , sync_tensorboard=True)). This will create a wandb project with the given name. Each time the script is rerun with the same project name will show up as a new run in the same project. Setting sync with tensorboard to true automatically logs the tensorboard graphs created during training to your project page.  In the next line, add wandb. config. update(args) to store all the arguments and model parameters inputted to the script On line 759, after sending model to device, add wandb. watch(model, log='all'), which saves the weights of the model to wandb as it trains. Now that you’ve added wandb statements into your script, you can watch your training progress from the wandb project dashboard. Two particularly helpful sections are the charts, where you can see loss and evaluation metrics, and the logs, where you can see the output of your script as it runs to track what percent complete your training is.  Wandb charts from our Google Natural Questions training PART 2: DOWNLOADING AND FORMATTING DATA: We’re going to download the data for the model to train on, and tweak it a bit to get it into the right format. (Note that a lot of the information in this section is about Google Natural Questions, but the same tips for handling and reformatting large datasets could apply to any dataset of your choice. ) About the Datasets: Both SQuAD and Google Natural Questions are datasets containing pairs of questions and answers based on Wikipedia articles. Fundamentally, they serve the same purpose - to teach a model reading comprehension by having it locate answers within a large amount of text. However, there are also some interesting differences between the two datasets that made us want to experiment with training a model on Google Natural Questions instead of SQuAD. To start, Google Natural questions has about twice the amount of training data as SQuAD does. SQuAD gives paragraphs from Wikipedia articles as context for its answers and has multiple questions per article, while Google Natural Questions gives entire Wikipedia articles for context, and only has one question per article. Additionally, all of SQuAD’s answers are short (about a sentence long), but Google Natural Questions has some questions with short answers, some with long answers, some with yes-no answers, and some with combinations of all of the above. Example of a SQuAD 2. 0 datapoint: Answer_start indicates the character count where the answer is located in the context. Some questions in SQuAD 2. 0 are flagged is_impossible, if answers to them cannot be found in the given context. To learn more about the differences between the datasets, their compositions and the decisions we made when reformatting, please see Nick’s SQuAD vs. Google Natural Questions blog post. Downloading the Data: Or, how to actually access 40GB worth of question-answer pairs for training Our goal was to train on the Google Natural Questions dataset using Huggingface’s run_squad. py script. To do so, we first had to convert the Google Natural Questions examples into SQuAD format. (This might seem like a convoluted approach but it’s actually one of the easiest ways to train with a new question answer dataset, since the Huggingface transformers library has functions specifically designed to process and evaluate SQuAD-formatted examples. ) To get the data for reformatting, you can either download it directly to your computer or you can upload it to an Amazon S3 bucket and stream it into your reformatting script from there. S3 buckets are an AWS service that allow you to store data in the cloud and share it with others. Downloading Google Natural Questions to your local computer If you have 40 gb of room on your local computer, you can download all the GNQ files using the google command line interface gsutil as described on the GNQ homepage. (We didn’t want to use the simplified train data that is directly linked to on the homepage because it didn’t give all the information we needed for our reformatting script. Alternatively, you can download the files directly within your reformatting script, as demonstrated in our code, by using the python request library to download the files from the urls of the google bucket where they are publicly stored. Streaming Google Natural Questions from an S3 Bucket If you don’t have enough storage to download the files to your local computer, you can use gsutil to upload the data to an Amazon S3 bucket and then stream the data into your python reformatting code from the bucket without needing to download it first. To do so, follow these steps:  Make an AWS S3 bucket to hold the data Install gustil and AWS CLI, which are command line interaces for Google and AWS, respectively Configure your AWS cli following these instructions Check that everything works by running aws s3 ls in your terminal, which displays the list of buckets associated with your AWS account Run gsutil -m cp -R gs://natural_questions/v1. 0 s3://your_bucket_name to download all data, or to download individual files replace gs://natural_questions/v1. 0 with gs://natural_questions/v1. 0/dev/nq-dev-00. jsonl. gz (there are 5 dev files altogether, so repeat this command through “dev-04”) and gs://natural_questions/v1. 0/train/nq-train-01. jsonl. gz (there are 50 train files altogether, so repeat the command through “train-49”) To stream the data from the s3 bucket in a python script, follow this tutorialDownloading SQuAD Data If you want to use the SQuAD dataset instead of Google Natural Questions to follow our tutorial, the data can be downloaded from https://rajpurkar. github. io/SQuAD-explorer/ using the links “Training Set v2. 0 (40MB)” and “Dev Set v2. 0 (4MB)” for the training and dev sets respectively on the left hand side bar. Processing Data from a Zip File: The Google Natural Questions training files take up 40 GB even when contained in gzip (similar to zip) format. When unzipped into json format, any one of the files is large enough to likely crash whatever program you try to open them with. Therefore, when processing Natural Questions or any similarly large dataset in python, it is easiest to do so straight from a zip file. To process a gzip in python, import gzip library and use a command such as with gzip. open('v1. 0_train_nq-train-00. jsonl. gz', 'rb') as f:	for line in f: 	  . . . To process a zip file, import zipfile library and use a command like with zipfile. ZipFile(“filename. zip”,”r”) as z:  with z. open(“unzippedfilename”) as f:  	  for line in f:  	    . . . Reformatting Google NQ Data to SQuAD Format: We wrote a python script that can be found here for reformatting the data on our local computers before uploading the data to our EC2 instance. Since the way boths datasets are formatted is so different, we had to make certain decisions about how to reformat Google Natural Questions, like using the long answers as context and the short answers as answers. (Entire Wikipedia articles took way too long to process. ) To hear more about the decisions we made when reformatting, see our code, linked above, and the SQuAD vs. Google Natural Questions blogpost. PART 3: TRAINING THE MODEL: Model Comparisons: There are a number of different deep learning language models out there, many of which are based on BERT. For our purposes we choose ALBERT, which is a smaller and more computationally manageable version of BERT. For more detailed information on the different language models see here: (https://github. com/huggingface/transformers#model-architectures). Understanding the Parameters of run_squad. py: Almost time to launch the training! Script parameters Below is a list of parameters that we used for running run_squad. py. Note that not all of the parameters are necessary (only model_type, model_name_or_path and output_dir are needed for the script to run). --_model_type_: This is the type of model you are using. The main types are (‘distilbert’, ‘albert’, ‘bart’, ‘longformer’, ‘xlm-roberta’, ‘roberta’, ‘bert’, ‘xlnet’, ‘flaubert’, ‘mobilebert’, ‘xlm’, ‘electra’, ‘reformer’). We used ‘albertxlv2’. --_model_name_or_path_: This is where you indicate the specific pretrained model you want to use from https://huggingface. co/models, we used albert-xlarge-v2. A list of Huggingface provided pretrained models is here https://huggingface. co/transformers/pretrained_models. html. If you want to load a model from a checkpoint, which we will explain how to do later, you would provide the path to the checkpoint here. --_output dir_: This is where you specify the path to the directory you want the model outputs to go. The directory should be empty. --_do_train_: This indicates that you want to train your model. --_do_eval_: This indicates that you want to evaluate the performance of your model. --_train_file_: This is the path to the file with your training data. --_predict_file_: This is the path to the file with your evaluation data. --_learning_rate_: Determines the step size when moving towards minimizing the loss. We used 3e-5. --_num_train_epochs_: Determines the number of times you want to go through the dataset. For our training the metrics started to plateau around the end of the third epoch. We would recommend 3 to 5 epochs, but you can stop training prematurely if you see your metrics plateauing before the training has finished. --_max_seq_length_: The maximum length of a sequence after tokenization. We used 384, which is the default. --_threads_: The number of vCPU threads you want to use for the process of converting examples to features. Check how many vCPU threads you have, it should be the number of vCPU cores times 2. We used 128 threads. --_per_gpu_train_batch_size_: The batch size for training you use per gpu, aka the number of examples the model will look at in one iteration. That is if you have 4 gpus and your --_per_gpu_train_batch_size_ is 4, your total batch size will be 16. A larger batch size will result in a more accurate gradient, but a smaller batch size will make the training go faster and use less memory. We used 1. --_per_gpu_eval_batch_size_: Same thing as per_gpu_train_batch_size except for evaluation. We used 8. --_version_2with_negative: If you are using the SQuAD V2 dataset with negative examples use this flag. --_evaluate_during_training_: Determines if you want to evaluate during training, but note that this will only work if you are only using 1 gpu due to averaging concerns. --_fp_16_: Determines if you want to use 16 point precision instead of the default 32 point precision. This will decrease the amount of ram you use and decrease the training time. Note if you want to use this flag you must have Nvidia apex installed from https://www. github. com/nvidia/apex (as described above). --_gradient_accumulation_steps_: Makes gradient descent more stable (in a similar training to the one described here, we used 4) Screen command We know you’re probably excited to start training, but here are a few helpful commands before jumping in. Before you start training your model there is one important terminal functionality you should use. Since you don’t want the training process to stop every time your terminal disconnects from the EC2 instance, which will happen after periods of inactivity, you need to disconnect the process of training the model from the EC2 terminal. To do this, go to the terminal logged into your EC2 instance and run the command screen which will create a blank terminal screen. Once you have done that you can run the command to start the training. (Note that in order for a virtual environment to work within the screen, you must be running the base environment outside of the screen. Before activating the screen, if you are in a virtual environment, run the command conda activate to return to the base environment, and only activate your virtual environment once you have entered the screen). The command consists of python run_squad. py followed by all of the flags you want to use for your training. Here is the command we ran: python run_squad. py --model_type ALBERTxlv2 --model_name_or_path ALBERT-xlarge-v2 --do_train --do_eval --evaluate_during_training --train_file ~/real_data/modified_GNQ_las_as_context_1_10th_train. json --predict_file ~/real_data/modified_GNQ_las_as_context_1_10th_dev. json --learning_rate 3e-5 --num_train_epochs 7 --max_seq_length 384 --doc_stride 128 --output_dir ~/reformatted_outputs --per_gpu_eval_batch_size=8 --per_gpu_train_batch_size=1 --fp16 --threads 128 --version_2_with_negativeOnce you start the training and see that it has started without any initial errors you can hit control a and then control d which will return you to your previous terminal screen with the training running in the background. You can now disconnect from the EC2 instance without affecting the training. Please note that you must not actually turn the instance off during training or the training will be halted. To reconnect to the screen where the training is running, run the command screen -r. It is a good idea to check in on the training once in a while to make sure it has not halted, which can be easily accomplished by looking at the wandb logging, and you can even set your wandb account to send you email alerts if the training fails. Common Errors We Encountered and How to Fix Them: To save you much of the frustration we endured, here are some errors we encountered and information on how to deal with them. If you want to be proactive about your training you could look through this section before you start the training and take measures to prevent any possible errors you may feel are likely to come up. The two main errors we encountered were running out of memory (RAM) and computer storage within our EC2 instance. (The commands presented here are meant to be run while logged into your instance, and assume that your instance runs on a linux operating system. ) How to manage storage: To see your current storage usage, run the command df -h. To find the largest files on your instance, run sudo du -x -h / | sort -h | tail -40 If you find yourself running out of storage there are a number of ways to get around it. Firstly, you could simply delete unneeded files on your amazon aws instance. To do that, navigate to the directory the files or folder you want to delete are located. Then run the command rm 'filename' to delete a single file, or the rm -R ‘foldername’ if you want to delete a folder and all files in it. You can also delete virtual environments you aren’t using that come preinstalled on the EC2 instance. (For running run_squad. py, you only need the pytorch_latest_p36. ) Do so by running the command ‘conda remove –name ‘myenv’ –all’ where myenv is the name of the environment you’d like to delete. A list of all installed environments is shown when starting up the instance. Another method is to simply add more storage to your machine through AWS. You can do this by simply going to the “volumes” section under the elastic block store header in the EC2 section of AWS. Once you are there select the instance you want to add more storage to, click the actions drop down menu, and then select modify volume. From here you can add as much storage as you like, but keep in mind you will be charged for it. See pricing info here: https://aws. amazon. com/ebs/pricing/. Note that you can only increase the volume size and not decrease it, so once you add more storage there is no going back. There are other methods for dealing with lack of storage issues, such as saving checkpoints to an s3 bucket as training runs, but we did not end up needing to use them. How to manage memory (RAM, different than storage space): To figure out your free memory in your instance at a given moment run the command free -h. A good way to keep track of your memory usage throughout the run is to look at the wandb logging in the system section. If you keep encountering out of memory issues, there are a few ways to get around it. A way of increasing your overall memory is to use something called swap space. If you are not familiar with swap space, it basically involves partitioning part of your storage to be used for memory. A detailed guide can be found here: https://www. digitalocean. com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04. One method to decrease memory usage is to decrease your batch size. Note that if you are having out of memory issues in the convert examples to features part of the training process this will not help. Another method to decrease memory usage is to run the run_sqad. py script with the –fp_16 flag on if you are not already. If you are getting out of memory issues in the convert examples to features part of the training process, it might be necessary to decrease the size of your dataset. One final word of advice to avoid random errors is to make sure that you have the most recent version of the transformers library installed, and you are using the most recent run_squad. py script. We encountered some errors related to mismatched versions. Note:Besides looking at the Wandb logging, you can use the command nvidia-smi to view available gpus and current gpu usage on your instance, and the command top to see your current cpu usage, as well as your memory usage. Restoring Training From a Checkpoint: Annoyed you have to start from the beginning after your model training failed? Well, you don’t have to! Here’s how you can resume training from a checkpoint. After every 500 examples that the model trains on, it will output a “model checkpoint” that is a representation of the model at that point in training. The model checkpoints can be found in the output folder, and each one is stored in a folder called “checkpoint-x” where x is the number of examples the model had looked at when that checkpoint was generated. Resuming training from a checkpoint can save you a lot of time and convenience.  If you encounter an error and want to pick up the training where you left off, or just want to resume training from a checkpoint for any other reason, here’s how to do it. All that is necessary is to run the same command you ran to start the training except --model_name_or_path must be set to the path to the folder for the checkpoint you want to resume training from. For example if your output folder is called outputs-2 and you want to resume training from checkpoint-1500 you should have --model_name_or_path /path_to_outputs_folder/outputs-2/checkpoint-1500. For consistency, if you’re restarting training from the middle, you may want to see what the learning rate was at the checkpoint you’re using (on wandb), and use that as the starting learning rate for your new training. Note that if you do end up stopping your training early due to the metrics leveling off, you will have stopped the script before it reached the evaluation section, so if you want the evaluation results you will have to run the script again with only the –do_eval flag, and not the –do_train flag, and –model_name_or_path should be set to the path to the checkpoint you want to evaluate. Understanding Your Model Outputs: What’s the point in training a model if we can’t understand what it’s telling us? Here is a primer on comprehending model terms. Understanding some run_squad evaluation metrics These metrics allow you to judge how well your model learned the dataset that you gave to it:  ‘Exact’ - compares answers that the model predicts for the dev set questions with the real answers, returns percentage of answers that model predicted correctly (eg answer spans match exactly) out of total dev set examples ‘F1’ - f1 is an accuracy measure that equals 2((precisionrecall)/(precision+recall)), where precision is the number of true positives over all positive results that the test returns, and recall is the number of true true positives over everything that should have been identified as positive. Here, precision and recall are computed by splitting the correct answers and the predicted answers into tokens by word. The number of tokens that appear in both answers is counted. This intersecting token count is divided by the number of tokens in the predicted answer to find the precision, and divided by number of tokens in the real answer to find recall. The final f1 score returned is the sum of f1 scores for each example in the dev set divided by the total number of examples.  ‘Total’ - total number of examples in dev set ‘HasAns_exact’ - average exact score (computed as above) but only taking the average for all examples in the dev set that have answers (e. g. not impossible) ‘HasAns_f1’ - average f1 score over all examples in dev set that have answers ‘HasAns_total’ - total # of examples in dev set that have answers ‘NoAns_exact’ - average exact score over all examples in dev set that have no answer (e. g. are impossible)‘NoAns_f1’ - average f1 score over all examples in dev set that have no answer ‘NoAns_total’ - total number of examples in dev set that have no answerUnderstanding the final model outputs Every checkpoint folder has files containing information about the model, optimizer, tokenizer, scheduler, tokens, and the training arguments. When the model has finished evaluating, it will output a predictions_. json file, a nbest_predections_. json file, and a null_odds_. json if you are using the SQuAD V2 dataset. The predictions file contains the model prediction for each example in the evaluation file, while the nbest_predictions file contains the n best predictions for each example in the evaluation file, where n is by default 20. Downloading Your Model/ Uploading to S3 Bucket: Congratulations! You’ve officially trained a ML model for Question Answering! Now that we have our model, where should we store it for future use? Once your model is trained, the last step is to download it from the EC2 instance so that you and others can access it. (Once an EC2 instance is deleted the information on it also is, so be sure to save the model information you want before terminating your instance for good. ) You can save all checkpoints that were stored throughout training, or you can choose to save only the best checkpoint based on evaluation metrics (be aware that the best checkpoint is not necessarily the last. ) To download the checkpoints to your local computer, use the scp command described above for file transfers from SSH. In order to share the model with others, or to upload it to Huggingface for public use, you will need to upload it to an Amazon s3 bucket. You can upload locally downloaded files to the s3 bucket using the s3 section of the AWS console. Alternatively, you can download the AWS client software onto your EC2 instance and upload the model files directly to a bucket without first downloading to your local computer. Do this by creating an s3 bucket from the s3 web console. Then connect the s3 bucket to your instance using an IAM role and the AWS cli software by following the steps outlined in this post. Once you have downloaded AWS cli, you can use the command “aws s3 cp filetocopy. txt s3://mybucketpath” to copy files into the bucket. Set permissions for public access using the command flags described here, or set permissions directly from the s3 web console. Sources: https://medium. com/@dearsikandarkhan/files-copying-between-aws-EC2-and-local-d07ed205eefa – file transfer between EC2 and local\https://www. digitalocean. com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04 – adding swap space\https://medium. com/@arnab. k/how-to-keep-processes-running-after-ending-ssh-session-c836010b26a3 – screen command\https://docs. aws. amazon. com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance. html – authorizing inbound traffic to instance\https://github. com/huggingface/transformers#run_squadpy-fine-tuning-on-squad-for-question-answeringhttps://machinelearningmastery. com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ – meaning of f1 score\http://www. kodyaz. com/aws/list-all-amazon-EC2-instances-using-aws-gui-tools. aspx – EC2 instance dashboard image "
    }, {
    "id": 8,
    "url": "http://localhost:4000/category-1/category-2/2019/12/31/Speed-up-requests-Asyncio-for-requests-in-python.html",
    "title": "Speed up requests: Asyncio for Requests in Python",
    "body": "2019/12/31 - Speed up requests: Asyncio for Requests in PythonDon’t be like this. : As you have probably already noticed because you decided to visit this page, requests can take forever to run, so here’s a nice blog written while I was an intern at NLMatics to show you how to use asyncio to speed them up. What is asyncio?: It is a Python library that uses the async/await syntax to make code run asynchronously. What does it mean to run asynchronously?: Synchronous (normal) vs. Asynchronous (using asyncio):  Synchronous: you must wait for the completion of the first task before starting another task.  Asynchronous: you can start another task before the completion of the first task. For more information on the distinction between concurrency, parallelism, threads, sync, and async, check out this Medium article. Simple analogy: Brick and Mortar: Simon and Ash are building 5 walls of brick.  Simon builds one wall and waits for it to set before starting to build the next wall (synchronous).  Ash, on the other hand, starts building the next wall before the first one sets (asynchronous).  &lt;/br&gt;Ash starts the next task whereas Simon waits, so Ash (asynchronous) will finish faster. The lack of waiting is the key to why asynchronous programming provides a performance boost. To compare, here’s an example where asynchronous programming does not provide any benefits. &lt;/br&gt;Simone and Asher are laying down bricks to build 1 wall.  Simone applies mortar to a brick and then sticks it in place, working through each brick one at a time (inefficient synchronous).  Asher, on the other hand, lathers mortar across the entire row of bricks, then sticks a row of bricks on top (efficient synchronous). &lt;/br&gt;It may seem like Asher is taking an asynchronous approach since she starts lathering the mortar of the next brick before finishing sticking the first brick. However, the concept of an asynchronous approach requires that there is waiting involved, and there is no waiting in this case. Simone and Asher are just constantly lathering mortar and laying bricks down. The coding equivalent of this is that Simon and Ash doing API calls (waiting) whereas Simone and Asher are mathematical calculations (no-waiting). As you can see from the second example, it is possible that an asynchronous approach (asyncio) does not necessarily benefit your code, so be aware. Moreover, be wary that an asynchronous approach does not provide any performance boost when all the tasks are dependent on each other.  For example, if you are washing and drying clothes, you must wait for the clothes to finish washing first before drying them no matter what, because drying clothes is dependent on the output of the washing. In this laundry example, there is indeed waiting. However, the existence of a dependent relationship causes the asynchronous pipeline to be the same as the synchronous pipeline, so there is no use in using an asynchronous approach. The coding equivalent of this laundry example is when the output of your current request is used as the input of the next request. For a further look into when and when not to use asynchronous programming, check out this Stackify thread. What syntax do I need to know?: | Syntax | Description / Example || — | — || async | Used to indicate which methods are going to be run asynchronously &lt;p&gt;	 → These new methods are called coroutines. &lt;/br&gt; async def p(): &lt;/br&gt; &#9;print( Hello World ) &lt;/p&gt;|| await | Used to run a coroutine once an asynchronous event loop has already started running → await can only be used inside a coroutine → Coroutines must be called with await, otherwise there will be a RuntimeWarning about enabling tracemalloc. &lt;/br&gt; async def r(): await p()|asyncio. run()Used to start running an asynchronous event loop from a normal programasyncio. run() cannot be called in a nested fashion. You have to use await instead. asyncio. run() cannot be used if you are running the Python file in a Jupyter Notebook because Jupyter Notebook already has a running asynchronous event loop. You have to use await. (More on this in the Running the Code section) async def pun():  await p() def run():  asyncio. run(pun())asyncio. create_task()Used to schedule a coroutine executionDoes not need to be awaitedAllows you to line things up without actually running them first. tasks = []task = asyncio. create_task(p())tasks. append(task)asyncio. gather()Used to run the scheduled executionsNeeds to be awaitedThis is vital to the asynchronous program, because you let it know which is the next task it can pick up before finishing the previous one. await asyncio. gather(*tasks) If you are thirsting for more in-depth knowledge on asyncio, check out these links: Async IO in Python: A Complete Walkthroughasyncio - Python Documentation But with that, let’s jump straight into the code. Follow along with the Python file and Jupyter Notebook in my github!https://github. com/clxxu/asyncio4requests Code: Preliminary: Get imports and generate the list of urls to get requests from. Here, I use this placeholder url. Don’t forget to do pip install -r requirements. txt in the terminal for all the modules that you don’t have. Keep in mind that normal requests cannot be awaited, so you will need to import requests_async. import requests, requests_async, asyncio, time itr = 200tag = ‘https://jsonplaceholder. typicode. com/todos/’urls = []for i in range(1, itr):  urls. append(tag + str(i))SynchronousThis is what a typical Python code for requests would look like. def synchronous(urls):  for url in urls:    r = requests. get(url)    print(r. json()) AsynchronousIncorrect Alteration async def asynchronous_fail(urls):  for url in urls:    r = await requests_async. get(url)    print(r. json()) This is an understandable but bad alteration to the synchronous code. The runtime for this is the same as the runtime for the synchronous method. This is because you have not created a list of tasks that the program knows it needs to execute together, thus you essentially still have synchronous code. Correct Alterationasync def asynchronous(urls):  tasks = []  for url in urls:    task = asyncio. create_task(requests_async. get(url))    tasks. append(task)  responses = await asyncio. gather(*tasks)  for response in responses:    print(response. json()) Here, we created a list of tasks, and then ran all of them together using asyncio. gather(). Running the CodePythonSimply add these three lines to the bottom of your Python file and run it. starttime = time. time()asyncio. run(asynchronous(urls))print(time. time() - starttime) If you try to run this same code in Jupyter Notebook, you will get this error:RuntimeError: asyncio. run() cannot be called from a running event loop This happens because Jupyter is already running an event loop. More info here. You need to use the following:Jupyter Notebookstarttime = time. time()await asynchronous(urls)print(time. time() - starttime) OrderingAsynchronous running can cause your responses to be out of order. If this is an issue, create your own responses list and fill it up, rather than receiving the output from asyncio. gather(). async def asynchronous_ordered(urls):  responses = [None] * len(urls)  tasks = []  for i in range(len(urls)):    url = urls[i]    task = asyncio. create_task(fetch(url, responses, i))    tasks. append(task)  await asyncio. gather(*tasks)  for response in responses:    print(response. json()) async def fetch(url, responses, i):  response = await requests. get(url)  responses[i] = response BatchingSometimes running too many requests concurrently can cause timeout errors in your resource. In my own experience, MongoDB had timeout errors. Create tasks in batches and gather them separately to avoid the issue. Change the batch_size to fit your purposes. async def asynchronous_ordered_batched(urls, batch_size=10):  responses = [None] * len(urls)  kiterations = int(len(urls) / batch_size) + 1  for k in range(0, kiterations):    tasks = []    m = min((k + 1) * batch_size, len(urls))    for i in range(k * batch_size, m):      url = urls[i]      task = asyncio. create_task(fetch(url, responses, i))      tasks. append(task)    await asyncio. gather(*tasks)  for response in responses:    print(response. json()) Runtime Results Synchronous and asynchronous_fail have similar runtimes because the asynchronous_fail method was not implemented correctly and is in reality synchronous code. Asynchronous, asynchronous_ordered, and asynchronous_ordered_batched have noticeably better runtimes in comparison to synchronous code - up to 4 times as fast. In general, asynchronous ordered batched gives fast and stable code, so use that if you are going for consistency. However, the runtimes of asynchronous and asynchronous ordered can sometimes be better than asynchronous ordered batched, depending on your database and servers. So, I would recommend using asynchronous first and then adding extra things (order and batch) as necessary. SummaryOverall, asyncio is a helpful tool that can greatly boost your runtime if you are running a lot of independent API requests. It’s also very easy to implement when compared to threading, so you should definitely try it out! Now I must conclude by saying that using asyncio improperly implemented can cause many bugs, so sometimes it is not worth the hassle. If you really must, use my guide and use it s p a r i n g l y. Further ResourcesConcurrency, parallelism, threads, sync, and async:Medium articleWhen/when not to use asynchronous programming:Stackify threadMore knowledge on asyncio:Async IO in Python: A Complete Walkthroughasyncio - Python Documentation "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});