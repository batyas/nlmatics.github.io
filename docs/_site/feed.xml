<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-29T04:08:35+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nlmatics.github.io</title><entry><title type="html">Optimizing Transformer Q&amp;amp;A Models for Naturalistic Search</title><link href="http://localhost:4000/2020/08/25/Optimizing-Transformer-Q&A-Models-for-Naturalistic-Search.html" rel="alternate" type="text/html" title="Optimizing Transformer Q&amp;A Models for Naturalistic Search" /><published>2020-08-25T19:30:00+09:00</published><updated>2020-08-25T19:30:00+09:00</updated><id>http://localhost:4000/2020/08/25/Optimizing%20Transformer%20Q&amp;A%20Models%20%20for%20Naturalistic%20Search</id><content type="html" xml:base="http://localhost:4000/2020/08/25/Optimizing-Transformer-Q&amp;A-Models-for-Naturalistic-Search.html">&lt;h1 id=&quot;optimizing-transformer-qa-models-for-naturalistic-search&quot;&gt;Optimizing Transformer Q&amp;amp;A Models for Naturalistic Search&lt;/h1&gt;

&lt;p&gt;By Batya Stein&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Have you ever wondered what the most popular pizza topping in America is? To find out, you might Google “what is the most popular pizza topping in America?”. If you’re feeling lazy, maybe you’d leave out the question mark, or drop the question format altogether, and go with a simple “most popular pizza topping in America”. 
&lt;img src=&quot;/site_files/batya-post-imgs/pizza-phrases.png&quot; alt=&quot;pizza phrases&quot; /&gt;
This probably seems like an excessive amount of thought to put into a Google search query, especially since all the above prompts return the same information. (Pepperoni, for the curious.) With or without the question word and question mark, you can intuitively recognize that all these formulations are essentially asking for the same answer.&lt;/p&gt;

&lt;p&gt;However, imagine that you wanted to train a machine learning model to complete a similar exercise. If you fed your model background material on America’s pizza-eating habits, and then asked it some variation of the topping question, you’d like it to identify the answer “pepperoni” within the text. Models are usually trained for this type of question-answering task using reading comprehension datasets, like the &lt;strong&gt;S&lt;/strong&gt;tanford &lt;strong&gt;Qu&lt;/strong&gt;estion &lt;strong&gt;A&lt;/strong&gt;nswering &lt;strong&gt;D&lt;/strong&gt;ataset (&lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD&lt;/a&gt;). SQuAD’s examples consist of context paragraphs from Wikipedia, and questions which either have answers located within the context paragraphs, or are unanswerable. Each question is written as a full sentence, complete with a question mark at the end. Nearly all questions contain a question word (“who”, “what”, “where”, etc.).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/batya-post-imgs/squad-data-pt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Example of SQuAD question and context paragraph&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Given that a model trained on SQuAD has only been exposed to prompts in the form of questions, the specific way a user words their query takes on new importance. Would a model trained only on questions be able to process phrases and match them to answers with the same accuracy as it can questions? In other words, which characteristics are most necessary for a prompt to be accurately understood by a SQuAD-trained model? Though it may seem obvious to you and me that “what is the most popular topping?” is basically synonymous with “the most popular topping”, it is not clear that the same would be apparent to a SQuAD-trained model.&lt;/p&gt;

&lt;h2 id=&quot;experiment-motivation&quot;&gt;Experiment Motivation&lt;/h2&gt;

&lt;p&gt;As an intern at NLMatics this summer, I ran a series of experiments using transformer-based question-answering models and the SQuAD 2.0 dataset to explore how the wording of a dataset’s questions impacts a model’s accuracy. My goal was to see if:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;a model &lt;strong&gt;trained&lt;/strong&gt; only on &lt;ins&gt;question&lt;/ins&gt;-answer pairs (e.g. the standard SQuAD dataset) would give accurate results when &lt;strong&gt;evaluated&lt;/strong&gt; on &lt;ins&gt;phrase&lt;/ins&gt;-answer pairs, and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If a model trained on phrase-answer pairs could achieve the same performance scores (F1 and exact match) when asked to predict answers for phrases, as a model trained on normal SQuAD achieves when asked to predict answers to questions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I also trained models on different permutations of the phrase and question datasets - phrases with question marks, and questions without question marks - to isolate the characteristics that make a question answering model effective. Is it the presence of a question mark, or of a question word, or simply the keywords of a phrase?&lt;/p&gt;

&lt;p&gt;The results of these experiments are relevant for any system where a user directly queries a text using a question-answering model. The designer of the system doesn’t necessarily have control over the questions users ask, or a way of knowing if they will choose to input questions (ex. “where is the building?”) or phrases (ex. “building location”). If a model could be trained for accuracy in phrase answering tasks without a significant decrease in accuracy on question answering tasks, users would be able to input a much broader range of questions, without the designer having to worry about precise wording.&lt;/p&gt;

&lt;h2 id=&quot;question-to-phrase-conversion&quot;&gt;Question to Phrase Conversion&lt;/h2&gt;

&lt;p&gt;In order to create a dataset of phrases directly comparable to a question dataset, I decided to reformat the SQuAD 2.0 dataset into phrases. (&lt;a href=&quot;https://ai.google.com/research/NaturalQuestions&quot;&gt;Google Natural Questions&lt;/a&gt;, another large reading comprehension dataset, has some examples that are already written as phrases and some written as questions, but the variability makes it less useful for direct comparisons.) My goal when converting the questions was to create phrases that sounded like something natural a user might type into a search engine, so that the training data would be similar to what would be seen in a real use case.&lt;/p&gt;

&lt;p&gt;To this end, I wrote a &lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/code-examples/qs_to_phrases.py&quot;&gt;python script to process SQuAD&lt;/a&gt; that, besides deleting question words, changed tenses when necessary, moved verbs after subjects, and reordered the clauses of complex questions. I used the &lt;a href=&quot;https://spacy.io/&quot;&gt;spaCy&lt;/a&gt; NLP library for part-of-speech tagging and dependency parsing (ex. identifying the subject of a sentence), and the &lt;a href=&quot;https://github.com/clips/pattern#pattern&quot;&gt;pattern&lt;/a&gt; library for verb conjugation. The code had rules for handling questions based on the question words they began with, and a separate rule for handling questions with more complex structures.&lt;/p&gt;

&lt;p&gt;Some example outputs from my phrase conversion code:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Original Question&lt;/th&gt;
      &lt;th&gt;New Phrase&lt;/th&gt;
      &lt;th&gt;Comment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;“On what date did Henry Kissinger negotiate an Israeli troop withdrawal from the Sinai Peninsula?”&lt;/td&gt;
      &lt;td&gt;“date Henry Kissinger &lt;strong&gt;negotiated&lt;/strong&gt; an Israeli troop withdrawal from Sinai Peninsula on”&lt;/td&gt;
      &lt;td&gt;To create a natural sounding phrase after deleting “did” from questions, I used the pattern library’s verb conjugation function to change remaining verbs to past tense&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“Atticus Finch’s integrity has become a model for which job?”&lt;/td&gt;
      &lt;td&gt;“job Atticus Finch’s integrity has become a model for”&lt;/td&gt;
      &lt;td&gt;For examples where question words appeared mid-sentence (instead of as the first word), I split questions at the question word and moved the first clause to the back&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“Where &lt;strong&gt;would&lt;/strong&gt; present day Joara be?”&lt;/td&gt;
      &lt;td&gt;“present day Joara &lt;strong&gt;would&lt;/strong&gt; be”&lt;/td&gt;
      &lt;td&gt;When a phrase wouldn’t make sense without an auxiliary verb like “would”, “could”, “can”, etc., I used spaCy to identify the question’s subject, and moved the auxiliary verb behind it (otherwise, I deleted unnecessary auxiliary verbs such as “are” or “is”)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;model-training&quot;&gt;Model Training&lt;/h2&gt;

&lt;p&gt;To figure out which characteristics of a question were most important, I trained four ALBERT models on four different datasets. ALBERT is a light variation of the BERT transformer model with fewer parameters for an even quicker training time than BERT. (See the original paper on BERT &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;here&lt;/a&gt;, with a helpful explanation &lt;a href=&quot;https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/&quot;&gt;here&lt;/a&gt; and a general overview of transformer architecture &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The four datasets I used were:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Normal SQuAD (2.0) dataset&lt;/strong&gt; - &lt;em&gt;a baseline for comparison&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/site_files/batya-post-imgs/normal-q.png&quot; alt=&quot;img&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SQuAD converted to phrases&lt;/strong&gt; - &lt;em&gt;can the model match phrases to answers effectively since they are more open ended than questions?&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/site_files/batya-post-imgs/phrase.png&quot; alt=&quot;img&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SQuAD questions without question marks&lt;/strong&gt; - &lt;em&gt;how important is question mark in getting the model to recognize a question?&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/site_files/batya-post-imgs/q-no-mark.png&quot; alt=&quot;img&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SQuAD converted to phrases, with question marks appended&lt;/strong&gt; - &lt;em&gt;does the addition of a question mark compensate for the lack of a question structure?&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/site_files/batya-post-imgs/phrase-with-q.png&quot; alt=&quot;img&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The ALBERT XL model that I used came from Huggingface’s &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;transformer library&lt;/a&gt;, and for training I used Huggingface’s &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py&quot;&gt;run_squad.py&lt;/a&gt; script. I used &lt;a href=&quot;https://www.wandb.com/&quot;&gt;wandb&lt;/a&gt;, a web-based training visualization library, to monitor metrics throughout my trainings. The trainings were run on Amazon EC2 p2.8xl instances (8 NVIDIA K80 GPUs).&lt;/p&gt;

&lt;h2 id=&quot;squad-metrics&quot;&gt;SQuAD Metrics&lt;/h2&gt;

&lt;p&gt;The official metrics for evaluating a SQuAD trained model are calculated in the &lt;a href=&quot;https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/&quot;&gt;evaluation script&lt;/a&gt; linked on SQuAD’s website. The two main metrics are “exact” and “F1”. Exact” returns the percentage of examples that the model predicted correct answers for out of all test examples. “&lt;a href=&quot;https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9&quot;&gt;F1&lt;/a&gt;” is an accuracy measure combining precision (number of true positives over total predicted positives) and recall (number of true positives over everything that should have been identified as positive). To calculate F1 scores for SQuAD, for each example in the evaluation set, precision is the number of overlapping words between the correct and predicted answers, divided by the number of words in the &lt;ins&gt;predicted&lt;/ins&gt; answer. Recall is the number of overlapping words divided by the number of words in the &lt;ins&gt;correct&lt;/ins&gt; answer.&lt;/p&gt;

&lt;p&gt;The metrics return F1 and exact scores over all the evaluation set examples, as well as F1 and exact scores over the subset of evaluation examples that have answers, (“HasAns”) and over the subset of examples that are impossible to answer (“NoAns”).&lt;/p&gt;

&lt;h2 id=&quot;results-and-discussion&quot;&gt;Results and Discussion&lt;/h2&gt;

&lt;p&gt;To analyze the results of my 4 model trainings, I first compared the performances of each model evaluated on its own dev set (I used dev sets for evaluations since SQuAD’s test set is not publicly available). In other words, after training, how accurately would each model perform on unseen data formatted the same way as its training data?&lt;/p&gt;

&lt;p&gt;(For consistency, results shown below are evaluated at the 7000th training step, towards the end of the 2nd epoch. The exception is the F1 score chart, which spans from beginning to end of training.)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Metrics of 4 datasets, evaluated on their own dev sets at 7000th step&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Normal SQuAD (questions with question marks)&lt;/th&gt;
      &lt;th&gt;Phrases&lt;/th&gt;
      &lt;th&gt;Phrases with question marks&lt;/th&gt;
      &lt;th&gt;Questions without question marks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;F1&lt;/td&gt;
      &lt;td&gt;84.42&lt;/td&gt;
      &lt;td&gt;83.3&lt;/td&gt;
      &lt;td&gt;83.59&lt;/td&gt;
      &lt;td&gt;84.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Exact&lt;/td&gt;
      &lt;td&gt;80.86&lt;/td&gt;
      &lt;td&gt;80.27&lt;/td&gt;
      &lt;td&gt;80.67&lt;/td&gt;
      &lt;td&gt;81.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_f1&lt;/td&gt;
      &lt;td&gt;83.2&lt;/td&gt;
      &lt;td&gt;80.77&lt;/td&gt;
      &lt;td&gt;81.36&lt;/td&gt;
      &lt;td&gt;85.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_exact&lt;/td&gt;
      &lt;td&gt;76.06&lt;/td&gt;
      &lt;td&gt;74.7&lt;/td&gt;
      &lt;td&gt;75.52&lt;/td&gt;
      &lt;td&gt;79.91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_f1&lt;/td&gt;
      &lt;td&gt;85.63&lt;/td&gt;
      &lt;td&gt;85.82&lt;/td&gt;
      &lt;td&gt;85.8&lt;/td&gt;
      &lt;td&gt;82.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_exact&lt;/td&gt;
      &lt;td&gt;85.63&lt;/td&gt;
      &lt;td&gt;85.82&lt;/td&gt;
      &lt;td&gt;85.8&lt;/td&gt;
      &lt;td&gt;82.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_f1&lt;/td&gt;
      &lt;td&gt;84.42&lt;/td&gt;
      &lt;td&gt;83.3&lt;/td&gt;
      &lt;td&gt;83.59&lt;/td&gt;
      &lt;td&gt;84.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_exact&lt;/td&gt;
      &lt;td&gt;80.86&lt;/td&gt;
      &lt;td&gt;80.27&lt;/td&gt;
      &lt;td&gt;80.67&lt;/td&gt;
      &lt;td&gt;81.24&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/batya-post-imgs/metrics.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;F1 scores for all models throughout training&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Step&lt;/th&gt;
      &lt;th&gt;SQuAD&lt;/th&gt;
      &lt;th&gt;Phrases&lt;/th&gt;
      &lt;th&gt;Phrases with q marks&lt;/th&gt;
      &lt;th&gt;Q’s without q marks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;500&lt;/td&gt;
      &lt;td&gt;64.98&lt;/td&gt;
      &lt;td&gt;63.54&lt;/td&gt;
      &lt;td&gt;68.50&lt;/td&gt;
      &lt;td&gt;72.47&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;77.08&lt;/td&gt;
      &lt;td&gt;76.08&lt;/td&gt;
      &lt;td&gt;77.05&lt;/td&gt;
      &lt;td&gt;77.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1500&lt;/td&gt;
      &lt;td&gt;77.75&lt;/td&gt;
      &lt;td&gt;77.01&lt;/td&gt;
      &lt;td&gt;77.40&lt;/td&gt;
      &lt;td&gt;78.93&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2000&lt;/td&gt;
      &lt;td&gt;80.52&lt;/td&gt;
      &lt;td&gt;79.35&lt;/td&gt;
      &lt;td&gt;79.44&lt;/td&gt;
      &lt;td&gt;80.93&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2500&lt;/td&gt;
      &lt;td&gt;80.82&lt;/td&gt;
      &lt;td&gt;79.50&lt;/td&gt;
      &lt;td&gt;79.05&lt;/td&gt;
      &lt;td&gt;81.19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3000&lt;/td&gt;
      &lt;td&gt;81.81&lt;/td&gt;
      &lt;td&gt;79.05&lt;/td&gt;
      &lt;td&gt;80.62&lt;/td&gt;
      &lt;td&gt;82.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3500&lt;/td&gt;
      &lt;td&gt;80.91&lt;/td&gt;
      &lt;td&gt;80.36&lt;/td&gt;
      &lt;td&gt;80.71&lt;/td&gt;
      &lt;td&gt;80.93&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4000&lt;/td&gt;
      &lt;td&gt;82.28&lt;/td&gt;
      &lt;td&gt;80.37&lt;/td&gt;
      &lt;td&gt;80.95&lt;/td&gt;
      &lt;td&gt;83.07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4500&lt;/td&gt;
      &lt;td&gt;83.09&lt;/td&gt;
      &lt;td&gt;81.01&lt;/td&gt;
      &lt;td&gt;81.15&lt;/td&gt;
      &lt;td&gt;82.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5000&lt;/td&gt;
      &lt;td&gt;83.06&lt;/td&gt;
      &lt;td&gt;81.65&lt;/td&gt;
      &lt;td&gt;82.47&lt;/td&gt;
      &lt;td&gt;83.59&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5500&lt;/td&gt;
      &lt;td&gt;83.84&lt;/td&gt;
      &lt;td&gt;82.85&lt;/td&gt;
      &lt;td&gt;82.50&lt;/td&gt;
      &lt;td&gt;84.09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6000&lt;/td&gt;
      &lt;td&gt;84.27&lt;/td&gt;
      &lt;td&gt;82.48&lt;/td&gt;
      &lt;td&gt;83.20&lt;/td&gt;
      &lt;td&gt;84.46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6500&lt;/td&gt;
      &lt;td&gt;84.46&lt;/td&gt;
      &lt;td&gt;82.02&lt;/td&gt;
      &lt;td&gt;83.38&lt;/td&gt;
      &lt;td&gt;84.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7000&lt;/td&gt;
      &lt;td&gt;84.42&lt;/td&gt;
      &lt;td&gt;83.30&lt;/td&gt;
      &lt;td&gt;83.59&lt;/td&gt;
      &lt;td&gt;84.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7500&lt;/td&gt;
      &lt;td&gt;84.80&lt;/td&gt;
      &lt;td&gt;83.01&lt;/td&gt;
      &lt;td&gt;83.54&lt;/td&gt;
      &lt;td&gt;84.83&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/batya-post-imgs/F1 Scores for all models throughout training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As expected, the &lt;strong&gt;standard SQuAD-trained model&lt;/strong&gt; has high metrics across the board and high F1 scores throughout training, since its examples have the most context for the model to pick up on (question format and question marks). Interestingly, the &lt;strong&gt;SQuAD without question marks model&lt;/strong&gt; does better than the standard SQuAD model for the “HasAns” metrics (HasAns F1 of 85.90 vs 83.20), and notably worse for the “NoAns” (NoAns F1 of 82.57 vs 85.63)- I am not sure exactly what to conclude from this (it may not be a conclusive trend, as the bar graph only represents one checkpoint from training). Overall though, throughout training, the F1 scores of the SQuAD without question marks model were equal to, or higher than, the standard SQuAD F1’s, showing that the inclusion or exclusion of question marks seems relatively insignificant for achieving accuracy.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;phrase-trained model&lt;/strong&gt; metrics are equal to, or 1-3 points lower than the standard SQuAD metrics, and its F1 scores throughout training are lower overall but not to a super significant extent. This answers one of my main beginning questions, indicating that while there is some increased accuracy associated with the additional context of a question structure, &lt;ins&gt;a model trained on phrase-answer pairs does not show a significant decrease in accuracy&lt;/ins&gt;.&lt;/p&gt;

&lt;p&gt;Adding question marks to phrases does yield slightly better F1 scores throughout training (see graph above). A possible interpretation of this trend is that all the contextual question information is present, question marks are essentially negligible, but with less information presented, as in phrases, the question mark does help increase accuracy.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;For the second part of my analysis, I evaluated each of the models on the standard SQuAD dev set, and the standard SQuAD trained model on the phrase dev set. (All results evaluated at the 7000th step, as above). Through these evaluations I hoped to see how well a model trained on a modified dataset could perform on a differently formatted question.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Question-trained model evaluated on questions vs on phrases&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;eval on phrase dev set&lt;/th&gt;
      &lt;th&gt;eval on question dev set&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;F1&lt;/td&gt;
      &lt;td&gt;83.3&lt;/td&gt;
      &lt;td&gt;81.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Exact&lt;/td&gt;
      &lt;td&gt;80.27&lt;/td&gt;
      &lt;td&gt;77.83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_f1&lt;/td&gt;
      &lt;td&gt;80.77&lt;/td&gt;
      &lt;td&gt;80.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_exact&lt;/td&gt;
      &lt;td&gt;74.7&lt;/td&gt;
      &lt;td&gt;74.06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_f1&lt;/td&gt;
      &lt;td&gt;85.82&lt;/td&gt;
      &lt;td&gt;81.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_exact&lt;/td&gt;
      &lt;td&gt;85.82&lt;/td&gt;
      &lt;td&gt;81.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_f1&lt;/td&gt;
      &lt;td&gt;83.3&lt;/td&gt;
      &lt;td&gt;81.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_exact&lt;/td&gt;
      &lt;td&gt;80.27&lt;/td&gt;
      &lt;td&gt;77.83&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/batya-post-imgs/question-trained model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notably, &lt;ins&gt;a standard SQuAD-trained model performs really poorly when evaluated on phrases&lt;/ins&gt;, as shown in the charts above. Its F1 score drops from 84.42 (evaluated on questions) to 64.11 (evaluated on phrases), and its HasAnsF1 score drops from 83.20 to 50.96. Practically, this proves the necessity of training models that are better equipped to deal with phrases. On a theoretical level, this indicates that the model comes to rely on question words/ structure for context when trained on questions, and therefore it can’t process questions accurately without them. (That’s not to say that question words are inherently necessary for achieving accuracy in training - see phrase-trained model metrics above - rather, once a model &lt;em&gt;is&lt;/em&gt; trained on questions, it will become reliant on question characteristics).&lt;/p&gt;

&lt;p&gt;On the other hand, as shown in the charts below, a model trained on phrases performs almost as accurately on standard questions as it does on its own dev set - it’s well adapted to inputs of both phrases and questions. It does slightly better on the phrase evaluation, seemingly because the model learns weights corresponding to the format of its training data, but since here the second dev set (questions) introduces new information instead of taking away expected context, the extra input barely hurts. In other words, the phrase-trained model is more robust for a general set of inputs than a question-based model because it was forced to learn from less context per data point originally. However, it performs slightly less accurately on questions as compared to a question-trained model evaluated on questions (or as compared to its own performance on phrases).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Phrase trained model evaluated on phrases vs. questions&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;eval on phrase dev set&lt;/th&gt;
      &lt;th&gt;eval on question dev set&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;F1&lt;/td&gt;
      &lt;td&gt;83.3&lt;/td&gt;
      &lt;td&gt;81.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Exact&lt;/td&gt;
      &lt;td&gt;80.27&lt;/td&gt;
      &lt;td&gt;77.83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_f1&lt;/td&gt;
      &lt;td&gt;80.77&lt;/td&gt;
      &lt;td&gt;80.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_exact&lt;/td&gt;
      &lt;td&gt;74.7&lt;/td&gt;
      &lt;td&gt;74.06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_f1&lt;/td&gt;
      &lt;td&gt;85.82&lt;/td&gt;
      &lt;td&gt;81.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_exact&lt;/td&gt;
      &lt;td&gt;85.82&lt;/td&gt;
      &lt;td&gt;81.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_f1&lt;/td&gt;
      &lt;td&gt;83.3&lt;/td&gt;
      &lt;td&gt;81.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_exact&lt;/td&gt;
      &lt;td&gt;80.27&lt;/td&gt;
      &lt;td&gt;77.83&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/batya-post-imgs/phrase-trained model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;One other interesting result came from the model trained on &lt;strong&gt;questions without question marks&lt;/strong&gt;, shown in the charts below. When evaluated on &lt;strong&gt;questions with question marks&lt;/strong&gt; (i.e. normal SQuAD) the metrics were slightly better than on its own dev set for the “HasAns” metrics, but slightly worse for the “NoAns”. I would guess this is because if the model isn’t exposed to question marks during training, but then is given a question with a question mark, it will be more likely to assume that the question is answerable given the extra input, which leads to increased accuracy if the question actually has an answer, but decreased accuracy when the question is impossible.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Questions without question mark model evaluated on own dev set vs. normal SQuAD&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;eval on q no q mark dev set&lt;/th&gt;
      &lt;th&gt;eval on question dev set&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;F1&lt;/td&gt;
      &lt;td&gt;84.23&lt;/td&gt;
      &lt;td&gt;83.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Exact&lt;/td&gt;
      &lt;td&gt;81.24&lt;/td&gt;
      &lt;td&gt;80.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_f1&lt;/td&gt;
      &lt;td&gt;85.9&lt;/td&gt;
      &lt;td&gt;87.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HasAns_exact&lt;/td&gt;
      &lt;td&gt;79.91&lt;/td&gt;
      &lt;td&gt;81.51&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_f1&lt;/td&gt;
      &lt;td&gt;82.57&lt;/td&gt;
      &lt;td&gt;78.73&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NoAns_exact&lt;/td&gt;
      &lt;td&gt;82.57&lt;/td&gt;
      &lt;td&gt;78.73&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_f1&lt;/td&gt;
      &lt;td&gt;84.23&lt;/td&gt;
      &lt;td&gt;83.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best_exact&lt;/td&gt;
      &lt;td&gt;81.24&lt;/td&gt;
      &lt;td&gt;80.13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/batya-post-imgs/questions without q marks.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In conclusion, it seems that BERT transformer models begin to recognize and expect specific characteristics from their training data. This affects the results when a model is evaluated on data formatted differently from its original training set, but to a much less dramatic extent if information is being added rather than taken away. 
On the other hand, the initial dataset a model is trained on doesn’t affect accuracy as much as you might think it would - a model trained on phrases will be within a few points of a question-trained model’s accuracy, and adding or taking away question marks makes a mostly negligible difference.
Running these experiments was cool because I was able to get a better feel for how a model actually adapts to a dataset when training on it. I also think the results I found can be practically pretty helpful for anyone designing a question-answering system and wondering what types of input would be most effective.&lt;/p&gt;

&lt;h2 id=&quot;further-developments&quot;&gt;Further Developments&lt;/h2&gt;

&lt;p&gt;Each model presented here was only trained once. To run this as a full experiment, ensuring that the results are significant, you would have to set random seeds and run each training about ten times to get standard deviations.&lt;/p&gt;

&lt;p&gt;Additionally, there are a few ways to build on this project that I think could be interesting. You could train an ALBERT model twice, on normal SQuAD and then on SQuAD converted to phrases, to see if it achieves better results on both questions and phrases. However, training a model twice on the same data might lead to overfitting on SQuAD. An alternative would be to train a model using MAML, &lt;a href=&quot;https://towardsdatascience.com/model-agnostic-meta-learning-maml-8a245d9bc4ac&quot;&gt;Model Agnostic Meta Learning&lt;/a&gt;, which allows a model to learn multiple different tasks, treating questions and phrases as two separate tasks. It would also be interesting to train a model on the full &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions&quot;&gt;Google Natural Questions&lt;/a&gt; dataset, which is already a mixture of questions and phrases, and evaluate it just with a question dataset and just with a phrase dataset to see how the mixture and larger dataset size affect the results.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Thank you to my supervisor, Sonia Joseph, who initially proposed this idea to me, helped along the way, and introduced me to MAML.&lt;/p&gt;</content><author><name>Batya Stein</name></author><summary type="html">Optimizing Transformer Q&amp;amp;A Models for Naturalistic Search</summary></entry><entry><title type="html">How to Extract Keyphrases and Visualize Text</title><link href="http://localhost:4000/keyphrase-extraction/visualization/text/nlp/python/javascript/keppler-mapper/d3/2020/08/07/How-to-Extract-Keyphrases-and-Visualize-Text.html" rel="alternate" type="text/html" title="How to Extract Keyphrases and Visualize Text" /><published>2020-08-07T19:30:00+09:00</published><updated>2020-08-07T19:30:00+09:00</updated><id>http://localhost:4000/keyphrase-extraction/visualization/text/nlp/python/javascript/keppler-mapper/d3/2020/08/07/How-to-Extract-Keyphrases-and-Visualize-Text</id><content type="html" xml:base="http://localhost:4000/keyphrase-extraction/visualization/text/nlp/python/javascript/keppler-mapper/d3/2020/08/07/How-to-Extract-Keyphrases-and-Visualize-Text.html">&lt;h4 id=&quot;by-cheyenne-zhang&quot;&gt;by Cheyenne Zhang&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Cheyenne Zhang&lt;/strong&gt; is a rising junior at Princeton University studying Computer Science with certificates in East Asian Studies and Cognitive Science. She’s originally from just outside of Seattle, WA, and enjoys watching movies and learning languages in her free time. Cheyenne was one of NLMatics’ 2020 summer interns.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/visualizekeyphrases.gif&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;The end product of following this article on the 20 newsgroup dataset. Clusters represent related phrases, and each cluster is labeled with a representative phrase.&lt;/i&gt;&lt;/div&gt;

&lt;h2 id=&quot;introduction&quot;&gt;INTRODUCTION&lt;/h2&gt;

&lt;p&gt;Imagine that you’ve just gotten a huge dataset; for example, the &lt;a href=&quot;http://qwone.com/~jason/20Newsgroups/&quot;&gt;20 newsgroup dataset&lt;/a&gt;, which has 11,314 entries, an example of which is shown below. What would you do to try to understand the data as a whole? Maybe you would look at some entries and their categorization. Maybe you would search the dataset for certain keywords or topics.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/20newsgroupexample.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;The first entry in the newsgroup20 dataset.&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;These approaches will take reading through a large amount of data—maybe even the whole dataset—to gain an understanding of what is going on. However, if we were somehow able to find a way to visualize it, perhaps that might allow us to condense meaning into something we can view in a short amount of time. That’s the goal of the visualization approach—to provide information at a glance, as well as new ways of interacting with this information.&lt;/p&gt;

&lt;p&gt;In this article, we’ll see some quick but successful ways to extract meaningful keyphrases from text and how we can use that to produce helpful visualizations.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;MOTIVATION&lt;/h2&gt;
&lt;p&gt;Over the course of about four weeks during my internship at NLMatics, I worked on creating a visualization of search results from the NLMatics engine. Currently, asking a question to our system provides a textual list of possible answers. While this is very useful in many cases, there is an opportunity to add another lens through which to view information by displaying it in a more intuitive manner, and to guide the user to see what is important about this particular document. With visualization, there are so many possibilities for expressing connections and relationships among the data–color-coding, relative size, text labels, link lengths, link colors…the list goes on! We’ll explore some of these ideas throughout this article.&lt;/p&gt;

&lt;p&gt;In this piece, I’ll walk through a simplified version of the process I went through and also some of what I learned from this project. There are two main steps: Keyphrase Extraction, i.e. getting the most meaningful phrases from our text, and then Visualization, which involves clustering based on those phrases and displaying these clusters in an aesthetically appealing manner. If you just want to see the final product, I have set up a &lt;a href=&quot;https://github.com/cznlm/visualize-keyphrases&quot;&gt;repository&lt;/a&gt; that you can just download and run. It provides a notebook with all of the code from the both steps as well as the modified kmapper code from the Visualization section.&lt;/p&gt;

&lt;h2 id=&quot;keyphrase-extraction&quot;&gt;KEYPHRASE EXTRACTION&lt;/h2&gt;
&lt;h3 id=&quot;intro-to-rake-and-yake&quot;&gt;INTRO. TO RAKE AND YAKE&lt;/h3&gt;
&lt;p&gt;Keyphrase extraction is not an easy task for a computer; it’s not even easy for humans! Take this sentence from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Harry_Potter&quot;&gt;Harry Potter Wikipedia page&lt;/a&gt; for example:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;“The central character in the series is Harry Potter, a boy who lives in the fictional town of Little Whinging, Surrey with his aunt, uncle, and cousin.”&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;What are the important keyphrases here? Are they the proper nouns—“Harry Potter” and “Little Whinging, Surrey”? Or are they people—“boy,” “aunt,” “uncle,” and “cousin”? Or are the book-related terms most important—”central character”, “series”, “fictional”? As you can see, there are many different interpretations of what makes a keyphrase “meaningful,” even to human judgement. This makes keyphrase extraction a very interesting area of NLP research. For background, &lt;a href=&quot;https://arxiv.org/pdf/1905.05044.pdf&quot;&gt;this paper&lt;/a&gt;, published in July 2019, provides a very thorough overview of the unsupervised and supervised methods currently in the field. Additionally, &lt;a href=&quot;https://www.aclweb.org/anthology/P14-1119.pdf&quot;&gt;this paper&lt;/a&gt; from 2014 is also a very prominent literature review, if a bit outdated.&lt;/p&gt;

&lt;p&gt;For this demonstration, we will focus on a couple of automatic methods (i.e. no training of a model required), namely &lt;a href=&quot;https://www.researchgate.net/profile/Stuart_Rose/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents/links/55071c570cf27e990e04c8bb.pdf&quot;&gt;RAKE&lt;/a&gt; (Rapid Automatic Keyword Extraction) and &lt;a href=&quot;https://github.com/LIAAD/yake&quot;&gt;YAKE!&lt;/a&gt; (Yet Another Keyphrase Extraction). The reason I chose these two methods is because they are very fast while still producing decent results. At NLMatics, there is often a large number of long documents to be searched by the clients, so speed is most definitely a primary concern. Because of this need for speed and thus the relative simplicity of the algorithms, it is important to note that these methods may cut off phrases in the wrong places, return some not-so-relevant phrases, or miss important phrases entirely, etc. At the end of the day though, I think the speed is worth it!&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/speedmeme.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;This is us when we use RAKE and YAKE.&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Another note is that the reason that we are using both RAKE and YAKE is in the hopes that combining the keyphrases extracted through both of these methods on the same text will provide us with more, higher quality keyphrases. You’ll see below how we will use the keyphrases when clustering sentences based on meaning, but suffice to say that combining the two helps preserve meaning by drowning out the irrelevant ones and increasing the number of relevant ones.&lt;/p&gt;

&lt;p&gt;RAKE is a fairly simple algorithm introduced in 2010 that makes use of the observation that good keyphrases rarely include stopwords. First, it uses stopwords and phrase delimiters to split up the text into potential keyphrases. Then, a score is calculated by taking the ratio of degree (defined as the sum of how many times it occurs next to other words in the text) to word frequency for every word in the phrase and summing them to get the phrase’s overall score. Here, I used the &lt;a href=&quot;https://github.com/csurfer/rake-nltk&quot;&gt;RAKE-NLTK&lt;/a&gt; implementation, which takes advantage of the power of NLTK.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/rakeexample.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;An example of RAKE-extracted keyphrases and their scores. High scores represent high relevance.&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;YAKE is another keyword extraction technique (YAKE literally stands for “Yet Another Keyphrase Extractor”) which makes use of a few more word features to calculate a score. YAKE starts by calculating a score for each individual word using this set of five features: casing (ratio of uppercase and lowercase to overall count), position (favors words near beginning of document), relatedness to context (computes number of terms to the left/right), sentence difference (how often the word appears in different sentences). A sliding window of 3-grams (i.e. three-word-long phrases) is then used to determine the lowest scoring (i.e. most meaningful) possible keyphrases.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/yakeexample.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;An example of YAKE-extracted keyphrases and their scores. Low scores represent high relevance.&lt;/i&gt;&lt;/div&gt;

&lt;h3 id=&quot;using-rake-and-yake&quot;&gt;USING RAKE AND YAKE&lt;/h3&gt;
&lt;p&gt;I would recommend following along in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;notebook.ipynb&lt;/code&gt; from the repository above. Alternatively, you can copy and paste these code blocks into your own new notebook. First, install and import these two libraries:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!pip install rake_nltk
!pip install yake
from rake_nltk import Rake
import yake
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Initialize:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Initialize
r = Rake(min_length=1, max_length=4) # defining min and max length for keyphrases
y = yake.KeywordExtractor()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are other parameters you can adjust if you so desire, such as providing your own stopwords for RAKE and n-gram size for YAKE. Check the official &lt;a href=&quot;https://csurfer.github.io/rake-nltk/_build/html/advanced.html&quot;&gt;RAKE NLTK usage details&lt;/a&gt; and &lt;a href=&quot;https://github.com/LIAAD/yake#specifying-parameters&quot;&gt;YAKE documentation&lt;/a&gt; for more on those.&lt;/p&gt;

&lt;p&gt;Let’s initialize some sample sentences (again from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Harry_Potter&quot;&gt;Harry Potter Wikipedia page&lt;/a&gt;):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Create a list of sample sentences
sentences = []
sentences.append(&quot;Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, critical acclaim and commercial success worldwide.&quot;)
sentences.append(&quot;The Harry Potter novels are mainly directed at a young adult audience as opposed to an audience of middle grade readers, children, or adults.&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And use RAKE and YAKE on them!:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# For each sentence, print the keyphrases that RAKE and YAKE extract
for sentence in sentences:
    print(&quot;sentence: &quot;, sentence)	 
r.extract_keywords_from_text(sentence)
rake_keywords = r.get_ranked_phrases_with_scores()
print(&quot;rake keyphrases: &quot;)
for kw in rake_keywords:
    print(kw)
yake_keywords = y.extract_keywords(sentence)
print(&quot;yake keyphrases: &quot;)
for kw in yake_keywords:
    print(kw)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s that simple! As you can see, the keyphrases aren’t perfect. To help get rid of some of the noise, we’re going to set a cutoff. You can play around with the number more if you’d like, but I decided to use 3 as a cutoff for RAKE and 0.4 for YAKE. We’ll also define a function that returns a list of the phrases rather than just printing them out for this so that we can use it later:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Define a function which takes in text and returns a list of keyphrases
def get_keyphrases(text):
    # yake has some trouble dealing with empty strings
    if text.strip() == &quot;&quot;:
         return [&quot;&quot;]

  r.extract_keywords_from_text(text)
  rake_keywords = r.get_ranked_phrases_with_scores()
  yake_keywords = y.extract_keywords(text)
  keyphrases = []
  for kw in rake_keywords:
      if kw[0] &amp;gt; 3:
          keyphrases.append(kw[1])
  for kw in yake_keywords:
      if kw[0] &amp;lt; 0.4:
          keyphrases.append(kw[1])
  return list(set(keyphrases)) # to remove duplicates
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A bit better! There are still some inconsistencies that we would be able to fix up with different heuristics, but for now, I think that the keyphrases give an acceptable overview of our example text.&lt;/p&gt;

&lt;h2 id=&quot;visualization&quot;&gt;VISUALIZATION&lt;/h2&gt;
&lt;h3 id=&quot;keppler-mapper-without-modifications&quot;&gt;KEPPLER MAPPER WITHOUT MODIFICATIONS&lt;/h3&gt;

&lt;p&gt;Next, we’ll move onto visualization of these extracted keyphrases. We’ll be using &lt;a href=&quot;https://kepler-mapper.scikit-tda.org/&quot;&gt;Kepler Mapper&lt;/a&gt;, a scikit library that provides the functionality to project data visually, to cluster it, and to create connections between it. This project is inspired by &lt;a href=&quot;https://kepler-mapper.scikit-tda.org/notebooks/KeplerMapper-Newsgroup20-Pipeline.html&quot;&gt;this Kepler Mapper &amp;amp; NLP example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, we’ll run Keppler Mapper without modifications. Note that if you are following along using the code in the repository above, since everything in there is already modified, running it will give you the final product and not these intermittent steps I will be talking about. But I have included screenshots every so often so hopefully, that will help clear up what each step does.&lt;/p&gt;

&lt;p&gt;There will be a lot of steps and it is not necessary to know all the intricate details of each one, but understanding them at a high level will be helpful. Basically, we first need to project the data down into two dimensions so that it can be seen with the human eye on a screen. Then, we cluster it to create our nodes of varying sizes that represent an area of related data. Finally, we label the nodes with the appropriate identifying label.&lt;/p&gt;

&lt;p&gt;First, you’ll have to &lt;a href=&quot;https://github.com/scikit-tda/kepler-mapper&quot;&gt;download Kepler Mapper from source&lt;/a&gt;, since we’ll be modifying some of the code for our purposes. Clone into your working directory:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/MLWave/kepler-mapper
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You now have the Kepler Mapper code locally, so we can go in and modify it later. But first, we’ll run an example of what a normal Kepler Mapper visualization would look like without modifications to the frontend, and then we’ll adapt it for our purposes.&lt;/p&gt;

&lt;p&gt;As mentioned above, we’ll be using the &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups&quot;&gt;20 newsgroup dataset&lt;/a&gt;, which scikit conveniently provides.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Imports
import kmapper as km
import numpy as np
!pip install scikit-learn
from sklearn.datasets import fetch_20newsgroups
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Make sure to move the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kepler-mapper/kmapper&lt;/code&gt; folder (you won’t need any other folders from the repository besides this one) to the same directory as your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;notebook.ipynb&lt;/code&gt; so that your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import kmapper as km&lt;/code&gt; statement can find it.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Get the 20newsgroup dataset
newsgroups = fetch_20newsgroups(subset='train')
raw_data = newsgroups.data
X, category, category_names = np.array(raw_data), np.array(newsgroups.target), np.array(newsgroups.target_names)
print(&quot;SAMPLE ENTRY: &quot;, X[0])
print(&quot;SHAPE: &quot;, X.shape)
print(&quot;SAMPLE CATEGORY: &quot;, category[0])
print(&quot;SAMPLE CATEGORY NAME: &quot;, category_names[category[0]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have extracted three lists named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;category&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;category_names&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From the “SAMPLE ENTRY” line, you can see what &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt;, our dataset, looks like–various emails with headers.&lt;/li&gt;
  &lt;li&gt;The “SHAPE” line shows that the dataset is quite large, with 11,314 entries.&lt;/li&gt;
  &lt;li&gt;The “CATEGORY” is simply an integer ID for each category that is used to index &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;category_names&lt;/code&gt; which has the category stored as a string; for example, category 7 is “rec.autos” means that it’s a recreational email about automobiles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ll be using a subset of the entire dataset and cleaning it up a little bit, including removing the header from these emails. Then, we’ll run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_keyphrases&lt;/code&gt; function we created above on the data to get our RAKE and YAKE phrases.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Format data -- get rid of email headers, new lines and tabs, and then create joined &quot;sentences&quot; for each sentence
import re
headers = [&quot;Lines: &quot;, &quot;NNTP-Posting-Host: &quot;, &quot;NNTP Posting Host: &quot;]
data_formatted = []
for entry in raw_data[:1000]:
occurrences = []
for header in headers:
    occurrences.append(entry.lower().find(header.lower()))
champ = max(occurrences)
if champ != -1:
    start = entry.find(&quot;\n&quot;, champ)
    data_formatted.append(entry[start:])
else:
    data_formatted.append(entry)
data_formatted = [re.sub(&quot;[\n\t-]&quot;, &quot; &quot;,
entry) for entry in data_formatted]
extracted_phrases = [get_keyphrases(entry)
for entry in data_formatted]
    print(&quot;SAMPLE EXTRACTED PHRASES: &quot;, extracted_phrases[0])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Now this is where our keyphrases come in.&lt;/strong&gt; We want to preserve as much meaning as possible from the sentence when we project it down into two dimensions and cluster them. In many cases, raw sentences have a lot of junk in them. We can see this even with our example sentence from above:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;“&lt;b&gt;The&lt;/b&gt; central character &lt;b&gt;in the&lt;/b&gt; series &lt;b&gt;is&lt;/b&gt; Harry Potter, &lt;b&gt;a&lt;/b&gt; boy &lt;b&gt;who&lt;/b&gt; lives &lt;b&gt;in the&lt;/b&gt; fictional town &lt;b&gt;of&lt;/b&gt; Little Whinging, Surrey &lt;b&gt;with his&lt;/b&gt; aunt, uncle, &lt;b&gt;and&lt;/b&gt; cousin.”&lt;/div&gt;

&lt;p&gt;Out of the 27 words in the sentences, 12 of them are stopwords (bolded above). That’s almost half (!!!) of the words that essentially add no meaning to the sentence!&lt;/p&gt;

&lt;p&gt;What we’ll do to overcome this is create “sentences” from our extracted keyphrases; i.e. stringing together all of the keyphrases for a given entry. This will emphasize the meaningful bits in it and de-emphasize the not-so-meaningful ones. Let’s use a shorter example; the sentence:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;“There is a big black cat.”&lt;/div&gt;

&lt;p&gt;gets the keyphrases:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;“big black cat”, “black cat”&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;when passed in to our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_keyphrases&lt;/code&gt; function above. The joined “sentence” would then become “big black cat black cat,” essentially boiling the sentence down into its core meaning. We will use these for our projections. The below code will print an example from our dataset:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;extracted_phrases_joined = [&quot; &quot;.join(phrase) for phrase in extracted_phrases]
print(&quot;SAMPLE ORIGINAL SENTENCE: &quot;, data_formatted[0])
print(&quot;SAMPLE JOINED SENTENCE: &quot;, extracted_phrases_joined[0])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, we’ll start the process of projecting these meaningful “sentences.” We’ll use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TfidfVectorizer&lt;/code&gt; to convert our documents into a matrix of TF-IDF features, then perform dimensionality reduction with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TruncatedSVD&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Isomap&lt;/code&gt;. Finally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MinMaxScaler&lt;/code&gt; scales them to an appropriate size.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Project all of the &quot;sentences&quot; down into a 2D representation
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.manifold import Isomap
from sklearn.preprocessing import MinMaxScaler

mapper = km.KeplerMapper(verbose=2)

projected_X = mapper.fit_transform(np.array(extracted_phrases_joined),
projection=[TfidfVectorizer(analyzer=&quot;char&quot;,
                            ngram_range=(1,6),
                            max_df=0.83,
                            min_df=0.05),
            TruncatedSVD(n_components=100,
                         random_state=1729),
            Isomap(n_components=2,
                   n_jobs=-1)],
scaler=[None, None, MinMaxScaler()])

print(&quot;SHAPE&quot;,projected_X.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;projected_X&lt;/code&gt;, a list of 2D representations of each of our sentences, we’re going to cluster the sentences. The clustering algorithm is pretty important here, as different algorithms are suited for different purposes; you can learn more about scikit’s different clustering algorithms &lt;a href=&quot;https://scikit-learn.org/stable/modules/clustering.html&quot;&gt;here&lt;/a&gt;. KeplerMapper uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AgglomerativeClustering&lt;/code&gt; in their example, but that ends up with a large number of clusters which is hard to look at and visually take in. Feel free to play around with the algorithms to see what best suits your needs, but here we’ll be using &lt;a href=&quot;https://scikit-learn.org/stable/modules/clustering.html#dbscan&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DBSCAN&lt;/code&gt;&lt;/a&gt;, which is characterized by uneven clusters and non-flat geometry (i.e. shapes that aren’t just circles or squares). Here’s a comparison of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AgglomerativeClustering&lt;/code&gt; vs. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DBSCAN&lt;/code&gt; on our data.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/dbscan.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;DBSCAN on the newsgroup20 dataset.&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/agglomerative.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;AgglomerativeClustering on the newsgroup20 dataset.&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, I had to zoom out much more to fit all of the points in with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AgglomerativeClustering&lt;/code&gt; version. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DBSCAN&lt;/code&gt; seems to be a lot more manageable for our purposes of wanting to see information at a glance. So here is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DBSCAN&lt;/code&gt; clustering code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Cluster the projected data
from sklearn import cluster
graph = mapper.map(projected_X, clusterer=cluster.DBSCAN(eps=0.5, min_samples=3))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we’re going to get TF-IDF wordgrams (1-3) to help us identify each cluster. Essentially, we’ll find the phrases of one to three words that are not too common but at the same time not too uncommon (hence the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_df&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_df&lt;/code&gt; parameters, i.e. the cut-offs for document frequency values) and later the Kepler Mapper code will calculate some statistics on it that you’ll see in the side info panel.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Get counts of phrases from data (to be used for the label and for cluster statistics)
vec = TfidfVectorizer(analyzer=&quot;word&quot;,
                  strip_accents=&quot;unicode&quot;,
                  stop_words=&quot;english&quot;,
                  ngram_range=(1,3),
                  max_df=0.97,
                  min_df=0.02)

interpretable_inverse_X = vec.fit_transform(X).toarray()
interpretable_inverse_X_names = vec.get_feature_names()

print(&quot;SHAPE&quot;, interpretable_inverse_X.shape)
print(&quot;FEATURE NAMES SAMPLE&quot;,
interpretable_inverse_X_names[:400])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we’ll put it all together by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;visualize()&lt;/code&gt; function. It will write to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;path_html&lt;/code&gt; filename you provide it. We’ll see our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;category_names&lt;/code&gt; in the panel on the left-side, and our nodes will be colored according to their categories.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Visualize data!
html = mapper.visualize(graph,
	X=interpretable_inverse_X,
	X_names=interpretable_inverse_X_names,
	path_html=&quot;newsgroups20.html&quot;,
	lens=projected_X,
	lens_names=[&quot;ISOMAP1&quot;, &quot;ISOMAP2&quot;],
	title=&quot;Visualizing Text - Newsgroup20&quot;,
  custom_tooltips=np.array([category_names[ys] for ys in category]),
	color_function=category)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The very last cell also gives you the option to open the file in the notebook.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Open visualization here in the notebook
from kmapper import jupyter
jupyter.display(path_html=&quot;newsgroups20.html&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s it! If you’re following along in the repository, the screenshot below is the outcome of the process so far. If you’re creating your own file from scratch with these code blocks, go ahead and run the whole kernel (it may take a while due to the clustering and projection), and you should see something like this at the bottom. Or, if you prefer, you can open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;newsgroup20.html&lt;/code&gt; in your preferred web browser.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/unmodifiedkm.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;Default Kepler Mapper on the newsgroup20 dataset using DBSCAN clustering.&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;It already looks really cool! Feel free to play around with it; zoom in and out, open the toggles in the navigation bar, hover on nodes, etc.&lt;/p&gt;

&lt;h3 id=&quot;kepler-mapper-with-modifications&quot;&gt;KEPLER MAPPER WITH MODIFICATIONS&lt;/h3&gt;

&lt;p&gt;However, one drawback is that you can’t really get much information at a glance from this screen. It’s hard to glean any sort of relationship between the data without knowing what the nodes represent! What we’re going to do next is modify the Kepler Mapper code in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmapper&lt;/code&gt; folder so that each node above retains its relative size to other nodes (based on how many members are in the cluster) and also make it big enough so that a defining label for each node can fit.&lt;/p&gt;

&lt;p&gt;Please note that the defining label we will be using is the &lt;strong&gt;most common word or phrase&lt;/strong&gt; in all of the sentences in that cluster. This generally provides a good label for the cluster as a whole, as opposed to running RAKE or YAKE on the sentences in the cluster which, as I found in my experimentation, tends to focus on one or two entries in the cluster. The sentences in a cluster do have overlap in meaning (that’s why they’re clustered together), but oftentimes it might not be clear enough to simple techniques like RAKE/YAKE why this is so. Generally, these automatic techniques are run on one contained document, not a group of documents. All that is to say, we will be using the most frequent phrase as a label so that we can get an idea of the cluster as a whole.&lt;/p&gt;

&lt;p&gt;Navigate to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmapper&lt;/code&gt; directory. This is where all of the relevant source code we will be working with is. First, we’ll change the size of the nodes so that they are large enough to hold text. Go into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;visuals.py&lt;/code&gt; and find the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_size_node&lt;/code&gt; function in line 406, which is where the size of each node is defined based on how many members are in the cluster. It should look like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _size_node(member_ids):
    return int(np.log(len(member_ids) + 1) + 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Change it to:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _size_node(member_ids):
    return 35 * int(np.log(len(member_ids) + 1) + 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you were to run again at this point, you would see something like the screenshot below.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/halfmodifiedkm.png&quot; /&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;i&gt;Kepler Mapper after adjusting node size.&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The nodes would appear bigger, but still at a good relative ratio with each other. As you can see, one problem is that there’s some overlap between nodes. We can play around with the forces a bit to try to fix this. If you navigate to line 127, you should see the force settings:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Force settings
var force = d3.layout.force()
        .linkDistance(5)
        .gravity(0.2)
        .charge(-1200)
        .size([w,h]);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For this dataset, I’ve found that changing the &lt;a href=&quot;http://bl.ocks.org/sathomas/191a8a302a363ac6a4b0&quot;&gt;gravity&lt;/a&gt; from 0.2 to 0.1 and &lt;a href=&quot;http://bl.ocks.org/sathomas/1ca23ee9588580d768aa&quot;&gt;charge&lt;/a&gt; from -1200 to -1800 repels the clusters enough to mostly get rid of the overlap problem. As a side note, Keppler Mapper actually runs on an outdated version of d3, so I wouldn’t worry too much about understanding the forces because in the most current release, these are basically deprecated anyways and there’s a cool function called &lt;a href=&quot;https://www.d3indepth.com/force-layout/#forcemanybody&quot;&gt;forceManyBody()&lt;/a&gt; that just gets rid of all overlap! But generally, a negative charge represents more repelling of nodes, and gravity closer to 0 allows the nodes to wander further apart.&lt;/p&gt;

&lt;p&gt;Next, we’ll add the label of the top feature, i.e. most common phrase, to each node. Find the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;format_mapper_data&lt;/code&gt; function around line 166, which formats the data that will be passed to the HTML template. You can see that each node has this information:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n = {
        &quot;id&quot;: &quot;&quot;,
        &quot;name&quot;: node_id,
        &quot;color&quot;: c,
        &quot;type&quot;: t,
        &quot;size&quot;: s,
        &quot;tooltip&quot;: tt,
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’re going to add a field called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature&lt;/code&gt; which represents the top occurring feature, and we’ll write this on the node later in the frontend.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cluster_stats = _format_cluster_statistics(member_ids, X, X_names)
f = cluster_stats['above'][0]['feature']

n = {
        &quot;id&quot;: &quot;&quot;,
        &quot;name&quot;: node_id,
        &quot;color&quot;: c,
        &quot;type&quot;: t,
        &quot;size&quot;: s,
        &quot;tooltip&quot;: tt,
        &quot;feature&quot;: f
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster_stats[‘above’]&lt;/code&gt; is a list of the “above-average features”, meaning the features i.e. phrases that occur more than an average amount of times. We’re taking the most common one and feeding it into the frontend.&lt;/p&gt;

&lt;p&gt;Now, we can go into the JavaScript and make the feature appear. Kepler Mapper was built using &lt;a href=&quot;https://d3js.org/&quot;&gt;d3&lt;/a&gt;, which is a JavaScript library that “helps you bring data to life using HTML, SVG, and CSS.” In terms of this application, it is taking care of a lot of the interactivity, the physics, etc. The creator of d3 has a very thorough &lt;a href=&quot;https://observablehq.com/collection/@d3/learn-d3&quot;&gt;tutorial&lt;/a&gt; (it’s also interactive!) if you want to learn it from the ground up, but we won’t need to know much about it for this. I would recommend looking into it because you can make some really beautiful visualizations.&lt;/p&gt;

&lt;p&gt;At line 194, you’ll see this code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Draw circles
var circle = node.append(&quot;path&quot;)
  .attr(&quot;d&quot;, d3.svg.symbol()
    .size(function(d) { return d.size * 50; })
    .type(function(d) { return d.type; }))
  .attr(&quot;class&quot;, &quot;circle&quot;)
  .style(tocolor, function(d) {
    console.log(&quot;Node color:&quot;, d.color);
    console.log(&quot;becomes color &quot;, color(d.color));
    return color(d.color);
  });
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the code that creates the visual nodes we see. It gives each a size (note the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d.size * 50&lt;/code&gt;, we’ll make use of that later), a type attribute, a class, and colors it according to the provided color function (the color scheme is &lt;a href=&quot;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html#the-color-scales&quot;&gt;viridis&lt;/a&gt;). We’ll add code right after this to put text labels on top of this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Add labels
var label = node.append(&quot;text&quot;)
    .attr(&quot;dy&quot;, &quot;.35em&quot;)
    .attr(&quot;fill&quot;, &quot;white&quot;)
    .style(&quot;font-family&quot;, &quot;Roboto&quot;)
    .style(&quot;font-weight&quot;, &quot;300&quot;)
    .text(d =&amp;gt; d.feature)
    .style(&quot;font-size&quot;, function(d) {
        var length = this.getComputedTextLength(); // supposedly returns a pixel integer
        var available = Math.pow((d.size * 50 / Math.PI), 0.5) * 2 - 18; // d.size is the squared area
        return (available / length) + &quot;em&quot;;
    });
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To break this chunk of code down:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first line adds text to each node.&lt;/li&gt;
  &lt;li&gt;The second centers it vertically; “em” is a unit of measurement which is equal to the point size of the current font, i.e. in size 12 font an em would be 12 pixels. Basically, this line centers vertically for your current font size.&lt;/li&gt;
  &lt;li&gt;Then we make it white, Roboto, and adjust its weight, i.e. its boldness.&lt;/li&gt;
  &lt;li&gt;Then, we add the text itself by returning the property called feature that we just added for each node.&lt;/li&gt;
  &lt;li&gt;All that’s left is the font size, which is a bit trickier but nothing crazy. The idea is that we have the size of each node that was passed in from the backend, which when multiplied by 50 (in line 197 if you recall from above, not sure why they designed it this way but it works I suppose) is the area it takes up in pixels, as well as the length of our text label, also in pixels. We then find the diameter of the node, subtract 18 to account for padding from the shadow on the outside of the pixel, then use the ratio of that available diameter to the length of our text as the em value for our font size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, we have to go up to line 45 and make sure that all text is anchored to the middle, i.e. centered. Change this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var svg = d3.select(&quot;#canvas&quot;).append(&quot;svg&quot;)
      .attr(&quot;width&quot;, width)
      .attr(&quot;height&quot;, height);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var svg = d3.select(&quot;#canvas&quot;).append(&quot;svg&quot;)
      .attr(&quot;width&quot;, width)
      .attr(&quot;height&quot;, height)
      .attr(&quot;text-anchor&quot;, &quot;middle&quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And that’s it! Refresh your kernel and run all cells, and you should get something like this (or just go ahead and run without refreshing if you’re using my &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;notebook.ipynb&lt;/code&gt;). A colorful, interactive way of visualizing the most meaningful clusters of your data.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;http://localhost:4000/site_files/2020-08-07-How-to-Extract-Keyphrases-and-Visualize-Text/visualizekeyphrases.gif&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Again, here’s the link to a &lt;a href=&quot;https://github.com/cznlm/visualize-keyphrases&quot;&gt;repository with the relevant code&lt;/a&gt; if you wish to download and run directly.&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&quot;further-directions&quot;&gt;FURTHER DIRECTIONS&lt;/h2&gt;
&lt;p&gt;There are a few of clear directions from here, namely working on reducing noise, fine-tuning the clustering algorithm, and improving the label quality. There are a lot of “junk” keyphrases that we get from RAKE and YAKE, so looking into a more advanced technique might be helpful to help us create even more meaningful “sentences” to project. Also, the choice of clustering algorithm we use on these projections is also relatively arbitrary. Finally, as you can see from the demo GIF, the labels themselves are far from perfect. It would be great to have a representative keyphrase of an entire cluster as opposed to just the top feature. However, it seems that the speedy automatic keyphrase extraction techniques we were using in this article are not suited for this purpose, so looking into more advanced techniques would be a great future direction as well.&lt;/p&gt;

&lt;h2 id=&quot;resources-compiled&quot;&gt;RESOURCES COMPILED&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://qwone.com/~jason/20Newsgroups/&quot;&gt;20 Newsgroup Dataset&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.05044.pdf&quot;&gt;A 2019 Keyphrase Extraction Literature Review&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/P14-1119.pdf&quot;&gt;A 2014 Keyphrase Extraction Literature Review&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0DzcUXkn9HY&amp;amp;t=548s&quot;&gt;An interesting lecture on keyphrase extraction methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Stuart_Rose/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents/links/55071c570cf27e990e04c8bb.pdf&quot;&gt;RAKE paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Stuart_Rose/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents/links/55071c570cf27e990e04c8bb.pdf&quot;&gt;YAKE paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/P17-1102/&quot;&gt;PositionRank, another cool technique that I tried out&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/clustering.html&quot;&gt;Scikit-learn’s clustering algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kepler-mapper.scikit-tda.org/&quot;&gt;Keppler Mapper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://observablehq.com/collection/@d3/learn-d3&quot;&gt;Learn d3 Tutorial Series&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bl.ocks.org/sathomas/11550728&quot;&gt;Understanding d3 v3 Forces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.d3indepth.com/force-layout/&quot;&gt;Understanding d3 v5 Forces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&quot;&gt;viridis color palettes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Cheyenne Zhang</name></author><summary type="html">by Cheyenne Zhang</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/kepmapthumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/kepmapthumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Smooth Inverse Frequency (SIF) Embeddings in Golang</title><link href="http://localhost:4000/nlp/sentence-embeddings/2020/08/07/Smooth-Inverse-Frequency-Frequency-(SIF)-Embeddings-in-Golang.html" rel="alternate" type="text/html" title="Smooth Inverse Frequency (SIF) Embeddings in Golang" /><published>2020-08-07T18:45:00+09:00</published><updated>2020-08-07T18:45:00+09:00</updated><id>http://localhost:4000/nlp/sentence-embeddings/2020/08/07/Smooth%20Inverse%20Frequency%20Frequency%20(SIF)%20Embeddings%20in%20Golang</id><content type="html" xml:base="http://localhost:4000/nlp/sentence-embeddings/2020/08/07/Smooth-Inverse-Frequency-Frequency-(SIF)-Embeddings-in-Golang.html">&lt;h4 id=&quot;by-daniel-ye&quot;&gt;by Daniel Ye&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Daniel Ye&lt;/em&gt;&lt;/strong&gt; is a sophomore at Cornell University majoring in Computer Science and minoring in Operations Research and Information Engineering. I am interested in machine learning, natural language processing, and data science. I have worked on projects involving manufacturing data collection/analysis, greenhouse environmental regulation, and a multiplayer programming game. In my free time I enjoy playing tennis, running, hiking, and playing cello or guitar.Daniel was one of NLMatics’ 2020 summer interns.
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Smooth Inverse Frequency (SIF) Embeddings in Golang&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Daniel Ye&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#golang&quot;&gt;Why GoLang?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#grpc&quot;&gt;GRPC Improvements&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#postprocess&quot;&gt;Post-Processing Improvements&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#docker&quot;&gt;Productionalizing With Docker&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;a name=&quot;introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am a rising junior majoring in computer science and minoring in operations research and information engineering at Cornell Engineering. This summer, I interned at NLMatics, and one of the projects I worked on was implementing a &lt;a href=&quot;https://openreview.net/pdf?id=SyK00v5xx#page=12&amp;amp;zoom=100,110,217&quot;&gt;Smooth Inverse Frequency&lt;/a&gt; model using Golang. This is able to calculate sentence embeddings from sequences of words in the form of vectors, which mathematically represent the meaning of the sentence. We use it to encode documents and queries into embeddings which are then processed further using other natural language processing models to get search results. However, our original Python implementation was fairly slow at calculating these embeddings, and it scaled poorly with increasing document sizes or concurrent requests, so we needed to find a way to speed up the service.&lt;/p&gt;

&lt;p&gt;Over the course of about four weeks, I worked on combating this issue by developing a Golang implementation of the model and switching from HTTP 1.0 protocol to gRPC protocol for the server. This increased the amount of concurrent processing we were able to utilize, and reduced overhead for connecting and sending requests to the server, speeding up the service greatly. Ultimately, I was able to build a system that generated more accurate sentence embeddings at much faster speeds.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a name=&quot;motivation&quot;&gt;&lt;/a&gt;2. Motivation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Word embeddings are one of the most important developments in the field of modern Natural Language Processing. Translating the meaning behind words and the semantic relationships between them into measurable quantities is a crucial step in processing language. Many words, such as “cat” and “dog” or “Mozart” and “Beethoven” have almost no physical characteristics that would reveal their similarities. Instead, modern algorithms like Google’s &lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;Word2Vec&lt;/a&gt; developed in 2013 or Stanford’s &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;&gt;GloVe&lt;/a&gt; essentially count the cooccurrences of words with other words, and condense these values into dense, relatively low-dimensional vectors. Their models train on massive corpora of English text such as all of Wikipedia, and embed words as vectors based on which other words they appear in proximity to. So, if “cat” and “dog” are found together in many sentences or documents, they will have very similar vector values. This method is able to capture not only semantic similarity, but also analogies (woman is to man as king is to __) and the effects of prefixes or suffixes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/daniels_post/image_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;semantic relationships represented by Word2Vec and GloVe&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A natural next step in the field was the development of sentence embeddings, or being able to extract meaning from a sequence of words. Early methods include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;TF-IDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.08198.pdf&quot;&gt;Paragram Phrase (PP)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;Recurrent Neural Network (RNN)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.00075.pdf&quot;&gt;Long Short-Term Memory Networks (LSTM)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&quot;&gt;Deep Averaging Network (DAN)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In 2017, Arora et. al proposed SIF, or &lt;a href=&quot;https://openreview.net/pdf?id=SyK00v5xx#page=12&amp;amp;zoom=100,110,217&quot;&gt;Smooth Inverse Frequency&lt;/a&gt;, a weighting scheme to improve performance of sentence embeddings. When encoding a sentence, it is important to identify which words in the sentence are more significant. For example, if calculating the embedding of the sentence “who was Mozart?” the word “was” doesn’t add much meaning; looking for sentences or documents relating to the word “was” will not yield any useful results for the original question. It’s clear that “Mozart” holds the most meaning in the question from a human standpoint, but how do you program a machine to identify that? SIF operates under the assumption that the most important words tend to also be used less frequently. If you counted all the words in Wikipedia, the word “was” would most likely appear much more frequently than in “Mozart”. Weights of a word &lt;em&gt;w&lt;/em&gt; are computed by &lt;em&gt;a/(a + p(w))&lt;/em&gt; where a is a parameter and p(w) is the word frequency of w, which can be estimated by scraping a large corpus.* &lt;em&gt;The hyperparameter *a&lt;/em&gt; adjusts which words are quantitatively “common” and “uncommon.” Here is the formal algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/daniels_post/image_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Arora et. al found that despite its simplicity, SIF worked surprisingly well on semantic text similarity (STS), entailment, and sentiment tasks. STS tasks involve scoring pairs of sentences from 0-5 based on how similar their meanings are, which are then checked against a golden standard of human generated scores. For example, “The bird is bathing in the sink” and “Birdie is washing itself in the water basin” should receive a 5. Entailment tasks involve identifying if one sentence &lt;em&gt;entails&lt;/em&gt; that another one is true. For example, if you read the sentence “There is a soccer game with multiple males playing,” you could infer that the sentence “Several men are playing a sport” is true. Thus the first sentence, commonly referred to as the &lt;em&gt;text&lt;/em&gt; entails the following, also known as the &lt;em&gt;hypothesis&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/daniels_post/image_2.png&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;/site_files/daniels_post/image_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tables detailing SIF performance on various semantic tasks. “GloVe + WR”, “PSL + WR”, and “Ours” correspond to SIF systems.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Due to its effectiveness and simplicity, SIF is an incredibly practical method of embedding sentences for commercial or enterprise products that rely on both accurate and fast results while consuming low amounts of resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a name=&quot;golang&quot;&gt;&lt;/a&gt;3. Why Golang?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/PrincetonML/SIF&quot;&gt;original code&lt;/a&gt; corresponding to the paper describing SIF is implemented in Python, which by design has a &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;Global Interpreter Lock (GIL)&lt;/a&gt;, a mutex that prevents multi threaded processes from utilizing multiple cores of a processor. We hosted our Cython implementation of the SIF embedding model on a cloud service, which provided us with multiple cores of processing power. However, the GIL meant that we could not make use of the full processing power we were paying for.&lt;/p&gt;

&lt;p&gt;GoLang however, has no such restrictions and also provides built-in structures for concurrency through the form of Goroutines, a form of very lightweight threads.  They are organized by channels, which allow goroutines to either block on awaiting input or signal that they have completed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;lt;table&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;func main() {
    jobs := make(chan int, 100)
    results := make(chan int, 100)
    for i := 0; i &amp;lt; 4; i++ {
        go worker(jobs, results)
    }
    for i := 0; i &amp;lt; 100; i++ {
        jobs &amp;lt;- i
    }
    close(jobs)
    for j := 0; j &amp;lt; 100; j++ {
        fmt.Println(&amp;lt;-results)
    }
}
func worker(jobs &amp;lt;-chan int, results chan&amp;lt;- int) {
    for n := range jobs {
        results &amp;lt;- fib(n)
    }
}

func fib(n int) int {
    if n &amp;lt;= 1 {
        return n
    }
    return fib(n-1) + fib(n-2)
}&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Basic architecture of a worker pool using goroutines&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is important to note that this structure does not enforce the order in which workers complete their jobs or the order in which their results are sent to the results channel. Executing this code won’t necessarily return the fibonacci numbers in their correct order.&lt;/p&gt;

&lt;p&gt;Our API allowed for clients to send multiple sentences at a time to be encoded by the SIF, often many at a time, and implementing the SIF in Golang allowed us to leverage powerful concurrency to speed up our calculations. My first iteration of the SIF server used &lt;a href=&quot;https://golang.org/pkg/net/http/&quot;&gt;Golang’s http package&lt;/a&gt; to host the model on an HTTP 1.0 server. I compared it with  our old system, as well as the original Python implementation. I used three different independent variables to benchmark how the performance scaled up: total words per ‘sentence’ to be embedded, total number of calls with fixed number of concurrent requests, and number of concurrent requests with fixed total number of calls. These benchmarks were made using &lt;a href=&quot;https://httpd.apache.org/docs/2.4/programs/ab.html&quot;&gt;Apache Benchmark&lt;/a&gt;, a software that allows you to make many concurrent requests to a server and reports significant timing data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/daniels_post/image_4.png&quot; class=&quot;graph&quot; /&gt;
&lt;img src=&quot;/site_files/daniels_post/image_7.png&quot; class=&quot;graph&quot; /&gt;
&lt;img src=&quot;/site_files/daniels_post/image_6.png&quot; class=&quot;graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The results were impressive, to say the least. At a sentence level, around 10-100 words, the new implementation outperforms the old by up to &lt;strong&gt;30x&lt;/strong&gt; and at a document level, around 1000-10000+ words, it is still &lt;strong&gt;10x&lt;/strong&gt;** **faster in almost every scenario. The improved speed for sentence level embeddings is really useful, since it means that we can embed at lower levels of granularity much more easily. For example, if you were searching “who was Mozart” and only had embeddings for each document, your system might flag a document about Joseph Haydn, friend and mentor of Mozart, as relevant. As you descend levels of granularity of embeddings however, you can much more easily locate relevant information about your query. Perhaps it allows you to find a paragraph detailing their time in Vienna together, or a specific sentence about the pieces of music they worked on together.&lt;/p&gt;

&lt;p&gt;I found that Go works incredibly well for creating a web service capable of handling many concurrent requests efficiently. However, Go’s build in functionality does have a significant limitation of restricting HTTP requests to sizes of 1MB or less, which was not ideal for our use cases where we had to embed large amounts of text in a single request. For example, a legal company looking to process Google’s 2019 environmental and sustainability report would need about 11 MB of payload. Or a stressed-out undergraduate trying to find tips in &lt;em&gt;Cracking the Coding Interview&lt;/em&gt; would require around 90 MB of allowance. Additionally, every HTTP request requires a new connection to be made between the server and client, which adds a significant amount of overhead to our typical use case which often requires embedding many documents at once and sending many requests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a name=&quot;grpc&quot;&gt;&lt;/a&gt;4. GRPC Improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Developed by Google, gRPC is an open source Remote Procedure Call framework and is what I turned to in order to hopefully remove the size limit and connection overhead problems with Go’s http package. gRPC processes payloads from requests using buffers, so it removes the 1MB size cap on requests. It also maintains connections between individual clients, so a single user can make multiple requests without having to create a connection more than once. It has its own Interface Definition Language called protocol buffers that also serve as a mechanism for serializing structured data and uses HTTP/2 to transport data. gRPC services are defined in .proto files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;lt;table&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;syntax = &quot;proto3&quot;
option go_package = “greeter”
package main
// The greeter service definition.
service Greeter {
  // Sends a greeting
  rpc SayHello (HelloRequest) returns (HelloReply) {}
}

// The request message containing the user's name.
message HelloRequest {
  string name = 1;
}

// The response message containing the greetings
message HelloReply {
  string message = 1;
}
&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then use the protoc command to compile your service into any of &lt;a href=&quot;https://grpc.io/docs/languages/&quot;&gt;gRPC’s supported languages&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;lt;table&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
//  protoc-gen-go v1.23.0
//  protoc        v3.6.1
// source: greeter.proto

package greeter

import (
    proto &quot;github.com/golang/protobuf/proto&quot;
    protoreflect &quot;google.golang.org/protobuf/reflect/protoreflect&quot;
    protoimpl &quot;google.golang.org/protobuf/runtime/protoimpl&quot;
    reflect &quot;reflect&quot;
    sync &quot;sync&quot;
)

const (
    // Verify that this generated code is sufficiently up-to-date.
    _ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
    // Verify that runtime/protoimpl is sufficiently up-to-date.
    _ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// This is a compile-time assertion that a sufficiently up-to-date version
// of the legacy proto package is being used.
const _ = proto.ProtoPackageIsVersion4

// The request message containing the user's name.
type HelloRequest struct {
    state         protoimpl.MessageState
    sizeCache     protoimpl.SizeCache
    unknownFields protoimpl.UnknownFields

    Name string `protobuf:&quot;bytes,1,opt,name=name,proto3&quot; json:&quot;name,omitempty&quot;`
}
//... excess code has been left out&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to how gRPC buffers data being received and sent, we no longer had to worry about size limits on requests to our server. gRPC servers also have a very helpful feature in that connections between client and server are maintained across multiple requests, whereas with HTTP requests, a new connection has to be established for every POST. As a result, I saw more improvements in performance as this overhead was removed from the new system:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/daniels_post/image_7.png&quot; class=&quot;graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The largest improvements are seen when requests have very small payloads, so the majority of time is spent on overhead from connecting to the server. However, once you get to larger payloads, the times converge to become about equal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a name=&quot;postprocess&quot;&gt;&lt;/a&gt;5. Post-Processing Improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A coworker sent me this &lt;a href=&quot;https://arxiv.org/pdf/1702.01417.pdf&quot;&gt;paper about post-processing word vectors&lt;/a&gt; in order to improve their representation of meaning. The algorithm essentially takes a list of pre-computed word vectors and performs principal component analysis on them, and then removes the top N components from every vector via Gram-Schmidt. Here is their formal algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/daniels_post/image_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Python’s Numpy and sklearn packages have all the built-in tools needed to implement this algorithm, which you can find the code for &lt;a href=&quot;https://github.com/daniel-ye137/WordVectorProcessing&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;lt;table&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;import numpy as np
from sklearn.decomposition import PCA
import argparse


parser = argparse.ArgumentParser(description='postprocess word embeddings')
parser.add_argument(&quot;file&quot;, help=&quot;file containing embeddings to be processed&quot;)
args = parser.parse_args()
N = 2
embedding_file = args.file
embs = []

#map indexes of word vectors in matrix to their corresponding words
idx_to_word = dict()
dimension = 0
#append each vector to a 2-D matrix and calculate average vector
with open(embedding_file, 'rb') as f:
    first_line = []
    for line in f:
        first_line = line.rstrip().split()
        dimension = len(first_line) - 1
        if dimension &amp;lt; 100 :
            continue
        print(&quot;dimension: &quot;, dimension)

        break
    avg_vec = [0] * dimension
    vocab_size = 0
    word = str(first_line[0].decode(&quot;utf-8&quot;))
    word = word.split(&quot;_&quot;)[0]
    # print(word)
    idx_to_word[vocab_size] = word
    vec = [float(x) for x in first_line[1:]]
    avg_vec = [vec[i] + avg_vec[i] for i in range(len(vec))]
    vocab_size += 1
    embs.append(vec)
    for line in f:
        line = line.rstrip().split()
        word = str(line[0].decode(&quot;utf-8&quot;))
        word = word.split(&quot;_&quot;)[0]
        idx_to_word[vocab_size] = word
        vec = [float(x) for x in line[1:]]
        avg_vec = [vec[i] + avg_vec[i] for i in range(len(vec))]
        vocab_size += 1
        embs.append(vec)
    avg_vec = [x / vocab_size for x in avg_vec]
# convert to numpy array
embs = np.array(embs)

#subtract average vector from each vector
for i in range(len(embs)):
    new_vec = [embs[i][j] - avg_vec[j] for j in range(len(avg_vec))]
    embs[i] = np.array(new_vec)

#principal component analysis using sklearn
pca = PCA()
pca.fit(embs)

#remove the top N components from each vector
for i in range(len(embs)):
    preprocess_sum = [0] * dimension
    for j in range(N):
        princip = np.array(pca.components_[j])
        preprocess = princip.dot(embs[i])
        preprocess_vec = [princip[k] * preprocess for k in range(len(princip))]
        preprocess_sum = [preprocess_sum[k] + preprocess_vec[k] for k in range(len(preprocess_sum))]
    embs[i] = np.array([embs[i][j] - preprocess_sum[j] for j in range(len(preprocess_sum))])

file = open(&quot;postprocessed_embeddings.txt&quot;, &quot;w+&quot;, encoding=&quot;utf-8&quot;)

#write back new word vector file
idx = 0
for vec in embs:
    file.write(idx_to_word[idx])
    file.write(&quot; &quot;)
    for num in vec:
        file.write(str(num))
        file.write(&quot; &quot;)
    file.write(&quot;\n&quot;)
    idx+=1
file.close()

print(&quot;Wrote: &quot;, len(embs), &quot;word embeddings&quot;)&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the new word embedding file, I saw meaningful improvements in semantic similarity tasks, similar to the results they found in their paper:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;lt;table&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;Dataset&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;k&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;Original&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;Post-Processed Vectors&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;MSR Paraphrase&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;k = 1&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;76.43&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;78.34&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;MS MARCO, 10000&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;k = 1&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;18.2002&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;20.4&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;MSR Paraphrase&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;k = 3&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;87.122&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;88.84&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;MS MARCO, 10000&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;k = 3&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;26.915&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;29.37&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;MSR Paraphrase&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;k = 5&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;89.37944567&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;90.92&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;MS MARCO, 10000&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;k = 5&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;33.05634374&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;36.2&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I benchmarked the system on semantic text similarities tests using  the MSR Paraphrase dataset, as well as the first 10000 entries of the MS MARCO dataset using p@k with k = 1, 3, and 5. These tasks involved identifying a sentence from a large pool of candidates as being semantically equivalent to another. p@k testing introduces some leeway into the testing, where the program chooses the top K candidates from the pool as potential matches instead of only getting one attempt at the correct answer. This kind of testing is more representative of a search engine, where you care not only about getting the single best result, but also a reasonable amount of relevant information.&lt;/p&gt;

&lt;p&gt;The post-processed vectors outperformed the original in every case. I also benchmarked our system using a word frequency file generated by fellow intern Connie Xu using a June snapshot of all of Wikipedia. It had a far more comprehensive vocabulary than our current word frequency file, with over 50x as many entries, but performance results were inconclusive. The results do indicate, however, that post-processed word vectors also increase performance of sentence embedding systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a name=&quot;docker&quot;&gt;&lt;/a&gt;6. Productionalizing With Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After finishing the code for our new SIF, my final step was to prepare it for production, which meant an inevitable encounter with Docker. Here is my Dockerfile, which I got from &lt;a href=&quot;https://www.callicoder.com/docker-golang-image-container-example/&quot;&gt;this tutorial:&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;lt;table&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td&amp;gt;# Start from the latest golang base image
FROM golang:latest

# Add Maintainer Info
LABEL maintainer=&quot;Daniel Ye &amp;lt;daniel.ye@nlmatics.com&amp;gt;&quot;

# Set the Current Working Directory inside the container
WORKDIR /app

# Copy go mod and sum files
COPY go.mod go.sum ./

# Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changed
RUN go mod download

# Copy the source from the current directory to the Working Directory inside the container
COPY . .

# Build the Go app
RUN go build -o main .

# Expose port 8080 to the outside world
EXPOSE 8080

# Command to run the executable
CMD [&quot;./main&quot;]&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Most components are fairly self-explanatory. It does require that you use Go modules to manage your dependencies, which you can read about &lt;a href=&quot;https://blog.golang.org/using-go-modules&quot;&gt;here&lt;/a&gt;. The steps I took to compile my code into a Docker image were as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create a go mod file in the root directory of your Golang project using go mod init &lt;project name=&quot;&quot;&gt;. Your project name can be anything you want, it does not have to correspond to any file or package names, although it probably should.&lt;/project&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Populate your go mod file with dependencies using go build. This will automatically detect the packages used by your project, and write them to your go mod file&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a Dockerfile in the root directory of your project&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Build your image using docker build -t &lt;project name=&quot;&quot;&gt; . Include the period!&lt;/project&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run your project with docker run -d -p 8080:8080 &lt;project name=&quot;&quot;&gt;&lt;/project&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;a name=&quot;conclusion&quot;&gt;&lt;/a&gt;7. Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Improving the SIF implementation was a really interesting project. There were a lot of fun challenges involved like solving the ordering of goroutines and dealing with concurrent writes to map. It was incredibly satisfying to run my own benchmarks and see quantitative improvements in performance go as high as 30x the original speed. Of course, more improvements can still be made. &lt;a href=&quot;https://arxiv.org/pdf/2005.09069.pdf&quot;&gt;This paper&lt;/a&gt; details how SIF embeddings of documents can be improved by producing and then combining topic vectors. Other models for embedding sentences such as &lt;a href=&quot;https://arxiv.org/pdf/1803.11175.pdf&quot;&gt;Universal Sentence Encoder&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/pdf/1908.10084.pdf&quot;&gt;Sentence-BERT&lt;/a&gt; have been developed in recent years as well and are able to outperform SIF in certain categories of NLP tasks.&lt;/p&gt;</content><author><name>Daniel Ye</name></author><summary type="html">by Daniel Ye</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/SIFthumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/SIFthumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Speed up requests: Asyncio for Requests in Python</title><link href="http://localhost:4000/requests/speedup/asyncio/python/2020/08/07/Speed-up-requests-Asyncio-for-Requests-in-Python.html" rel="alternate" type="text/html" title="Speed up requests: Asyncio for Requests in Python" /><published>2020-08-07T09:00:00+09:00</published><updated>2020-08-07T09:00:00+09:00</updated><id>http://localhost:4000/requests/speedup/asyncio/python/2020/08/07/Speed%20up%20requests%20-%20Asyncio%20for%20Requests%20in%20Python</id><content type="html" xml:base="http://localhost:4000/requests/speedup/asyncio/python/2020/08/07/Speed-up-requests-Asyncio-for-Requests-in-Python.html">&lt;h4 id=&quot;by-connie-xu&quot;&gt;by Connie Xu&lt;/h4&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt; 
Connie Xu is currently studying Computer Science at Princeton University, where she has sharpened her skills in Algorithms and Data Structures as well as Linear Algebra. She is interested in learning more about Natural Language Processing and Machine Learning applications. In her free time, she watches cooking videos or practices beatboxing. Connie was one of NLMatics’ 2020 summer interns. 
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;speed-up-requests-asyncio-for-requests-in-python&quot;&gt;Speed up requests: Asyncio for Requests in Python&lt;/h1&gt;

&lt;h4 id=&quot;dont-be-like-this&quot;&gt;Don’t be like this.&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://memegenerator.net/img/instances/78137468/my-code-cant-run-slow-if-i-never-write-it.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you have probably already noticed because you decided to visit this page, requests can take forever to run, so here’s a nice blog written while I was an intern at NLMatics to show you how to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt; to speed them up.&lt;/p&gt;

&lt;h2 id=&quot;what-is-asyncio&quot;&gt;What is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;It is a Python library that uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;async/await&lt;/code&gt; syntax to make code run asynchronously.&lt;/p&gt;
&lt;h2 id=&quot;what-does-it-mean-to-run-asynchronously&quot;&gt;What does it mean to run asynchronously?&lt;/h2&gt;
&lt;h3 id=&quot;synchronous-normal-vs-asynchronous-using-asyncio&quot;&gt;Synchronous (normal) vs. Asynchronous (using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt;)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Synchronous:&lt;/strong&gt; you must wait for the completion of the first task before starting another task.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Asynchronous:&lt;/strong&gt; you can start another task before the completion of the first task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/synchronous.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/asynchronous.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For more information on the distinction between concurrency, parallelism, threads, sync, and async, check out this &lt;a href=&quot;https://medium.com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d&quot;&gt;Medium article&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;simple-analogies&quot;&gt;Simple analogies&lt;/h2&gt;
&lt;h3 id=&quot;brick-and-mortar&quot;&gt;Brick and Mortar&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/brick-and-mortar.jpg&quot; width=&quot;500&quot; height=&quot;334&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Simon and Ash are building 5 walls of brick.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Simon builds one wall and waits for it to set before starting to build the next wall (synchronous).&lt;/li&gt;
  &lt;li&gt;Ash, on the other hand, starts building the next wall before the first one sets (asynchronous).  &lt;br /&gt;&amp;lt;/br&amp;gt;
Ash starts the next task whereas Simon &lt;strong&gt;waits&lt;/strong&gt;, so Ash (asynchronous) will finish faster.
The lack of &lt;strong&gt;&lt;em&gt;waiting&lt;/em&gt;&lt;/strong&gt; is the key to why asynchronous programming provides a performance boost.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A good coding use case would be when you have a lot of time-consuming requests lined up with the outputs independent of each other. request1 takes a while to finish running, so instead of waiting, you start request2, which doesn’t affect the output of request1.&lt;/p&gt;

&lt;h3 id=&quot;laundry&quot;&gt;Laundry&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/laundry.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Be wary that an asynchronous approach does not provide any performance boost when all the tasks are &lt;strong&gt;dependent&lt;/strong&gt; on each other. For example, if you are washing and drying clothes, you must wait for the clothes to finish washing first before drying them no matter what, because drying clothes is dependent on the output of the washing. There is no use in using an asynchronous approach, because the pipeline is just the same as a synchronous approach.&lt;/p&gt;

&lt;p&gt;The coding equivalent of this laundry example is when the output of request1 is used as the input in the request2.&lt;/p&gt;

&lt;p&gt;For a further look into when and when not to use asynchronous programming, check out this &lt;a href=&quot;https://stackify.com/when-to-use-asynchronous-programming/&quot;&gt;Stackify thread&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-syntax-do-i-need-to-know&quot;&gt;What syntax do I need to know?&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Syntax&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Example&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;async&lt;/td&gt;
      &lt;td&gt;Used to indicate which methods are going to be run asynchronously &lt;br /&gt; &amp;lt;p&amp;gt;	 → These new methods are called &lt;strong&gt;coroutines&lt;/strong&gt;.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/async.png&quot; width=&quot;651&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;await&lt;/td&gt;
      &lt;td&gt;Used to run a coroutine once an asynchronous event loop has already started running &lt;br /&gt; → &lt;code&gt;await&lt;/code&gt; can only be used inside a coroutine &lt;br /&gt; → Coroutines must be called with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;await&lt;/code&gt;, otherwise there will be a &lt;code&gt;RuntimeWarning&lt;/code&gt; about enabling &lt;code&gt;tracemalloc&lt;/code&gt;. &lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/await.png&quot; width=&quot;448&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;asyncio.run()&lt;/td&gt;
      &lt;td&gt;Used to start running an asynchronous event loop from a normal program &lt;br /&gt; → &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.run()&lt;/code&gt; cannot be called in a nested fashion. You have to use await instead. &lt;br /&gt; → &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.run()&lt;/code&gt; cannot be used if you are running the Python file in a Jupyter Notebook because Jupyter Notebook already has a running asynchronous event loop. You have to use await. (More on this in the Running the Code section)&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/run.png&quot; width=&quot;622&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;asyncio.create_task()&lt;/td&gt;
      &lt;td&gt;Used to schedule a coroutine execution &lt;br /&gt; → Does not need to be awaited &lt;br /&gt; → Allows you to line things up without actually running them first.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/create_task.png&quot; width=&quot;822&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;asyncio.gather()&lt;/td&gt;
      &lt;td&gt;Used to run the scheduled executions &lt;br /&gt; → Needs to be awaited &lt;br /&gt; → This is vital to the asynchronous program, because you let it know which is the next task it can pick up before finishing the previous one.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/gather.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If you are thirsting for more in-depth knowledge on asyncio, check out these links:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://realpython.com/async-io-python/&quot;&gt;Async IO in Python: A Complete Walkthrough&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/library/asyncio.html&quot;&gt;asyncio: Python Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But with that, let’s jump straight into the code.&lt;/p&gt;

&lt;p&gt;Follow along with the Python file and Jupyter Notebook in this &lt;a href=&quot;https://github.com/nlmatics/asyncio-for-requests&quot;&gt;github repo&lt;/a&gt; that I developed for this post!&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;h3 id=&quot;preliminary&quot;&gt;Preliminary&lt;/h3&gt;

&lt;p&gt;Get imports and generate the list of urls to get requests from. Here, I use &lt;a href=&quot;https://jsonplaceholder.typicode.com/&quot;&gt;this placeholder url&lt;/a&gt;. Don’t forget to do &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -r requirements.txt&lt;/code&gt; in the terminal for all the modules that you don’t have. Normal &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requests&lt;/code&gt; cannot be awaited, so you will need to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import requests_async&lt;/code&gt; to run the asynchronous code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;import requests, requests_async, asyncio, time

itr = 200
tag = 'https://jsonplaceholder.typicode.com/todos/'
urls = []
for i in range(1, itr):
    urls.append(tag + str(i))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;synchronous&quot;&gt;Synchronous&lt;/h3&gt;

&lt;p&gt;This is what some typical Python code for requests would look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def synchronous(urls):
    for url in urls:
        r = requests.get(url)
        print(r.json())
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;asynchronous&quot;&gt;Asynchronous&lt;/h3&gt;

&lt;h4 id=&quot;incorrect-alteration&quot;&gt;Incorrect Alteration&lt;/h4&gt;
&lt;p&gt;The following is an understandable but bad alteration to the synchronous code. The runtime for this is the same as the runtime for the synchronous method, because you have not created a list of tasks that the program knows it needs to execute together, thus you essentially still have synchronous code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;async def asynchronous_fail(urls):
    for url in urls:
        r = await requests_async.get(url)
        print(r.json())
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;correct-alteration&quot;&gt;Correct Alteration&lt;/h4&gt;
&lt;p&gt;Create a list of tasks, and run all of them together using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.gather()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;async def asynchronous(urls):
    tasks = []
    for url in urls:
        task = asyncio.create_task(requests_async.get(url))
        tasks.append(task)
    responses = await asyncio.gather(*tasks)
    for response in responses:
        print(response.json())
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;running-the-code&quot;&gt;Running the Code&lt;/h2&gt;

&lt;h3 id=&quot;python&quot;&gt;Python&lt;/h3&gt;
&lt;p&gt;Simply add these three lines to the bottom of your Python file and run it.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;asyncio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asynchronous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you try to run this same code in Jupyter Notebook, you will get this error:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RuntimeError: asyncio.run&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; cannot be called from a running event loop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This happens because Jupyter is already running an event loop. More info &lt;a href=&quot;https://stackoverflow.com/questions/55409641/asyncio-run-cannot-be-called-from-a-running-event-loop&quot;&gt;here&lt;/a&gt;. You need to use the following:&lt;/p&gt;

&lt;h3 id=&quot;jupyter-notebook&quot;&gt;Jupyter Notebook&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asynchronous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;ordering&quot;&gt;Ordering&lt;/h2&gt;

&lt;p&gt;Asynchronous running can cause your responses to be out of order. If this is an issue, create your own responses list and fill it up, rather than receiving the output from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.gather()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;async def asynchronous_ordered(urls):
    responses = [None] * len(urls) # create own responses list
    tasks = []
    for i in range(len(urls)):
        url = urls[i]
        task = asyncio.create_task(fetch(url, responses, i))
        tasks.append(task)
    await asyncio.gather(*tasks) # responses is not set to equal this
    for response in responses:
        print(response.json())

async def fetch(url, responses, i):
    response = await requests.get(url)
    responses[i] = response # fill up responses list
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;batching&quot;&gt;Batching&lt;/h2&gt;

&lt;p&gt;Sometimes running too many requests concurrently can cause timeout errors in your resource. This is when you need to create tasks in batches and gather them separately to avoid the issue. Find the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size&lt;/code&gt; that best fits your code by experimenting with a smaller portion of requests. Requests that take longer to process (long server delay) are more likely to cause errors than others. In my own experience with NLMatic’s engine, MongoDB had timeout errors whenever I ran batches of size greater than 10.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;asynchronous_ordered_batched&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kiterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kiterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asyncio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asyncio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;runtime-results&quot;&gt;Runtime Results&lt;/h2&gt;

&lt;p&gt;|&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/table.png&quot; width=&quot;500&quot; /&gt;|&lt;img src=&quot;http://localhost:4000/site_files/connie_post_images/chart.png&quot; width=&quot;500&quot; /&gt;|
|—|—|&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;synchronous&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_fail&lt;/code&gt; have similar runtimes because the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_fail&lt;/code&gt; method was not implemented correctly and is in reality synchronous code.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered_batched&lt;/code&gt; have noticeably better runtimes in comparison to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;synchronous&lt;/code&gt; - up to 4 times as fast.&lt;/p&gt;

&lt;p&gt;In general, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered_batched&lt;/code&gt; gives fast and stable code, so use that if you are going for consistency. However, the runtimes of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered&lt;/code&gt; can sometimes be better than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered_batched&lt;/code&gt;, depending on your database and servers. So, I recommend using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous&lt;/code&gt; first and then adding extra things (order and batch) as necessary.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;As you have seen, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt; is a helpful tool that can greatly boost your runtime if you are running a lot of independent API requests. It’s also very easy to implement when compared to threading, so definitely try it out. Of course, make sure that your requests are independent.&lt;/p&gt;

&lt;p&gt;Now I must conclude by saying that using asyncio improperly implemented can cause many bugs, so sometimes it is not worth the hassle. If you really must, use my guide and use it  s p a r i n g l y.&lt;/p&gt;

&lt;p&gt;And that’s a wrap!&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d&quot;&gt;Concurrency, parallelism, threads, sync, and async&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackify.com/when-to-use-asynchronous-programming/&quot;&gt;When/when not to use asynchronous programming&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://realpython.com/async-io-python/&quot;&gt;Async IO in Python: A Complete Walkthrough&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/library/asyncio.html&quot;&gt;asyncio: Python Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Connie Xu</name><email>rob@sor.com</email></author><summary type="html">by Connie Xu</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/asynciothumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/asynciothumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SQUAD 2.0 and Google Natural Questions: A Comparison and Investigation into Model Performance</title><link href="http://localhost:4000/2020/08/06/SQUAD-2.0-and-Google-Natural-Questions-A-Comparison-and-Investigation-into-Model-Performance.html" rel="alternate" type="text/html" title="SQUAD 2.0 and Google Natural Questions: A Comparison and Investigation into Model Performance" /><published>2020-08-06T22:00:00+09:00</published><updated>2020-08-06T22:00:00+09:00</updated><id>http://localhost:4000/2020/08/06/SQUAD%202.0%20and%20Google%20Natural%20Questions-%20A%20Comparison%20and%20Investigation%20into%20Model%20Performance</id><content type="html" xml:base="http://localhost:4000/2020/08/06/SQUAD-2.0-and-Google-Natural-Questions-A-Comparison-and-Investigation-into-Model-Performance.html">&lt;h4 id=&quot;by-nick-greenspan&quot;&gt;by Nick Greenspan&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;strong&gt;&lt;em&gt;Nicholas Greenspan&lt;/em&gt;&lt;/strong&gt; lives in New York City and is a freshman at Rice University who is majoring in computer science. Nicholas is interested in Machine Learning, Natural Language Processing, and their applications to various fields. Nicholas recently worked at the UTHealth School of Biomedical Informatics working on a Natural Language Processing project to help doctors find relevant treatments for their patients, and is excited to work on more interesting and meaningful problems at NLMatics. Outside of CS, Nicholas likes to read, play ice hockey, and listen to many genres of music including indie rock and electronic. Nicholas was one of NLMatics’ 2020 summer interns
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt; SQUAD 2.0 and Google Natural Questions: &lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; A Comparison and Investigation into Model Performance &lt;/h2&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;SQuAD 2.0 and Google Natural Questions are two of the most prominent datasets in NLP Questioning Answering today. Both include tens of thousands of training examples which consist of a question, context, and an answer span. Though they both have the same general structure, there are many differences, both major and nuanced, that distinguish the two datasets. Whether you want to learn about the datasets for research purposes, or are trying to decide which one to use to train a QA model for your business, this is an in-depth guide to understanding the two datasets, their differences, and their relationships.&lt;/p&gt;

&lt;h2 id=&quot;overviews-from-the-dataset-websites&quot;&gt;Overviews from the Dataset Websites&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/nick_post/squad_image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD 2.0&lt;/a&gt;: “SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/site_files/nick_post/nq_image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.google.com/research/NaturalQuestions&quot;&gt;Google Natural Questions&lt;/a&gt;: “The NQ corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.”&lt;/p&gt;

&lt;h2 id=&quot;important-takeaways-for-each-dataset&quot;&gt;Important takeaways for each dataset&lt;/h2&gt;

&lt;p&gt;SQuAD 2.0:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Questions generated by hired workers whose task was to write questions about a given article.&lt;/li&gt;
  &lt;li&gt;Sets of questions are from paragraphs in a wikipedia article.
    &lt;ul&gt;
      &lt;li&gt;442 different articles.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are unanswerable questions (33.4% of the dataset is unanswerable), which forces a model to “know what it doesn’t know”.
    &lt;ul&gt;
      &lt;li&gt;Ensures there are plausible answers in the paragraph if the question is unanswerable so that model can’t just use superficial clues to determine if an answer is in the paragraph.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;All questions end with a question mark.&lt;/li&gt;
  &lt;li&gt;Since there are series of questions that refer to the same article, some of the questions use pronouns such as “where did she grow up?”.&lt;/li&gt;
  &lt;li&gt;There are some ungrammatical sentences such as: “Why political movement was named for Joseph McCarthy?”.&lt;/li&gt;
  &lt;li&gt;There are occasional misspellings.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Google Natural Questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Real, user generated questions by people who are actually seeking information, not people who were hired for the explicit purpose of writing questions.&lt;/li&gt;
  &lt;li&gt;Requires a model to read a whole Wikipedia article, not just a paragraph, and has two different tasks: identifying a long answer, which is the paragraph or table that the contains the information to answer the question, and the short answer, which is the exact text that provides the answer to the question.
    &lt;ul&gt;
      &lt;li&gt;Note that since the article is so long BERT models are unequipped to handle them as they have a max sequence length capped at 512.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;All questions aren’t necessarily “questions” per se, some of them are phrase searches like  “benefits of colonial life for single celled organisms”.&lt;/li&gt;
  &lt;li&gt;Questions don’t necessarily have a long answer or short answer, so there is some of the unanswerable functionality that is also present in SQuAD 2.0.&lt;/li&gt;
  &lt;li&gt;If a question has a short answer, it definitely has a long answer, but not the other way around.&lt;/li&gt;
  &lt;li&gt;There is a “Yes or No” answer field which is “Yes” if the answer is yes, “No” if the answer is no, and “None” if there is not a yes or no answer to the question. If the “Yes or No” answer is not “None” then there is no short answer.&lt;/li&gt;
  &lt;li&gt;All questions don’t end with a question mark.&lt;/li&gt;
  &lt;li&gt;Since only one question per article there is more variety of topics of questions.&lt;/li&gt;
  &lt;li&gt;There seem to be less ungrammatical or misspelled sentences.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datapoint-examples&quot;&gt;Datapoint Examples&lt;/h2&gt;

&lt;p&gt;In version 2 of the dataset, SQuAD 2.0 examples have two formats –\
For answerable questions:
 	 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;question&quot;: &quot;In what country is Normandy located?&quot;, &quot;id&quot;: &quot;56ddde6b9a695914005b9628&quot;, &quot;answers&quot;: [ { &quot;text&quot;: &quot;France&quot;, &quot;answer_start&quot;: 159 } ], &quot;is_impossible&quot;: false }&lt;/code&gt; \
For impossible questions:
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;plausible_answers&quot;: [ { &quot;text&quot;: &quot;Normans&quot;, &quot;answer_start&quot;: 4 } ], &quot;question&quot;: &quot;Who gave their name to Normandy in the 1000's and 1100's&quot;, &quot;id&quot;: &quot;5ad39d53604f3c001a3fe8d1&quot;, &quot;answers&quot;: [], &quot;is_impossible&quot;: true }&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Google Natural Questions (abbreviated due to length of document html and tokens):\
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{&quot;annotations&quot;:[{&quot;annotation_id&quot;:6782080525527814293,&quot;long_answer&quot;:{&quot;candidate_index&quot;:92,&quot;end_byte&quot;:96948,&quot;end_token&quot;:3538,&quot;start_byte&quot;:82798,&quot;start_token&quot;:2114},&quot;short_answers&quot;:[{&quot;end_byte&quot;:96731,&quot;end_token&quot;:3525,&quot;start_byte&quot;:96715,&quot;start_token&quot;:3521}],&quot;yes_no_answer&quot;:&quot;NONE&quot;}],&quot;document_html&quot;:&amp;lt;/HTML&amp;gt;\n&quot;,&quot;document_title&quot;:&quot;The Walking Dead (season 8)&quot;,&quot;document_tokens&quot;:[{&quot;end_byte&quot;:95,&quot;html_token&quot;:false,&quot;start_byte&quot;:92,&quot;token&quot;:&quot;The&quot;},{&quot;end_byte&quot;:103,&quot;html_token&quot;:false,&quot;start_byte&quot;:96,&quot;token&quot;:&quot;Walking&quot;},{&quot;end_byte&quot;:108,&quot;html_token&quot;:false,&quot;start_byte&quot;:104,&quot;token&quot;:&quot;Dead&quot;},
…], 
document_url&quot;:&quot;https://en.wikipedia.org//w/index.php?title=The_Walking_Dead_(season_8)&amp;amp;amp;oldid=828222625&quot;,&quot;example_id&quot;:4549465242785278785,&quot;long_answer_candidates&quot;:[{&quot;end_byte&quot;:57620,&quot;end_token&quot;:216,&quot;start_byte&quot;:53609,&quot;start_token&quot;:24,&quot;top_level&quot;:true},{&quot;end_byte&quot;:53883,&quot;end_token&quot;:36,&quot;start_byte&quot;:53666,&quot;start_token&quot;:25,&quot;top_level&quot;:false},{&quot;end_byte&quot;:54388,&quot;end_token&quot;:42,&quot;start_byte&quot;:53884,&quot;start_token&quot;:36,&quot;top_level&quot;:false},…],
&quot;question_text&quot;:&quot;when is the last episode of season 8 of the walking dead&quot;,&quot;question_tokens&quot;:[&quot;when&quot;,&quot;is&quot;,&quot;the&quot;,&quot;last&quot;,&quot;episode&quot;,&quot;of&quot;,&quot;season&quot;,&quot;8&quot;,&quot;of&quot;,&quot;the&quot;,&quot;walking&quot;,&quot;dead&quot;]}&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Papers for more info on the datasets:\
&lt;a href=&quot;https://arxiv.org/pdf/1806.03822.pdf&quot;&gt;SQuAD 2.0&lt;/a&gt;\
&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf&quot;&gt;Google Natural Questions&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataset-statistics&quot;&gt;Dataset Statistics&lt;/h2&gt;

&lt;p&gt;To find the best dataset for your use case, here are some statistics about the different types of questions in the datasets. Note that this data is approximate, as general rules were used to separate questions into these categories and some may have fallen through the cracks mostly due to the fact that some question start with context and don’t lead or end with the question word such as “in september 1849 where did chopin take up residence?”. The “other” category in the table is made up of all questions that didn’t fall into our general descriptions of the question types, and the fact there are many more questions in the other category for Google Natural Questions vs SQuAD 2.0 is due to both the relative size of the dataset and the increased prevalence of phrase like “questions” such as “benefits of colonial life for single celled organisms”.&lt;/p&gt;

&lt;h3 id=&quot;question-type-distribution-of-train-sets&quot;&gt;Question Type Distribution of Train Sets&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Google Natural Questions&lt;/th&gt;
      &lt;th&gt;SQuAD 2.0&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Total Questions&lt;/td&gt;
      &lt;td&gt;307373&lt;/td&gt;
      &lt;td&gt;130319&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;What (%, total num)&lt;/td&gt;
      &lt;td&gt;17.1%, 52535&lt;/td&gt;
      &lt;td&gt;59.6%, 77701&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Where (%, total num)&lt;/td&gt;
      &lt;td&gt;10.3%, 31776&lt;/td&gt;
      &lt;td&gt;4.07%, 5303&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;When (%, total num)&lt;/td&gt;
      &lt;td&gt;13.6%, 41725&lt;/td&gt;
      &lt;td&gt;6.38%, 8308&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Who (%, total num)&lt;/td&gt;
      &lt;td&gt;25.1%, 77281&lt;/td&gt;
      &lt;td&gt;10.4%, 13533&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Why (%, total num)&lt;/td&gt;
      &lt;td&gt;1.31%, 4041&lt;/td&gt;
      &lt;td&gt;1.44%, 1881&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Which (%, total num)&lt;/td&gt;
      &lt;td&gt;2.83%, 8721&lt;/td&gt;
      &lt;td&gt;6.21%, 8088&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Whom (%, total num)&lt;/td&gt;
      &lt;td&gt;0%, 6&lt;/td&gt;
      &lt;td&gt;0.343%, 447&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;How (%, total num)&lt;/td&gt;
      &lt;td&gt;5.87%, 18041&lt;/td&gt;
      &lt;td&gt;9.95%, 12969&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Other (%, total num)&lt;/td&gt;
      &lt;td&gt;22.8%, 70157&lt;/td&gt;
      &lt;td&gt;1.50%, 1954&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Side Note:
The fact that question distribution is much more balanced for Google Natural Questions vs SQuAD 2.0, is an interesting comment on the types of questions people naturally vs artificially come up with.&lt;/p&gt;

&lt;h2 id=&quot;cross-training-experiment&quot;&gt;Cross Training Experiment&lt;/h2&gt;

&lt;p&gt;An interesting research question that we wanted to explore was how a model trained on one dataset would perform when tested on the other dataset. This would possibly allow us to see if one dataset allowed for better generalization to out of domain data, which could help inform a decision of the best dataset for one’s needs.&lt;/p&gt;

&lt;p&gt;Since the contexts in the Google Natural Questions (GNQ) dataset consist of whole wikipedia articles, which are much too long for the bert models we wanted to use, we decided to make use of the long answer as context for our training, and use the short answer as the target answer. We used data points which had long answers but not short answers to act as “unanswerable” questions. As there are 152148 data points with long answers in GNQ, our reformatted dataset had 152148 data points, as none of the data points without long answers were used in our reformatted dataset. 29.7% of the GNQ questions that have a long answer don’t have a short answer, which makes it comparable to the 33.4% of SQuAD 2.0 questions in the train data that are unanswerable.&lt;/p&gt;

&lt;p&gt;For maximum convenience and fair comparibility, we decided to use Huggingface’s run_squad.py script to train our model on our modified GNQ Dataset, which meant converting the GNQ dataset into the SQuAD 2.0 format, which involved stripping the text of html, finding the correct answer span in the the long answer context, among other things. If you want to do this yourself, or are just curious about the specifics check out part 2 of this guide to model training I helped write:&lt;/p&gt;

&lt;p&gt;One limitation we came across that caused us to change our training methods slightly was the amount of time it takes to train a model. Since there are a number of SQuAD 2.0 pretrained models available for &lt;a href=&quot;https://huggingface.co/ktrapeznikov/albert-xlarge-v2-squad-v2&quot;&gt;public use&lt;/a&gt;, we were able to use a model trained on the full SQuAD 2.0 dataset in this comparison. Since the GNQ dataset is very large, we initially choose to just run the training on 1/10th of the reformatted dataset, and then later on decided to train the model that was trained on the first 1/10th on the next 4/10th so that the model ended up being trained on 1/2th of the reformatted GNQ dataset.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Dataset model was trained on as the rows, and evaluation dataset as the columns.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;GNQ&lt;/th&gt;
      &lt;th&gt;SQuAD&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;GNQ 1/10th Data&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 53.137003841229195, 'f1': 57.66917657734894, 'total': 781, 'HasAns_exact': 40.0, 'HasAns_f1': 46.67854133379154, 'HasAns_total': 530, 'NoAns_exact': 80.87649402390439, 'NoAns_f1': 80.87649402390439, 'NoAns_total': 251, 'best_exact': 53.137003841229195, 'best_exact_thresh': 0.0, 'best_f1': 57.66917657734895, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 50.61062915859513, 'f1': 51.25986732355112, 'total': 11873, 'HasAns_exact': 12.179487179487179, 'HasAns_f1': 13.479825359737246, 'HasAns_total': 5928, 'NoAns_exact': 88.9318755256518, 'NoAns_f1': 88.9318755256518, 'NoAns_total': 5945, 'best_exact': 50.720121283584604, 'best_exact_thresh': 0.0, 'best_f1': 51.30938713471512, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GNQ ½ Data&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 57.01398285972034, 'f1': 63.63002288066548, 'total': 2217, 'HasAns_exact': 50.23380093520374, 'HasAns_f1': 60.03190429287585, 'HasAns_total': 1497, 'NoAns_exact': 71.11111111111111, 'NoAns_f1': 71.11111111111111, 'NoAns_total': 720, 'best_exact': 57.01398285972034, 'best_exact_thresh': 0.0, 'best_f1': 63.63002288066556, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 47.23321822622758, 'f1': 50.06099675107974, 'total': 11873, 'HasAns_exact': 35.27327935222672, 'HasAns_f1': 40.93694575330119, 'HasAns_total': 5928, 'NoAns_exact': 59.158957106812444, 'NoAns_f1': 59.158957106812444, 'NoAns_total': 5945, 'best_exact': 50.08843594710688, 'best_exact_thresh': 0.0, 'best_f1': 50.75645811811136, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SQuAD 2.0&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 30.985915492957748, 'f1': 35.324918748038996, 'total': 781, 'HasAns_exact': 15.283018867924529, 'HasAns_f1': 21.67690857022349, 'HasAns_total': 530, 'NoAns_exact': 64.14342629482071, 'NoAns_f1': 64.14342629482071, 'NoAns_total': 251, 'best_exact': 32.394366197183096, 'best_exact_thresh': 0.0, 'best_f1': 35.43007732875682, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{‘exact': 75.90331003116314, 'f1': 79.23560349162027, 'total': 11873, 'HasAns_exact': 64.97975708502024, 'HasAns_f1': 71.65390017813893, 'HasAns_total': 5928, 'NoAns_exact': 86.79562657695543, 'NoAns_f1': 86.79562657695543, 'NoAns_total': 5945, 'best_exact': 75.90331003116314, 'best_exact_thresh': 0.0, 'best_f1': 79.23560349162024, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;A number of interesting observations and plausible conclusions can be made by looking at the data, and the eval data from the model trained on 1/10th and ½ of the reformatted GNQ dataset gives insight into how the performance of the model varies as it is fed more data.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;One initial comparison to make is to look at the model’s performance on the dev set of the same dataset of the train set it was trained on. The SQuAD 2.0 trained model has a better f1 of 79.2 than either the GNQ trained model trained on 1/10th or ½ of the data with f1s of 57.7 and 63.6 respectively.&lt;/li&gt;
  &lt;li&gt;Note that GNQ is a much harder task, as state of the art f1 for short answer identification is .64 vs the around .93 for SQuAD 2.0. Much of the difficulty may have been offset by the fact we are using the long answer for context instead of the whole wikipedia article, but the fact that GNQ’s questions are naturally generated, and don’t tend to borrow pieces of the paragraph, is one way in which GNQ is still a harder task than SQuAD 2.0.&lt;/li&gt;
  &lt;li&gt;The fact that the GNQ trained model was only trained on half of the dataset whereas the SQuAD 2.0 trained model was trained on the whole thing also probably had an effect on the model performance.&lt;/li&gt;
  &lt;li&gt;Though the SQuAD 2.0 model did better on its own dev set than the GNQ model did, the GNQ model did better on the SQuAD 2.0 dev set than the SQuAD 2.0 model did on the GNQ dev set when the overall and the has_ans exact and f1 scores are compared. This implies that the GNQ dataset does a better job of instilling a general purpose language understanding in the model that can generalize to other domains.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing to keep track of which can yield interesting insights into a model’s behavior is a model’s tendency to answer questions or refrain from answering them.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Looking at how the no_ans scores decrease and the has_ans score increases for the GNQ model on GNQ eval on differing amounts of data likely demonstrates how as the model is fed more data it tends to guess that the question has an answer more often.&lt;/li&gt;
  &lt;li&gt;This can be seen even more clearly by looking at the GNQ model on the SQuAD 2.0 eval, where for the 1/10th data model the has_ans score are very low and the no_ans score is very high, likely demonstrating that the model does not have much understanding and just tends to say that the question is unanswerable. For the 1/2th data model, the overall exact and f1 scores are lower, but it is clear the model has better understanding and not just relying on guessing that the question is unanswerable all the time as the has_ans scores are much higher, and the no_ans scores are lower.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A couple of differences between the datasets that may have affected the experimental results are as follows.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There is no explicit plausible yet wrong answer in the long answer of questions that don’t have a short answer in the GNQ dataset, unlike the plausible answers in the SQuAD 2.0 dataset. Though there is no plausible answer per se, the long answer paragraph is necessarily related to the question, so there might be spans in the GNQ paragraphs that don’t have a short answer that are similar to the plausible answers found in the SQuAD 2.0 dataset.&lt;/li&gt;
  &lt;li&gt;Also, in the reformatting of the dataset we counted questions that had Yes or No answer as unanswerable as they don’t have a short answer span.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Hopefully this article gave you further insight into whether SQuAD 2.0 or Google Natural Questions is right to use to train your model, or just gave insight into the nature of the datasets. NLP Question Answering is a very exciting area of active research, and there are a number of different interesting datasets for it out there beyond the two mentioned in the article such as &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/YangYihMeek_EMNLP-15_WikiQA.pdf&quot;&gt;WikiQA&lt;/a&gt;, &lt;a href=&quot;https://hotpotqa.github.io/&quot;&gt;HotpotQA&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/1712.07040.pdf&quot;&gt;NarrativeQA&lt;/a&gt;. In the future, I hope to explore these other datasets and run more experiments to understand their strengths, weaknesses, and relations.&lt;/p&gt;

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD Website&lt;/a&gt;\
&lt;a href=&quot;https://ai.google.com/research/NaturalQuestions&quot;&gt;SQuAD Paper&lt;/a&gt;\
&lt;a href=&quot;https://arxiv.org/pdf/1806.03822.pdf&quot;&gt;Google Natural Questions Website&lt;/a&gt;\
&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf&quot;&gt;Google Natural Questions Paper&lt;/a&gt;&lt;/p&gt;</content><author><name>Nick Greenspan</name></author><summary type="html">by Nick Greenspan</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/squadthumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/squadthumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Comprehensive Guide to Training a Machine Learning Model for Question Answering</title><link href="http://localhost:4000/2020/08/06/A-Comprehensive-Guide-to-Training-a-Machine-Learning-Model-for-Question-Answering_.html" rel="alternate" type="text/html" title="A Comprehensive Guide to Training a Machine Learning Model for Question Answering" /><published>2020-08-06T19:30:00+09:00</published><updated>2020-08-06T19:30:00+09:00</updated><id>http://localhost:4000/2020/08/06/A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_</id><content type="html" xml:base="http://localhost:4000/2020/08/06/A-Comprehensive-Guide-to-Training-a-Machine-Learning-Model-for-Question-Answering_.html">&lt;h4 id=&quot;by-batya-stein--nicholas-greenspan&quot;&gt;by Batya Stein &amp;amp; Nicholas Greenspan&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;strong&gt;&lt;em&gt;Batya Stein&lt;/em&gt;&lt;/strong&gt; is a rising junior at Princeton University studying Computer Science and English Literature. Her research interests include Natural Language Processing and Software Engineering and Design. Previously, she has interned at a non-profit, designing a project to interface between its CRM softwares. When not coding, she enjoys drawing, swimming, and reading fiction. Batya was one of NLMatics’ 2020 summer interns. 
&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;Nicholas Greenspan&lt;/b&gt; lives in New York City and is a freshman at Rice University who is majoring in computer science. Nicholas is interested in Machine Learning, Natural Language Processing, and their applications to various fields. Nicholas recently worked at the UTHealth School of Biomedical Informatics working on a Natural Language Processing project to help doctors find relevant treatments for their patients, and is excited to work on more interesting and meaningful problems at NLMatics. Outside of CS, Nicholas likes to read, play ice hockey, and listen to many genres of music including indie rock and electronic. Nicholas was one of NLMatics’ 2020 summer interns.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt; A Comprehensive Guide to Training a Machine Learning Model for Question Answering: &lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Fine-tuning ALBERT on Google Natural Questions &lt;/h2&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#part-1-setting-up-the-ec2-instance&quot;&gt;Setting up the EC2 Instance&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#choosing-and-launching-an-ec2-instance&quot;&gt;Choosing and launching EC2 Instance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#preparing-the-instance-for-training&quot;&gt;Preparing instance for training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#setting-up-wandb-logging&quot;&gt;Setting up Wandb logging&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#part-2-downloading-and-formatting-data&quot;&gt;Downloading and Formatting Data&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#about-the-datasets&quot;&gt;About the datasets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#downloading-the-data&quot;&gt;Downloading the data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#processing-data-from-a-zip-file&quot;&gt;Processing from zip file&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#reformatting-google-nq-data-to-squad-format&quot;&gt;Reformatting Google Natural Questions to SQuAD format&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#part-3-training-the-model&quot;&gt;Training the Model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#model-comparisons&quot;&gt;Model comparisons&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#understanding-the-parameters-of-run_squadpy&quot;&gt;Understanding run_squad.py parameters&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#common-errors-we-encountered-and-how-to-fix-them&quot;&gt;Common errors encountered during training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#restoring-training-from-a-checkpoint&quot;&gt;Restoring from a checkpoint&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#understanding-your-model-outputs&quot;&gt;Understanding model outputs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#downloading-your-model-uploading-to-s3-bucket&quot;&gt;Downloading your model and uploading to an S3 bucket&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro:&lt;/h2&gt;

&lt;p&gt;Feel like a machine learning question answering system could give your business a boost? Just interested in trying out revolutionary AI tech? You’ve come to the right place. Training your first deep learning model can feel like navigating a labyrinth with no clear start or finish, so we’ve made a highly detailed guide to save you time, energy, and a lot of stackoverflow searches. This is a comprehensive guide to training a ML question answering model that will walk you through every step of the process from cloud computing to model training. We will use AWS for our cloud computing necessities, and Huggingface’s transformers repository to access our ALBERT model and run_squad.py script for training.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why do I care? What AI can do for you:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Extracting information from long text documents can be a time consuming, monotonous endeavour. A revolutionary cutting edge technology, NLP Question Answering systems can automate this process, freeing you up to use that information to make decisions to further your business. Given a piece of text and a question, a Question Answering system can tell you where the answer lies in the text, and even tell you if the answer isn’t present at all! If this technology interests you, and you want to apply it at a business-level scale, you should definitely check out NLMatics’ product at https://www.nlmatics.com/.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://static.wixstatic.com/media/97e2b2_df3bf50a5add420faa4f298c5c232584~mv2.png/v1/fill/w_336,h_64,al_c,q_85,usm_0.66_1.00_0.01/logo.webp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We ran the model training described here during our summer engineering internship at NLMatics. Currently, one of the most popular Question Answering transformer models is trained on the &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD&lt;/a&gt; dataset, and we were curious to see if training on &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions/&quot;&gt;Google Natural Questions&lt;/a&gt; could give us more effective results. (As of this post, there are no ALBERT models pretrained on Google Natural Questions available for use online.) Along the way, we learned a lot about the ins and outs of model training, memory management, and data-logging.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Huggingface’s &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;transformers github repository&lt;/a&gt;, an open source resource for training language models, greatly simplifies the training process and puts many essential materials all in one place. Huggingface provides scripts for training models on different datasets, which saves you the labor of writing your own. In order to train on a new dataset while still making use of Huggingface’s &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py&quot;&gt;run_squad.py script&lt;/a&gt; and functions for evaluating SQuAD training metrics, we converted the Google Natural Questions dataset into the same format as SQuAD. For this reason, this guide can help you whether you want to use Google Natural Questions, SQuAD, or any other question answering dataset to train a transformer model.&lt;/p&gt;

&lt;p&gt;To learn more about machine learning or language models, see the links below.\
&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/ml-intro&quot;&gt;Machine Learning&lt;/a&gt;\
&lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;Transformer architecture (BERT)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To learn more about the datasets we worked with, see the links above or our blog post comparing the characteristics of both sets.&lt;/p&gt;

&lt;h2 id=&quot;part-1-setting-up-the-ec2-instance&quot;&gt;PART 1: SETTING UP THE EC2 INSTANCE&lt;/h2&gt;

&lt;h2 id=&quot;choosing-and-launching-an-ec2-instance&quot;&gt;Choosing And Launching An EC2 Instance:&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;To start our model training, we have to choose the right platform (one with a bit more computing power than a personal computer)&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://www.kodyaz.com/images/aws/aws-ec2-dashboard-to-list-all-ec2-instances.png&quot; alt=&quot;ec2 instance list&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;List of instances in EC2 dashboard&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;What is AWS EC2?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Amazon Web Services’ EC2 service allows users to utilize cloud computing power from their local machines. An On-Demand EC2 Instance provides a virtual server that charges per hour that instance is running.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instance types and costs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To choose an instance for your use case, consider its GPU and vCPU capacities, storage space, and &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand/&quot;&gt;cost&lt;/a&gt;. For our training, we considered &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types#Accelerated_Computing&quot;&gt;P and G- type instances&lt;/a&gt;, since both classes come with GPU power. P2/P3 instances have up to 16 GPUs (P3 is the latest generation and comes at a slightly higher cost). G4 instances have up to 4 GPUs, but are more cost effective for larger amounts of memory. Note that if you’re using CUDA, a platform for computing with GPU power often used in machine learning applications, the maximum number of GPUs it will run on is 8, &lt;a href=&quot;https://forums.developer.nvidia.com/t/cuda-peer-resources-error-when-running-on-more-than-8-k80s-aws-p2-16xlarge/45351&quot;&gt;due to limitations in the “peer-to-peer” system&lt;/a&gt;. We chose the p2.8xl instance, which has 8 NVIDIA K80 GPUs and 96GB of storage.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Instance&lt;/th&gt;
      &lt;th&gt;GPUs&lt;/th&gt;
      &lt;th&gt;GPU Type and total GPU memory&lt;/th&gt;
      &lt;th&gt;vCPUs&lt;/th&gt;
      &lt;th&gt;RAM&lt;/th&gt;
      &lt;th&gt;Linux Pricing (USD/HR)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p2.xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA K80 GPU (12GiB)&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;61 GiB&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p2.8xlarge&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;NVIDIA K80 GPU(96GiB)&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;488 GiB&lt;/td&gt;
      &lt;td&gt;7.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p2.16xlarge&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;NVIDIA K80 GPU (192GiB)&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;732 GiB&lt;/td&gt;
      &lt;td&gt;14.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p3.2xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA Tesla V100 GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
      &lt;td&gt;3.06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p3.8xlarge&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;NVIDIA Tesla V100 GPU (64GiB)&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;244&lt;/td&gt;
      &lt;td&gt;12.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p3.16xlarge&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;NVIDIA Tesla V100 GPU (128GiB)&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;488&lt;/td&gt;
      &lt;td&gt;24.48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0.526&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.8xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;2.176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.12xlarge&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (64GiB)&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;192&lt;/td&gt;
      &lt;td&gt;3.912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.16xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;4.352&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Account limits&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before launching an instance, be aware that each type has a specific vCPU capacity that it needs to run, which can be found in the chart above. However, all AWS accounts have automatic limitations on the number of vCPUs that can be allocated per instance class. Current account limits can be viewed in the EC2 section of the “Service Quotas” tab in the AWS console. Make sure you are in the same region you plan on starting your instance in, then check the “applied quota value” for “Running On-Demand P instances” (or instance class of your choice). If the current vCPU limit isn’t large enough to launch your instance, use the “Request Limit Increase” button to request the necessary number of vCPUs. In our experience, requests can take up to 48 hours to be filled, so request increases in advance of when you plan to start your training.&lt;/p&gt;

&lt;p&gt;Once the limit is increased, you can launch your instance from the AWS EC2 console. Choose a platform with deep learning capabilities - we needed CUDA and a PyTorch environment, so we chose the Deep Learning AMI (Amazon Linux) Version 30.0. Next, choose instance type, and add additional storage if desired. Then configure the security group to allow from incoming connections from your IP address to the instance, by adding a rule of Type “SSH” with Source “My IP Address”, which automatically fills in your IP for you. You can also add additional rules to the security group later if needed to give others access to the instance. Do so by choosing your
instance from the console instance list. Under its description, click its security group, then from the pulldown menu in the security groups list choose actions -&amp;gt; edit inbound rules -&amp;gt; add SSH rule, with IP of the person who needs access as the source.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key-pair security file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To secure connections to your instance, create a key-pair file. In order to work, the key must not have public security permissions, so once you’ve downloaded the file, go to your terminal, navigate to the directory that the file is stored in, and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chmod 400 keypair-file-name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can then connect to the instance from your terminal/ command-line by selecting the instance in the console list and running the example command given by the connect button, which will look like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -i”xxx.pem” EC2-user@EC2-xx-xx-xxx-xxx.us-region.compute.amazonaws.com&lt;/code&gt;.
 (When logging onto the server for the first time, you may get a warning that “The authenticity of host… can’t be established” but this should not be a security issue since you are instantiating the connection directly from AWS.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IAM roles (sharing AWS resource access)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to give someone else access to the instance without giving them your account login information, you can create an IAM role in the IAM console. In the Users menu, add a User with AWS Management Console access, give user all EC2 permissions and then share the role’s login information.
Once you start using your instance, you can monitor the costs or credits used, with the AWS cost explorer service. Make sure to stop the instance when not in use to avoid extra charges!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/TLZQDUX.jpg&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;preparing-the-instance-for-training&quot;&gt;Preparing The Instance For Training&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Before we can get started with training, we’ll need to load all the necessary libraries and data onto our instance.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upload training data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prepare for training by connecting to your EC2 instance from the terminal of your choice and creating a directory inside it to store the data files in. (See section II, on downloading and reformatting data, to learn how to get your data into the right format before you start working with it.) Upload your training and dev sets into the directory (in json format). Upload the training script, which can be &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py&quot;&gt;found in the Transformers github repo&lt;/a&gt;, onto the instance. Lastly, create a new directory that will be used to store the evaluations and model checkpoints from the script’s output.&lt;/p&gt;

&lt;p&gt;Note that if you’ve previously trained a model of the same type on your EC2 instance, there will be a cached datafile with a name like “cached_dev_albert-xlarge-v2_384”. If you’re training on a different dataset than you used in the previous training, make sure to delete the cached file, otherwise the script will automatically load the data from it and you’ll end up training on different data then you meant to.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File transferring&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To upload and transfer files between an EC2 instance and local computer you can use the following terminal commands. (Run these commands while the instance is running, but in a separate terminal window from where the instance is logged in.)&lt;/p&gt;

&lt;p&gt;From &lt;strong&gt;local -&amp;gt; EC2&lt;/strong&gt; – run the command
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp -i /directory/to/abc.pem /your/local/file/to/copy user@EC2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;From &lt;strong&gt;EC2 -&amp;gt; local&lt;/strong&gt; (for downloading your model checkpoints, etc.) run
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp -i /directory/to/abc.pem user@EC2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file /your/local/directory/files/to/download&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Another option is to use &lt;a href=&quot;https://filezilla-project.org/&quot;&gt;FileZilla&lt;/a&gt; software for transferring files. This &lt;a href=&quot;https://stackoverflow.com/questions/16744863/connect-to-amazon-ec2-file-directory-using-filezilla-and-sftp&quot;&gt;post&lt;/a&gt; explains how to set up a connection to the EC2 instance through Filezilla with your pem key.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Library installations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prepare your environment by downloading the libraries needed for the run_squad script. Run the following commands in the terminal of your choice, while logged into your instance:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source activate env-name&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;This command activates a &lt;strong&gt;virtual environment&lt;/strong&gt;. Choose one of the environments that come pre-loaded onto the instance, listed when you first log into the instance from your terminal. 
Since run_squad.py relies on PyTorch, we used the environment “pytorch_latest_p36”. &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; is a python machine learning library that can carry out accelerated tensor computations using GPU power.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install transformers&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Transformers&lt;/strong&gt; is the Huggingface library that provides transformer model architectures that can be used with PyTorch or tensorflow. The transformers library also has processors for training on SQuAD data, which are used in the run_squad.py script.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install wandb&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Wandb&lt;/strong&gt; is web app that allows for easy visualization of training progress and evaluation graphs, lets you see logging output from terminal, keeps track of different runs within you project, and allows you to share results with others (We’ll talk more about how to use Wandb to view your results in the next section!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install tensorboardX&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The run_squad.py script uses the &lt;strong&gt;tensorboard&lt;/strong&gt; library to graph evaluations that are run as training progresses. We’ll sync wandb with tensorboard so the graphs can be seen directly from the wandb dashboard.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git clone https://github.com/NVIDIA/apex&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd apex&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -v --no-cache-dir ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Apex&lt;/strong&gt; is a pytorch extension that allows scripts to run with mixed-floating point precision (using 16 bit floats instead of 32 bit in order to decrease RAM usage)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setting-up-wandb-logging&quot;&gt;Setting Up Wandb Logging&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;You’re going to want to keep track of your training as it happens - here’s where we’ll set that up.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Wandb (Weights &amp;amp; Biases) provides a web-based platform for visualizing training loss and evaluation metrics as your training runs. Checking up on metrics periodically throughout training lets you visualize your progress, and course-correct if your loss suddenly swings upwards, or your metrics take a dramatic turn down.&lt;/p&gt;

&lt;p&gt;First you’ll want to create a free account on &lt;a href=&quot;https://www.wandb.com/&quot;&gt;wandb’s website&lt;/a&gt;. 
Then, to connect your wandb account to your EC2 instance, go to the wandb website settings section and copy your api key. Log into your EC2 instance from your terminal and run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb login your-api-key&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next, we’ll edit the run_squad.py script to integrate wandb logging. (You can edit it on your computer before uploading to the instance in the terminal editor of your choice after uploading it.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import wandb&lt;/code&gt; to the library imports.&lt;/li&gt;
  &lt;li&gt;Before line 742 (sending model to cuda), add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb.init(project=&quot;full_data_gnq_run&quot;, sync_tensorboard=True))&lt;/code&gt;. This will create a wandb project with the given name. Each time the script is rerun with the same project name will show up as a new run in the same project. Setting sync with tensorboard to true automatically logs the tensorboard graphs created during training to your project page.&lt;/li&gt;
  &lt;li&gt;In the next line, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb.config.update(args)&lt;/code&gt; to store all the arguments and model parameters inputted to the script&lt;/li&gt;
  &lt;li&gt;On line 759, after sending model to device, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb.watch(model, log='all')&lt;/code&gt;, which saves the weights of the model to wandb as it trains.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that you’ve added wandb statements into your script, you can watch your training progress from the wandb project dashboard. Two particularly helpful sections are the charts, where you can see loss and evaluation metrics, and the logs, where you can see the output of your script as it runs to track what percent complete your training is.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/checkpoints.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Wandb charts from our Google Natural Questions training&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;part-2-downloading-and-formatting-data&quot;&gt;PART 2: DOWNLOADING AND FORMATTING DATA&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;We’re going to download the data for the model to train on, and tweak it a bit to get it into the right format.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(Note that a lot of the information in this section is about Google Natural Questions, but the same tips for handling and reformatting large datasets could apply to any dataset of your choice.)&lt;/p&gt;

&lt;h2 id=&quot;about-the-datasets&quot;&gt;About the Datasets&lt;/h2&gt;

&lt;p&gt;Both &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD&lt;/a&gt; and &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions/&quot;&gt;Google Natural Questions&lt;/a&gt; are datasets containing pairs of questions and answers based on Wikipedia articles. Fundamentally, they serve the same purpose - to teach a model reading comprehension by having it locate answers within a large amount of text.&lt;/p&gt;

&lt;p&gt;However, there are also some interesting differences between the two datasets that made us want to experiment with training a model on Google Natural Questions instead of SQuAD. To start, Google Natural questions has about twice the amount of training data as SQuAD does. SQuAD gives paragraphs from Wikipedia articles as context for its answers and has multiple questions per article, while Google Natural Questions gives entire Wikipedia articles for context, and only has one question per article. Additionally, all of SQuAD’s answers are short (about a sentence long), but Google Natural Questions has some questions with short answers, some with long answers, some with yes-no answers, and some with combinations of all of the above.&lt;/p&gt;

&lt;p&gt;Example of a SQuAD 2.0 datapoint:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/squad_datapoint.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Answer_start indicates the character count where the answer is located in the context.
Some questions in SQuAD 2.0 are flagged is_impossible, if answers to them cannot be found in the given context.&lt;/p&gt;

&lt;p&gt;To learn more about the differences between the datasets, their compositions and the decisions we made when reformatting, please see Nick’s SQuAD vs. Google Natural Questions blog post.&lt;/p&gt;

&lt;h2 id=&quot;downloading-the-data&quot;&gt;Downloading the Data&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Or, how to actually access 40GB worth of question-answer pairs for training&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our goal was to train on the Google Natural Questions dataset using Huggingface’s run_squad.py script. To do so, we first had to convert the Google Natural Questions examples into SQuAD format. (This might seem like a convoluted approach but it’s actually one of the easiest ways to train with a new question answer dataset, since the Huggingface transformers library has functions specifically designed to process and evaluate SQuAD-formatted examples.)&lt;/p&gt;

&lt;p&gt;To get the data for reformatting, you can either download it directly to your computer or you can upload it to an Amazon S3 bucket and stream it into your reformatting script from there. S3 buckets are an AWS service that allow you to store data in the cloud and share it with others.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Downloading Google Natural Questions to your local computer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you have 40 gb of room on your local computer, you can download all the GNQ files using the google command line interface gsutil &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions/download&quot;&gt;as described on the GNQ homepage&lt;/a&gt;. (We didn’t want to use the simplified train data that is directly linked to on the homepage because it didn’t give all the information we needed for our reformatting script.&lt;/p&gt;

&lt;p&gt;Alternatively, you can download the files directly within your reformatting script, as demonstrated in &lt;a href=&quot;https://github.com/nyrnick/NLM-LM-Devel/blob/master/convert_and_download_las_as_context.py&quot;&gt;our code&lt;/a&gt;, by using the python request library to download the files from the urls of &lt;a href=&quot;https://console.cloud.google.com/storage/browser/natural_questions/v1.0/dev?authuser=0&quot;&gt;the google bucket where they are publicly stored&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streaming Google Natural Questions from an S3 Bucket&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you don’t have enough storage to download the files to your local computer, you can use gsutil to upload the data to an Amazon S3 bucket and then stream the data into your python reformatting code from the bucket without needing to download it first. To do so, follow these steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Make an AWS S3 bucket to hold the data&lt;/li&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://cloud.google.com/storage/docs/gsutil_install&quot;&gt;gustil&lt;/a&gt; and &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html&quot;&gt;AWS CLI&lt;/a&gt;, which are command line interaces for Google and AWS, respectively&lt;/li&gt;
  &lt;li&gt;Configure your AWS cli following &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html&quot;&gt;these instructions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Check that everything works by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 ls&lt;/code&gt; in your terminal, which displays the list of buckets associated with your AWS account&lt;/li&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil -m cp -R gs://natural_questions/v1.0 s3://your_bucket_name&lt;/code&gt; to download all data, or to download individual files replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://natural_questions/v1.0&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://natural_questions/v1.0/dev/nq-dev-00.jsonl.gz&lt;/code&gt; (there are 5 dev files altogether, so repeat this command through “dev-04”) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://natural_questions/v1.0/train/nq-train-01.jsonl.gz&lt;/code&gt; (there are 50 train files altogether, so repeat the command through “train-49”)&lt;/li&gt;
  &lt;li&gt;To stream the data from the s3 bucket in a python script, follow &lt;a href=&quot;https://www.slsmk.com/use-boto3-to-open-an-aws-s3-file-directly/&quot;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Downloading SQuAD Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to use the SQuAD dataset instead of Google Natural Questions to follow our tutorial, the data can be downloaded from https://rajpurkar.github.io/SQuAD-explorer/ using the links “Training Set v2.0 (40MB)” and “Dev Set v2.0 (4MB)” for the training and dev sets respectively on the left hand side bar.&lt;/p&gt;

&lt;h2 id=&quot;processing-data-from-a-zip-file&quot;&gt;Processing Data from a Zip File&lt;/h2&gt;

&lt;p&gt;The Google Natural Questions training files take up 40 GB even when contained in gzip (similar to zip) format. When unzipped into json format, any one of the files is large enough to likely crash whatever program you try to open them with. Therefore, when processing Natural Questions or any similarly large dataset in python, it is easiest to do so straight from a zip file.&lt;/p&gt;

&lt;p&gt;To process a &lt;strong&gt;gzip&lt;/strong&gt; in python, import gzip library and use a command such as&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gzip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'v1.0_train_nq-train-00.jsonl.gz'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
	    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;To process a &lt;strong&gt;zip&lt;/strong&gt; file, import zipfile library and use a command like&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zipfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ZipFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unzippedfilename&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   	    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   	        &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;reformatting-google-nq-data-to-squad-format&quot;&gt;Reformatting Google NQ Data to SQuAD Format&lt;/h2&gt;

&lt;p&gt;We wrote a python script that can be found &lt;a href=&quot;https://github.com/nyrnick/NLM-LM-Devel/blob/master/convert_and_download_las_as_context.py&quot;&gt;here&lt;/a&gt; for reformatting the data on our local computers before uploading the data to our EC2 instance. Since the way boths datasets are formatted is so different, we had to make certain decisions about how to reformat Google Natural Questions, like using the long answers as context and the short answers as answers. (Entire Wikipedia articles took way too long to process.)&lt;/p&gt;

&lt;p&gt;To hear more about the decisions we made when reformatting, see our code, linked above, and the SQuAD vs. Google Natural Questions blogpost.&lt;/p&gt;

&lt;h2 id=&quot;part-3-training-the-model&quot;&gt;PART 3: TRAINING THE MODEL&lt;/h2&gt;

&lt;h2 id=&quot;model-comparisons&quot;&gt;Model Comparisons&lt;/h2&gt;

&lt;p&gt;There are a number of different deep learning language models out there, many of which are based on BERT. For our purposes we choose ALBERT, which is a smaller and more computationally manageable version of BERT. For more detailed information on the different language models see here: (https://github.com/huggingface/transformers#model-architectures).&lt;/p&gt;

&lt;h2 id=&quot;understanding-the-parameters-of-run_squadpy&quot;&gt;Understanding the Parameters of run_squad.py&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Almost time to launch the training!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Script parameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Below is a list of parameters that we used for running run_squad.py. Note that not all of the parameters are necessary (only model_type, model_name_or_path and output_dir are needed for the script to run).&lt;/p&gt;

&lt;p&gt;--_model_type_: This is the type of model you are using. The main types are (‘distilbert’, ‘albert’, ‘bart’, ‘longformer’, ‘xlm-roberta’, ‘roberta’, ‘bert’, ‘xlnet’, ‘flaubert’, ‘mobilebert’, ‘xlm’, ‘electra’, ‘reformer’). We used ‘albertxlv2’.&lt;/p&gt;

&lt;p&gt;--_model_name_or_path_: This is where you indicate the specific pretrained model you want to use from https://huggingface.co/models, we used albert-xlarge-v2. A list of Huggingface provided pretrained models is here https://huggingface.co/transformers/pretrained_models.html. If you want to load a model from a checkpoint, which we will explain how to do later, you would provide the path to the checkpoint here.&lt;/p&gt;

&lt;p&gt;--_output dir_: This is where you specify the path to the directory you want the model outputs to go. The directory should be empty.&lt;/p&gt;

&lt;p&gt;--_do_train_: This indicates that you want to train your model.&lt;/p&gt;

&lt;p&gt;--_do_eval_: This indicates that you want to evaluate the performance of your model.&lt;/p&gt;

&lt;p&gt;--_train_file_: This is the path to the file with your training data.&lt;/p&gt;

&lt;p&gt;--_predict_file_: This is the path to the file with your evaluation data.&lt;/p&gt;

&lt;p&gt;--_learning_rate_: Determines the step size when moving towards minimizing the loss. We used 3e-5.&lt;/p&gt;

&lt;p&gt;--_num_train_epochs_: Determines the number of times you want to go through the dataset. For our training the metrics started to plateau around the end of the third epoch. We would recommend 3 to 5 epochs, but you can stop training prematurely if you see your metrics plateauing before the training has finished.&lt;/p&gt;

&lt;p&gt;--_max_seq_length_: The maximum length of a sequence after tokenization. We used 384, which is the default.&lt;/p&gt;

&lt;p&gt;--_threads_: The number of vCPU threads you want to use for the process of converting examples to features. Check how many vCPU threads you have, it should be the number of vCPU cores times 2. We used 128 threads.&lt;/p&gt;

&lt;p&gt;--_per_gpu_train_batch_size_: The batch size for training you use per gpu, aka the number of examples the model will look at in one iteration. That is if you have 4 gpus and your --_per_gpu_train_batch_size_ is 4, your total batch size will be 16. A larger batch size will result in a more accurate gradient, but a smaller batch size will make the training go faster and use less memory. We used 1.&lt;/p&gt;

&lt;p&gt;--_per_gpu_eval_batch_size_: Same thing as &lt;em&gt;per_gpu_train_batch_size&lt;/em&gt; except for evaluation. We used 8.&lt;/p&gt;

&lt;p&gt;--_version_2&lt;em&gt;with_negative&lt;/em&gt;: If you are using the SQuAD V2 dataset with negative examples use this flag.&lt;/p&gt;

&lt;p&gt;--_evaluate_during_training_: Determines if you want to evaluate during training, but note that this will only work if you are only using 1 gpu due to averaging concerns.&lt;/p&gt;

&lt;p&gt;--_fp_16_: Determines if you want to use 16 point precision instead of the default 32 point precision. This will decrease the amount of ram you use and decrease the training time. Note if you want to use this flag you must have Nvidia apex installed from https://www.github.com/nvidia/apex (as described above).&lt;/p&gt;

&lt;p&gt;--_gradient_accumulation_steps_: Makes gradient descent more stable (in a similar training to the one described here, we used 4)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screen command&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We know you’re probably excited to start training, but here are a few helpful commands before jumping in.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before you start training your model there is one important terminal functionality you should use. Since you don’t want the training process to stop every time your terminal disconnects from the EC2 instance, which will happen after periods of inactivity, you need to disconnect the process of training the model from the EC2 terminal. To do this, go to the terminal logged into your EC2 instance and run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen&lt;/code&gt; which will create a blank terminal screen. Once you have done that you can run the command to start the training.&lt;/p&gt;

&lt;p&gt;(Note that in order for a virtual environment to work within the screen, you must be running the base environment outside of the screen. Before activating the screen, if you are in a virtual environment, run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt; to return to the base environment, and only activate your virtual environment once you have entered the screen).&lt;/p&gt;

&lt;p&gt;The command consists of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python run_squad.py&lt;/code&gt; followed by all of the flags you want to use for your training.&lt;/p&gt;

&lt;p&gt;Here is the command we ran:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python run_squad.py &lt;span class=&quot;nt&quot;&gt;--model_type&lt;/span&gt; ALBERTxlv2 &lt;span class=&quot;nt&quot;&gt;--model_name_or_path&lt;/span&gt; ALBERT-xlarge-v2 &lt;span class=&quot;nt&quot;&gt;--do_train&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--do_eval&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--evaluate_during_training&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--train_file&lt;/span&gt; ~/real_data/modified_GNQ_las_as_context_1_10th_train.json &lt;span class=&quot;nt&quot;&gt;--predict_file&lt;/span&gt; ~/real_data/modified_GNQ_las_as_context_1_10th_dev.json &lt;span class=&quot;nt&quot;&gt;--learning_rate&lt;/span&gt; 3e-5 &lt;span class=&quot;nt&quot;&gt;--num_train_epochs&lt;/span&gt; 7 &lt;span class=&quot;nt&quot;&gt;--max_seq_length&lt;/span&gt; 384 &lt;span class=&quot;nt&quot;&gt;--doc_stride&lt;/span&gt; 128 &lt;span class=&quot;nt&quot;&gt;--output_dir&lt;/span&gt; ~/reformatted_outputs &lt;span class=&quot;nt&quot;&gt;--per_gpu_eval_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8 &lt;span class=&quot;nt&quot;&gt;--per_gpu_train_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nt&quot;&gt;--fp16&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--threads&lt;/span&gt; 128 &lt;span class=&quot;nt&quot;&gt;--version_2_with_negative&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you start the training and see that it has started without any initial errors you can hit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;control a&lt;/code&gt; and then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;control d&lt;/code&gt; which will return you to your previous terminal screen with the training running in the background. You can now disconnect from the EC2 instance without affecting the training. Please note that you must not actually turn the instance off during training or the training will be halted. To reconnect to the screen where the training is running, run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen -r&lt;/code&gt;. It is a good idea to check in on the training once in a while to make sure it has not halted, which can be easily accomplished by looking at the wandb logging, and you can even set your wandb account to send you email alerts if the training fails.&lt;/p&gt;

&lt;h2 id=&quot;common-errors-we-encountered-and-how-to-fix-them&quot;&gt;Common Errors We Encountered and How to Fix Them&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;To save you much of the frustration we endured, here are some errors we encountered and information on how to deal with them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you want to be proactive about your training you could look through this section before you start the training and take measures to prevent any possible errors you may feel are likely to come up.&lt;/p&gt;

&lt;p&gt;The two main errors we encountered were running out of memory (RAM) and computer storage within our EC2 instance. (The commands presented here are meant to be run while logged into your instance, and assume that your instance runs on a linux operating system.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to manage storage:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To see your current storage usage, run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df -h&lt;/code&gt;. To find the largest files on your instance, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo du -x -h / | sort -h | tail -40&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/file-sizes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you find yourself running out of storage there are a number of ways to get around it.&lt;/p&gt;

&lt;p&gt;Firstly, you could simply delete unneeded files on your amazon aws instance. To do that, navigate to the directory the files or folder you want to delete are located. Then run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm 'filename'&lt;/code&gt; to delete a single file, or the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm -R ‘foldername’&lt;/code&gt; if you want to delete a folder and all files in it. You can also delete virtual environments you aren’t using that come preinstalled on the EC2 instance. (For running run_squad.py, you only need the pytorch_latest_p36.) Do so by running the command ‘conda remove –name ‘myenv’ –all’ where myenv is the name of the environment you’d like to delete. A list of all installed environments is shown when starting up the instance.&lt;/p&gt;

&lt;p&gt;Another method is to simply add more storage to your machine through AWS. You can do this by simply going to the “volumes” section under the elastic block store header in the EC2 section of AWS. Once you are there select the instance you want to add more storage to, click the actions drop down menu, and then select modify volume. From here you can add as much storage as you like, but keep in mind you will be charged for it. See pricing info here: https://aws.amazon.com/ebs/pricing/. Note that you can only increase the volume size and not decrease it, so once you add more storage there is no going back.&lt;/p&gt;

&lt;p&gt;There are other methods for dealing with lack of storage issues, such as saving checkpoints to an s3 bucket as training runs, but we did not end up needing to use them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to manage memory (RAM, different than storage space):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To figure out your free memory in your instance at a given moment run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;free -h&lt;/code&gt;. A good way to keep track of your memory usage throughout the run is to look at the wandb logging in the system section. If you keep encountering out of memory issues, there are a few ways to get around it.&lt;/p&gt;

&lt;p&gt;A way of increasing your overall memory is to use something called swap space. If you are not familiar with swap space, it basically involves partitioning part of your storage to be used for memory. A detailed guide can be found here: https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04.&lt;/p&gt;

&lt;p&gt;One method to decrease memory usage is to decrease your batch size. Note that if you are having out of memory issues in the convert examples to features part of the training process this will not help.&lt;/p&gt;

&lt;p&gt;Another method to decrease memory usage is to run the run_sqad.py script with the –fp_16 flag on if you are not already.&lt;/p&gt;

&lt;p&gt;If you are getting out of memory issues in the convert examples to features part of the training process, it might be necessary to decrease the size of your dataset.&lt;/p&gt;

&lt;p&gt;One final word of advice to avoid random errors is to make sure that you have the most recent version of the transformers library installed, and you are using the most recent run_squad.py script. We encountered some errors related to mismatched versions.&lt;/p&gt;

&lt;p&gt;Note:
Besides looking at the Wandb logging, you can use the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvidia-smi&lt;/code&gt; to view available gpus and current gpu usage on your instance, and the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top&lt;/code&gt; to see your current cpu usage, as well as your memory usage.&lt;/p&gt;

&lt;h2 id=&quot;restoring-training-from-a-checkpoint&quot;&gt;Restoring Training From a Checkpoint&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Annoyed you have to start from the beginning after your model training failed? Well, you don’t have to! Here’s how you can resume training from a checkpoint.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;After every 500 examples that the model trains on, it will output a “model checkpoint” that is a representation of the model at that point in training. The model checkpoints can be found in the output folder, and each one is stored in a folder called “checkpoint-x” where x is the number of examples the model had looked at when that checkpoint was generated. Resuming training from a checkpoint can save you a lot of time and convenience.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/checkpoints.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you encounter an error and want to pick up the training where you left off, or just want to resume training from a checkpoint for any other reason, here’s how to do it.&lt;/p&gt;

&lt;p&gt;All that is necessary is to run the same command you ran to start the training except &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--model_name_or_path&lt;/code&gt; must be set to the path to the folder for the checkpoint you want to resume training from. For example if your output folder is called outputs-2 and you want to resume training from checkpoint-1500 you should have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--model_name_or_path /path_to_outputs_folder/outputs-2/checkpoint-1500&lt;/code&gt;. For consistency, if you’re restarting training from the middle, you may want to see what the learning rate was at the checkpoint you’re using (on wandb), and use that as the starting learning rate for your new training.&lt;/p&gt;

&lt;p&gt;Note that if you do end up stopping your training early due to the metrics leveling off, you will have stopped the script before it reached the evaluation section, so if you want the evaluation results you will have to run the script again with only the –do_eval flag, and not the –do_train flag, and –model_name_or_path should be set to the path to the checkpoint you want to evaluate.&lt;/p&gt;

&lt;p&gt;Also note that if you previously ran an evaluation on a checkpoint, and now you want to evaluate that checkpoint using a different data file than you originally used, be sure to delete the any cached dataset files associated with that checkpoint (which will have names like “cached_dev_checkpoint-7000-v2_384”), so that your evaluation runs on the new data file instead of the old cached data.&lt;/p&gt;

&lt;h2 id=&quot;understanding-your-model-outputs&quot;&gt;Understanding Your Model Outputs&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What’s the point in training a model if we can’t understand what it’s telling us? Here is a primer on comprehending model terms.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Understanding some run_squad evaluation metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These metrics allow you to judge how well your model learned the dataset that you gave to it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‘Exact’ - compares answers that the model predicts for the dev set questions with the real answers, returns percentage of answers that model predicted correctly (eg answer spans match exactly) out of total dev set examples&lt;/li&gt;
  &lt;li&gt;‘F1’ - f1 is an accuracy measure that equals 2&lt;em&gt;((precision&lt;/em&gt;recall)/(precision+recall)), where &lt;strong&gt;precision&lt;/strong&gt; is the number of true positives over all positive results that the test returns, and &lt;strong&gt;recall&lt;/strong&gt; is the number of true true positives over everything that should have been identified as positive.
Here, precision and recall are computed by splitting the correct answers and the predicted answers into tokens by word. The number of tokens that appear in both answers is counted. This intersecting token count is divided by the number of tokens in the predicted answer to find the precision, and divided by number of tokens in the real answer to find recall.
The final f1 score returned is the sum of f1 scores for each example in the dev set divided by the total number of examples.&lt;/li&gt;
  &lt;li&gt;‘Total’ - total number of examples in dev set&lt;/li&gt;
  &lt;li&gt;‘HasAns_exact’ - average exact score (computed as above) but only taking the average for all examples in the dev set that have answers (e.g. not impossible)&lt;/li&gt;
  &lt;li&gt;‘HasAns_f1’ - average f1 score over all examples in dev set that have answers&lt;/li&gt;
  &lt;li&gt;‘HasAns_total’ - total # of examples in dev set that have answers&lt;/li&gt;
  &lt;li&gt;‘NoAns_exact’ - average exact score over all examples in dev set that have no answer (e.g. are impossible)
‘NoAns_f1’ - average f1 score over all examples in dev set that have no answer&lt;/li&gt;
  &lt;li&gt;‘NoAns_total’ - total number of examples in dev set that have no answer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Understanding the final model outputs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every checkpoint folder has files containing information about the model, optimizer, tokenizer, scheduler, tokens, and the training arguments. When the model has finished evaluating, it will output a predictions_.json file, a nbest_predections_.json file, and a null_odds_.json if you are using the SQuAD V2 dataset. The predictions file contains the model prediction for each example in the evaluation file, while the nbest_predictions file contains the n best predictions for each example in the evaluation file, where n is by default 20.&lt;/p&gt;

&lt;h2 id=&quot;downloading-your-model-uploading-to-s3-bucket&quot;&gt;Downloading Your Model/ Uploading to S3 Bucket&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Congratulations! You’ve officially trained a ML model for Question Answering! Now that we have our model, where should we store it for future use?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once your model is trained, the last step is to download it from the EC2 instance so that you and others can access it. (Once an EC2 instance is deleted the information on it also is, so be sure to save the model information you want before terminating your instance for good.) You can save all checkpoints that were stored throughout training, or you can choose to save only the best checkpoint based on evaluation metrics (be aware that the best checkpoint is not necessarily the last.)&lt;/p&gt;

&lt;p&gt;To download the checkpoints to your local computer, use the scp command described above for file transfers from SSH. In order to share the model with others, or to upload it to Huggingface for public use, you will need to upload it to an Amazon s3 bucket. 
You can upload locally downloaded files to the s3 bucket using the s3 section of the AWS console.&lt;/p&gt;

&lt;p&gt;Alternatively, you can download the AWS client software onto your EC2 instance and upload the model files directly to a bucket without first downloading to your local computer. Do this by creating an s3 bucket from the s3 web console. Then connect the s3 bucket to your instance using an IAM role and the AWS cli software by following the steps outlined in &lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/&quot;&gt;this post&lt;/a&gt;. Once you have downloaded AWS cli, you can use the command “aws s3 cp filetocopy.txt s3://mybucketpath” to copy files into the bucket. Set permissions for public access using the command flags described &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html#using-s3-commands-managing-objects&quot;&gt;here&lt;/a&gt;, or set permissions directly from the s3 web console.&lt;/p&gt;

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;
&lt;p&gt;https://medium.com/@dearsikandarkhan/files-copying-between-aws-EC2-and-local-d07ed205eefa – file transfer between EC2 and local\
https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04 – adding swap space\
https://medium.com/@arnab.k/how-to-keep-processes-running-after-ending-ssh-session-c836010b26a3 – screen command\
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html – authorizing inbound traffic to instance\
https://github.com/huggingface/transformers#run_squadpy-fine-tuning-on-squad-for-question-answering
https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ – meaning of f1 score\
http://www.kodyaz.com/aws/list-all-amazon-EC2-instances-using-aws-gui-tools.aspx – EC2 instance dashboard image&lt;/p&gt;</content><author><name>Batya Stein &amp; Nicholas Greenspan</name></author><summary type="html">by Batya Stein &amp;amp; Nicholas Greenspan</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/MLQAthumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/MLQAthumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>