<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-19T08:47:29-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nlmatics.github.io</title><subtitle>This is a collection of blogs on various topics including Natural Language Understanding, etc.</subtitle><entry><title type="html">Smooth Inverse Frequency (SIF) Embeddings in Golang</title><link href="http://localhost:4000/nlp/sentence-embeddings/2020/08/07/Smooth-Inverse-Frequency-Frequency-(SIF)-Embeddings-in-Golang.html" rel="alternate" type="text/html" title="Smooth Inverse Frequency (SIF) Embeddings in Golang" /><published>2020-08-07T05:45:00-04:00</published><updated>2020-08-07T05:45:00-04:00</updated><id>http://localhost:4000/nlp/sentence-embeddings/2020/08/07/Smooth%20Inverse%20Frequency%20Frequency%20(SIF)%20Embeddings%20in%20Golang</id><content type="html" xml:base="http://localhost:4000/nlp/sentence-embeddings/2020/08/07/Smooth-Inverse-Frequency-Frequency-(SIF)-Embeddings-in-Golang.html">&lt;h4 id=&quot;by-daniel-ye&quot;&gt;by Daniel Ye&lt;/h4&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;b&gt;Daniel Ye &lt;/b&gt;is a sophomore at Cornell University majoring in Computer Science and minoring in Operations Research and Information Engineering. I am interested in machine learning, natural language processing, and data science. I have worked on projects involving manufacturing data collection/analysis, greenhouse environmental regulation, and a multiplayer programming game. In my free time I enjoy playing tennis, running, hiking, and playing cello or guitar.Daniel was one of NLMatics’ 2020 summer interns.
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Smooth Inverse Frequency (SIF) Embeddings in Golang&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Daniel Ye&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#bookmark=id.uv0gxlx5wixm&quot;&gt;Introduction &lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#bookmark=id.pcgyzkz8chfm&quot;&gt;Motivation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#bookmark=id.vij0hbt6t1id&quot;&gt;Why GoLang?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#bookmark=id.mmsg2fvvb5zz&quot;&gt;GRPC Improvements&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#bookmark=id.i0zeii5wg08t&quot;&gt;Post-Processing Improvements&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#bookmark=id.aol4ijt37be&quot;&gt;Productionalizing With Docker&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#bookmark=id.ouhqmef3a5d5&quot;&gt;Conclusion&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;**1. Introduction **&lt;/p&gt;

&lt;p&gt;I am a rising junior majoring in computer science and minoring in operations research and information engineering at Cornell Engineering. This summer, I interned at NLMatics, and one of the projects I worked on was implementing a &lt;a href=&quot;https://openreview.net/pdf?id=SyK00v5xx#page=12&amp;amp;zoom=100,110,217&quot;&gt;Smooth Inverse Frequency&lt;/a&gt; model using Golang. This is able to calculate sentence embeddings from sequences of words in the form of vectors, which mathematically represent the meaning of the sentence. We use it to encode documents and queries into embeddings which are then processed further using other natural language processing models to get search results. However, our original Python implementation was fairly slow at calculating these embeddings, and it scaled poorly with increasing document sizes or concurrent requests, so we needed to find a way to speed up the service.&lt;/p&gt;

&lt;p&gt;Over the course of about four weeks, I worked on combating this issue by developing a Golang implementation of the model and switching from HTTP 1.0 protocol to gRPC protocol for the server. This increased the amount of concurrent processing we were able to utilize, and reduced overhead for connecting and sending requests to the server, speeding up the service greatly. Ultimately, I was able to build a system that generated more accurate sentence embeddings at much faster speeds.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Word embeddings are one of the most important developments in the field of modern Natural Language Processing. Translating the meaning behind words and the semantic relationships between them into measurable quantities is a crucial step in processing language. Many words, such as “cat” and “dog” or “Mozart” and “Beethoven” have almost no physical characteristics that would reveal their similarities. Instead, modern algorithms like Google’s &lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;Word2Vec&lt;/a&gt; developed in 2013 or Stanford’s &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;&gt;GloVe&lt;/a&gt; essentially count the cooccurrences of words with other words, and condense these values into dense, relatively low-dimensional vectors. Their models train on massive corpora of English text such as all of Wikipedia, and embed words as vectors based on which other words they appear in proximity to. So, if “cat” and “dog” are found together in many sentences or documents, they will have very similar vector values. This method is able to capture not only semantic similarity, but also analogies (woman is to man as king is to __) and the effects of prefixes or suffixes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_0.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;semantic relationships represented by Word2Vec and GloVe&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A natural next step in the field was the development of sentence embeddings, or being able to extract meaning from a sequence of words. Early methods include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;TF-ID**F&lt;/a&gt;&lt;/strong&gt; **&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;**&lt;a href=&quot;https://arxiv.org/pdf/1511.08198.pdf&quot;&gt;Paragram Phrase (PP**)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;**&lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;Recurrent Neural Network (RNN**)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;**&lt;a href=&quot;https://arxiv.org/pdf/1503.00075.pdf&quot;&gt;Long Short-Term Memory Networks (LSTM**)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;**&lt;a href=&quot;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&quot;&gt;Deep Averaging Network (DAN**)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In 2017, Arora et. al proposed SIF, or &lt;a href=&quot;https://openreview.net/pdf?id=SyK00v5xx#page=12&amp;amp;zoom=100,110,217&quot;&gt;Smooth Inverse Frequency&lt;/a&gt;, a weighting scheme to improve performance of sentence embeddings. When encoding a sentence, it is important to identify which words in the sentence are more significant. For example, if calculating the embedding of the sentence “who was Mozart?” the word “was” doesn’t add much meaning; looking for sentences or documents relating to the word “was” will not yield any useful results for the original question. It’s clear that “Mozart” holds the most meaning in the question from a human standpoint, but how do you program a machine to identify that? SIF operates under the assumption that the most important words tend to also be used less frequently. If you counted all the words in Wikipedia, the word “was” would most likely appear much more frequently than in “Mozart”. Weights of a word &lt;em&gt;w&lt;/em&gt; are computed by &lt;em&gt;a/(a + p(w))&lt;/em&gt; where a is a parameter and p(w) is the word frequency of w, which can be estimated by scraping a large corpus.* &lt;em&gt;The hyperparameter *a&lt;/em&gt; adjusts which words are quantitatively “common” and “uncommon.” Here is the formal algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_1.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Arora et. al found that despite its simplicity, SIF worked surprisingly well on semantic text similarity (STS), entailment, and sentiment tasks. STS tasks involve scoring pairs of sentences from 0-5 based on how similar their meanings are, which are then checked against a golden standard of human generated scores. For example, “The bird is bathing in the sink” and “Birdie is washing itself in the water basin” should receive a 5. Entailment tasks involve identifying if one sentence &lt;em&gt;entails&lt;/em&gt; that another one is true. For example, if you read the sentence “There is a soccer game with multiple males playing,” you could infer that the sentence “Several men are playing a sport” is true. Thus the first sentence, commonly referred to as the &lt;em&gt;text&lt;/em&gt; entails the following, also known as the &lt;em&gt;hypothesis&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_2.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;img src=&quot;image_3.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tables detailing SIF performance on various semantic tasks. “GloVe + WR”, “PSL + WR”, and “Ours” correspond to SIF systems.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Due to its effectiveness and simplicity, SIF is an incredibly practical method of embedding sentences for commercial or enterprise products that rely on both accurate and fast results while consuming low amounts of resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Why Golang?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/PrincetonML/SIF&quot;&gt;original code&lt;/a&gt; corresponding to the paper describing SIF is implemented in Python, which by design has a &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;Global Interpreter Lock (GIL)&lt;/a&gt;, a mutex that prevents multi threaded processes from utilizing multiple cores of a processor. We hosted our Cython implementation of the SIF embedding model on a cloud service, which provided us with multiple cores of processing power. However, the GIL meant that we could not make use of the full processing power we were paying for.&lt;/p&gt;

&lt;p&gt;GoLang however, has no such restrictions and also provides built-in structures for concurrency through the form of Goroutines, a form of very lightweight threads.  They are organized by channels, which allow goroutines to either block on awaiting input or signal that they have completed.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;func main() {
    jobs := make(chan int, 100)
    results := make(chan int, 100)
    for i := 0; i &amp;lt; 4; i++ {
        go worker(jobs, results)
    }
    for i := 0; i &amp;lt; 100; i++ {
        jobs &amp;lt;- i
    }
    close(jobs)
    for j := 0; j &amp;lt; 100; j++ {
        fmt.Println(&amp;lt;-results)
    }
}
func worker(jobs &amp;lt;-chan int, results chan&amp;lt;- int) {
    for n := range jobs {
        results &amp;lt;- fib(n)
    }
}

func fib(n int) int {
    if n &amp;lt;= 1 {
        return n
    }
    return fib(n-1) + fib(n-2)
}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Basic architecture of a worker pool using goroutines&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is important to note that this structure does not enforce the order in which workers complete their jobs or the order in which their results are sent to the results channel. Executing this code won’t necessarily return the fibonacci numbers in their correct order.&lt;/p&gt;

&lt;p&gt;Our API allowed for clients to send multiple sentences at a time to be encoded by the SIF, often many at a time, and implementing the SIF in Golang allowed us to leverage powerful concurrency to speed up our calculations. My first iteration of the SIF server used &lt;a href=&quot;https://golang.org/pkg/net/http/&quot;&gt;Golang’s http package&lt;/a&gt; to host the model on an HTTP 1.0 server. I compared it with  our old system, as well as the original Python implementation. I used three different independent variables to benchmark how the performance scaled up: total words per ‘sentence’ to be embedded, total number of calls with fixed number of concurrent requests, and number of concurrent requests with fixed total number of calls. These benchmarks were made using &lt;a href=&quot;https://httpd.apache.org/docs/2.4/programs/ab.html&quot;&gt;Apache Benchmark&lt;/a&gt;, a software that allows you to make many concurrent requests to a server and reports significant timing data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_4.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_5.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_6.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The results were impressive, to say the least. At a sentence level, around 10-100 words, the new implementation outperforms the old by up to &lt;strong&gt;30x&lt;/strong&gt; and at a document level, around 1000-10000+ words, it is still &lt;strong&gt;10x&lt;/strong&gt;** **faster in almost every scenario. The improved speed for sentence level embeddings is really useful, since it means that we can embed at lower levels of granularity much more easily. For example, if you were searching “who was Mozart” and only had embeddings for each document, your system might flag a document about Joseph Haydn, friend and mentor of Mozart, as relevant. As you descend levels of granularity of embeddings however, you can much more easily locate relevant information about your query. Perhaps it allows you to find a paragraph detailing their time in Vienna together, or a specific sentence about the pieces of music they worked on together.&lt;/p&gt;

&lt;p&gt;I found that Go works incredibly well for creating a web service capable of handling many concurrent requests efficiently. However, Go’s build in functionality does have a significant limitation of restricting HTTP requests to sizes of 1MB or less, which was not ideal for our use cases where we had to embed large amounts of text in a single request. For example, a legal company looking to process Google’s 2019 environmental and sustainability report would need about 11 MB of payload. Or a stressed-out undergraduate trying to find tips in &lt;em&gt;Cracking the Coding Interview&lt;/em&gt; would require around 90 MB of allowance. Additionally, every HTTP request requires a new connection to be made between the server and client, which adds a significant amount of overhead to our typical use case which often requires embedding many documents at once and sending many requests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. GRPC Improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Developed by Google, gRPC is an open source Remote Procedure Call framework and is what I turned to in order to hopefully remove the size limit and connection overhead problems with Go’s http package. gRPC processes payloads from requests using buffers, so it removes the 1MB size cap on requests. It also maintains connections between individual clients, so a single user can make multiple requests without having to create a connection more than once. It has its own Interface Definition Language called protocol buffers that also serve as a mechanism for serializing structured data and uses HTTP/2 to transport data. gRPC services are defined in .proto files:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;syntax = &quot;proto3&quot;
option go_package = “greeter”
package main
// The greeter service definition.
service Greeter {
  // Sends a greeting
  rpc SayHello (HelloRequest) returns (HelloReply) {}
}

// The request message containing the user's name.
message HelloRequest {
  string name = 1;
}

// The response message containing the greetings
message HelloReply {
  string message = 1;
}
&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;You can then use the protoc command to compile your service into any of &lt;a href=&quot;https://grpc.io/docs/languages/&quot;&gt;gRPC’s supported languages&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
//  protoc-gen-go v1.23.0
//  protoc        v3.6.1
// source: greeter.proto

package greeter

import (
    proto &quot;github.com/golang/protobuf/proto&quot;
    protoreflect &quot;google.golang.org/protobuf/reflect/protoreflect&quot;
    protoimpl &quot;google.golang.org/protobuf/runtime/protoimpl&quot;
    reflect &quot;reflect&quot;
    sync &quot;sync&quot;
)

const (
    // Verify that this generated code is sufficiently up-to-date.
    _ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
    // Verify that runtime/protoimpl is sufficiently up-to-date.
    _ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// This is a compile-time assertion that a sufficiently up-to-date version
// of the legacy proto package is being used.
const _ = proto.ProtoPackageIsVersion4

// The request message containing the user's name.
type HelloRequest struct {
    state         protoimpl.MessageState
    sizeCache     protoimpl.SizeCache
    unknownFields protoimpl.UnknownFields

    Name string `protobuf:&quot;bytes,1,opt,name=name,proto3&quot; json:&quot;name,omitempty&quot;`
}
//... excess code has been left out&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Due to how gRPC buffers data being received and sent, we no longer had to worry about size limits on requests to our server. gRPC servers also have a very helpful feature in that connections between client and server are maintained across multiple requests, whereas with HTTP requests, a new connection has to be established for every POST. As a result, I saw more improvements in performance as this overhead was removed from the new system:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_7.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The largest improvements are seen when requests have very small payloads, so the majority of time is spent on overhead from connecting to the server. However, once you get to larger payloads, the times converge to become about equal.&lt;/p&gt;

&lt;p&gt;**4. Post-Processing Improvements **&lt;/p&gt;

&lt;p&gt;A coworker sent me this &lt;a href=&quot;https://arxiv.org/pdf/1702.01417.pdf&quot;&gt;paper about post-processing word vectors&lt;/a&gt; in order to improve their representation of meaning. The algorithm essentially takes a list of pre-computed word vectors and performs principal component analysis on them, and then removes the top N components from every vector via Gram-Schmidt. Here is their formal algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;image_8.png&quot; alt=&quot;image alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Python’s Numpy and sklearn packages have all the built-in tools needed to implement this algorithm, which you can find the code for &lt;a href=&quot;https://github.com/daniel-ye137/WordVectorProcessing&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;import numpy as np
from sklearn.decomposition import PCA
import argparse


parser = argparse.ArgumentParser(description='postprocess word embeddings')
parser.add_argument(&quot;file&quot;, help=&quot;file containing embeddings to be processed&quot;)
args = parser.parse_args()
N = 2
embedding_file = args.file
embs = []

#map indexes of word vectors in matrix to their corresponding words
idx_to_word = dict()
dimension = 0
#append each vector to a 2-D matrix and calculate average vector
with open(embedding_file, 'rb') as f:
    first_line = []
    for line in f: 
        first_line = line.rstrip().split()
        dimension = len(first_line) - 1
        if dimension &amp;lt; 100 :
            continue
        print(&quot;dimension: &quot;, dimension)
        
        break
    avg_vec = [0] * dimension
    vocab_size = 0
    word = str(first_line[0].decode(&quot;utf-8&quot;))
    word = word.split(&quot;_&quot;)[0]
    # print(word)
    idx_to_word[vocab_size] = word
    vec = [float(x) for x in first_line[1:]]
    avg_vec = [vec[i] + avg_vec[i] for i in range(len(vec))]
    vocab_size += 1
    embs.append(vec)
    for line in f:
        line = line.rstrip().split()
        word = str(line[0].decode(&quot;utf-8&quot;))
        word = word.split(&quot;_&quot;)[0]
        idx_to_word[vocab_size] = word
        vec = [float(x) for x in line[1:]]
        avg_vec = [vec[i] + avg_vec[i] for i in range(len(vec))]
        vocab_size += 1
        embs.append(vec)
    avg_vec = [x / vocab_size for x in avg_vec]
# convert to numpy array
embs = np.array(embs)

#subtract average vector from each vector
for i in range(len(embs)):
    new_vec = [embs[i][j] - avg_vec[j] for j in range(len(avg_vec))]
    embs[i] = np.array(new_vec)

#principal component analysis using sklearn
pca = PCA()
pca.fit(embs)

#remove the top N components from each vector
for i in range(len(embs)):
    preprocess_sum = [0] * dimension
    for j in range(N):
        princip = np.array(pca.components_[j])
        preprocess = princip.dot(embs[i])
        preprocess_vec = [princip[k] * preprocess for k in range(len(princip))]
        preprocess_sum = [preprocess_sum[k] + preprocess_vec[k] for k in range(len(preprocess_sum))]
    embs[i] = np.array([embs[i][j] - preprocess_sum[j] for j in range(len(preprocess_sum))])

file = open(&quot;postprocessed_embeddings.txt&quot;, &quot;w+&quot;, encoding=&quot;utf-8&quot;)

#write back new word vector file
idx = 0
for vec in embs:
    file.write(idx_to_word[idx])
    file.write(&quot; &quot;)
    for num in vec:
        file.write(str(num))
        file.write(&quot; &quot;)
    file.write(&quot;\n&quot;)
    idx+=1
file.close()

print(&quot;Wrote: &quot;, len(embs), &quot;word embeddings&quot;)&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Using the new word embedding file, I saw meaningful improvements in semantic similarity tasks, similar to the results they found in their paper:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Dataset&lt;/td&gt;
    &lt;td&gt;k&lt;/td&gt;
    &lt;td&gt;Original&lt;/td&gt;
    &lt;td&gt;Post-Processed Vectors&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MSR Paraphrase&lt;/td&gt;
    &lt;td&gt;k = 1&lt;/td&gt;
    &lt;td&gt;76.43&lt;/td&gt;
    &lt;td&gt;78.34&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MS MARCO, 10000&lt;/td&gt;
    &lt;td&gt;k = 1&lt;/td&gt;
    &lt;td&gt;18.2002&lt;/td&gt;
    &lt;td&gt;20.4&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MSR Paraphrase&lt;/td&gt;
    &lt;td&gt;k = 3&lt;/td&gt;
    &lt;td&gt;87.122&lt;/td&gt;
    &lt;td&gt;88.84&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MS MARCO, 10000&lt;/td&gt;
    &lt;td&gt;k = 3&lt;/td&gt;
    &lt;td&gt;26.915&lt;/td&gt;
    &lt;td&gt;29.37&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MSR Paraphrase&lt;/td&gt;
    &lt;td&gt;k = 5&lt;/td&gt;
    &lt;td&gt;89.37944567&lt;/td&gt;
    &lt;td&gt;90.92&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MS MARCO, 10000&lt;/td&gt;
    &lt;td&gt;k = 5&lt;/td&gt;
    &lt;td&gt;33.05634374&lt;/td&gt;
    &lt;td&gt;36.2&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;I benchmarked the system on semantic text similarities tests using  the MSR Paraphrase dataset, as well as the first 10000 entries of the MS MARCO dataset using p@k with k = 1, 3, and 5. These tasks involved identifying a sentence from a large pool of candidates as being semantically equivalent to another. p@k testing introduces some leeway into the testing, where the program chooses the top K candidates from the pool as potential matches instead of only getting one attempt at the correct answer. This kind of testing is more representative of a search engine, where you care not only about getting the single best result, but also a reasonable amount of relevant information.&lt;/p&gt;

&lt;p&gt;The post-processed vectors outperformed the original in every case. I also benchmarked our system using a word frequency file generated by fellow intern Connie Xu using a June snapshot of all of Wikipedia. It had a far more comprehensive vocabulary than our current word frequency file, with over 50x as many entries, but performance results were inconclusive. The results do indicate, however, that post-processed word vectors also increase performance of sentence embedding systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Productionalizing With Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After finishing the code for our new SIF, my final step was to prepare it for production, which meant an inevitable encounter with Docker. Here is my Dockerfile, which I got from &lt;a href=&quot;https://www.callicoder.com/docker-golang-image-container-example/&quot;&gt;this tutorial:&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;# Start from the latest golang base image
FROM golang:latest

# Add Maintainer Info
LABEL maintainer=&quot;Daniel Ye &amp;lt;daniel.ye@nlmatics.com&amp;gt;&quot;

# Set the Current Working Directory inside the container
WORKDIR /app

# Copy go mod and sum files
COPY go.mod go.sum ./

# Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changed
RUN go mod download

# Copy the source from the current directory to the Working Directory inside the container
COPY . .

# Build the Go app
RUN go build -o main .

# Expose port 8080 to the outside world
EXPOSE 8080

# Command to run the executable
CMD [&quot;./main&quot;]&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Most components are fairly self-explanatory. It does require that you use Go modules to manage your dependencies, which you can read about &lt;a href=&quot;https://blog.golang.org/using-go-modules&quot;&gt;here&lt;/a&gt;. The steps I took to compile my code into a Docker image were as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create a go mod file in the root directory of your Golang project using go mod init &lt;project name=&quot;&quot;&gt;. Your project name can be anything you want, it does not have to correspond to any file or package names, although it probably should.&lt;/project&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Populate your go mod file with dependencies using go build. This will automatically detect the packages used by your project, and write them to your go mod file&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a Dockerfile in the root directory of your project&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Build your image using docker build -t &lt;project name=&quot;&quot;&gt; . Include the period!&lt;/project&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run your project with docker run -d -p 8080:8080 &lt;project name=&quot;&quot;&gt;&lt;/project&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;6. Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Improving the SIF implementation was a really interesting project. There were a lot of fun challenges involved like solving the ordering of goroutines and dealing with concurrent writes to map. It was incredibly satisfying to run my own benchmarks and see quantitative improvements in performance go as high as 30x the original speed. Of course, more improvements can still be made. &lt;a href=&quot;https://arxiv.org/pdf/2005.09069.pdf&quot;&gt;This paper&lt;/a&gt; details how SIF embeddings of documents can be improved by producing and then combining topic vectors. Other models for embedding sentences such as &lt;a href=&quot;https://arxiv.org/pdf/1803.11175.pdf&quot;&gt;Universal Sentence Encoder&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/pdf/1908.10084.pdf&quot;&gt;Sentence-BERT&lt;/a&gt; have been developed in recent years as well and are able to outperform SIF in certain categories of NLP tasks.&lt;/p&gt;</content><author><name>Daniel Ye</name></author><summary type="html">by Daniel Ye</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/SIFthumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/SIFthumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Speed up requests: Asyncio for Requests in Python</title><link href="http://localhost:4000/requests/speedup/asyncio/python/2020/08/06/Speed-up-requests-Asyncio-for-Requests-in-Python.html" rel="alternate" type="text/html" title="Speed up requests: Asyncio for Requests in Python" /><published>2020-08-06T20:00:00-04:00</published><updated>2020-08-06T20:00:00-04:00</updated><id>http://localhost:4000/requests/speedup/asyncio/python/2020/08/06/Speed%20up%20requests%20-%20Asyncio%20for%20Requests%20in%20Python</id><content type="html" xml:base="http://localhost:4000/requests/speedup/asyncio/python/2020/08/06/Speed-up-requests-Asyncio-for-Requests-in-Python.html">&lt;h4 id=&quot;by-connie-xu&quot;&gt;by Connie Xu&lt;/h4&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt; 
Connie Xu is currently studying Computer Science at Princeton University, where she has sharpened her skills in Algorithms and Data Structures as well as Linear Algebra. She is interested in learning more about Natural Language Processing and Machine Learning applications. In her free time, she watches cooking videos or practices beatboxing. Connie was one of NLMatics’ 2020 summer interns. 
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;speed-up-requests-asyncio-for-requests-in-python&quot;&gt;Speed up requests: Asyncio for Requests in Python&lt;/h1&gt;

&lt;h4 id=&quot;dont-be-like-this&quot;&gt;Don’t be like this.&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://memegenerator.net/img/instances/78137468/my-code-cant-run-slow-if-i-never-write-it.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you have probably already noticed because you decided to visit this page, requests can take forever to run, so here’s a nice blog written while I was an intern at NLMatics to show you how to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt; to speed them up.&lt;/p&gt;

&lt;h2 id=&quot;what-is-asyncio&quot;&gt;What is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;It is a Python library that uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;async/await&lt;/code&gt; syntax to make code run asynchronously.&lt;/p&gt;
&lt;h2 id=&quot;what-does-it-mean-to-run-asynchronously&quot;&gt;What does it mean to run asynchronously?&lt;/h2&gt;
&lt;h3 id=&quot;synchronous-normal-vs-asynchronous-using-asyncio&quot;&gt;Synchronous (normal) vs. Asynchronous (using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt;)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Synchronous:&lt;/strong&gt; you must wait for the completion of the first task before starting another task.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Asynchronous:&lt;/strong&gt; you can start another task before the completion of the first task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../site_files/connie_post_images/synchronous.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;../site_files/connie_post_images/asynchronous.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For more information on the distinction between concurrency, parallelism, threads, sync, and async, check out this &lt;a href=&quot;https://medium.com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d&quot;&gt;Medium article&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;simple-analogies&quot;&gt;Simple analogies&lt;/h2&gt;
&lt;h3 id=&quot;brick-and-mortar&quot;&gt;Brick and Mortar&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../site_files/connie_post_images/brick-and-mortar.jpg&quot; width=&quot;500&quot; height=&quot;334&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Simon and Ash are building 5 walls of brick.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Simon builds one wall and waits for it to set before starting to build the next wall (synchronous).&lt;/li&gt;
  &lt;li&gt;Ash, on the other hand, starts building the next wall before the first one sets (asynchronous).  &lt;br /&gt;&amp;lt;/br&amp;gt;
Ash starts the next task whereas Simon &lt;strong&gt;waits&lt;/strong&gt;, so Ash (asynchronous) will finish faster.
The lack of &lt;strong&gt;&lt;em&gt;waiting&lt;/em&gt;&lt;/strong&gt; is the key to why asynchronous programming provides a performance boost.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A good coding use case would be when you have a lot of time-consuming requests lined up with the outputs independent of each other. request1 takes a while to finish running, so instead of waiting, you start request2, which doesn’t affect the output of request1.&lt;/p&gt;

&lt;h3 id=&quot;laundry&quot;&gt;Laundry&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../site_files/connie_post_images/laundry.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Be wary that an asynchronous approach does not provide any performance boost when all the tasks are &lt;strong&gt;dependent&lt;/strong&gt; on each other. For example, if you are washing and drying clothes, you must wait for the clothes to finish washing first before drying them no matter what, because drying clothes is dependent on the output of the washing. There is no use in using an asynchronous approach, because the pipeline is just the same as a synchronous approach.&lt;/p&gt;

&lt;p&gt;The coding equivalent of this laundry example is when the output of request1 is used as the input in the request2.&lt;/p&gt;

&lt;p&gt;For a further look into when and when not to use asynchronous programming, check out this &lt;a href=&quot;https://stackify.com/when-to-use-asynchronous-programming/&quot;&gt;Stackify thread&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-syntax-do-i-need-to-know&quot;&gt;What syntax do I need to know?&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Syntax&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Example&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;async&lt;/td&gt;
      &lt;td&gt;Used to indicate which methods are going to be run asynchronously &lt;br /&gt; &amp;lt;p&amp;gt;	 → These new methods are called &lt;strong&gt;coroutines&lt;/strong&gt;.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../site_files/connie_post_images/async.png&quot; width=&quot;651&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;await&lt;/td&gt;
      &lt;td&gt;Used to run a coroutine once an asynchronous event loop has already started running &lt;br /&gt; → &lt;code&gt;await&lt;/code&gt; can only be used inside a coroutine &lt;br /&gt; → Coroutines must be called with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;await&lt;/code&gt;, otherwise there will be a &lt;code&gt;RuntimeWarning&lt;/code&gt; about enabling &lt;code&gt;tracemalloc&lt;/code&gt;. &lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../site_files/connie_post_images/await.png&quot; width=&quot;448&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;asyncio.run()&lt;/td&gt;
      &lt;td&gt;Used to start running an asynchronous event loop from a normal program &lt;br /&gt; → &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.run()&lt;/code&gt; cannot be called in a nested fashion. You have to use await instead. &lt;br /&gt; → &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.run()&lt;/code&gt; cannot be used if you are running the Python file in a Jupyter Notebook because Jupyter Notebook already has a running asynchronous event loop. You have to use await. (More on this in the Running the Code section)&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../site_files/connie_post_images/run.png&quot; width=&quot;622&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;asyncio.create_task()&lt;/td&gt;
      &lt;td&gt;Used to schedule a coroutine execution &lt;br /&gt; → Does not need to be awaited &lt;br /&gt; → Allows you to line things up without actually running them first.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../site_files/connie_post_images/create_task.png&quot; width=&quot;822&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;asyncio.gather()&lt;/td&gt;
      &lt;td&gt;Used to run the scheduled executions &lt;br /&gt; → Needs to be awaited &lt;br /&gt; → This is vital to the asynchronous program, because you let it know which is the next task it can pick up before finishing the previous one.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../site_files/connie_post_images/gather.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If you are thirsting for more in-depth knowledge on asyncio, check out these links:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://realpython.com/async-io-python/&quot;&gt;Async IO in Python: A Complete Walkthrough&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/library/asyncio.html&quot;&gt;asyncio: Python Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But with that, let’s jump straight into the code.&lt;/p&gt;

&lt;p&gt;Follow along with the Python file and Jupyter Notebook in this &lt;a href=&quot;https://github.com/nlmatics/asyncio-for-requests&quot;&gt;github repo&lt;/a&gt; that I developed for this post!&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;h3 id=&quot;preliminary&quot;&gt;Preliminary&lt;/h3&gt;

&lt;p&gt;Get imports and generate the list of urls to get requests from. Here, I use &lt;a href=&quot;https://jsonplaceholder.typicode.com/&quot;&gt;this placeholder url&lt;/a&gt;. Don’t forget to do &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -r requirements.txt&lt;/code&gt; in the terminal for all the modules that you don’t have. Normal &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requests&lt;/code&gt; cannot be awaited, so you will need to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import requests_async&lt;/code&gt; to run the asynchronous code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;import requests, requests_async, asyncio, time

itr = 200
tag = 'https://jsonplaceholder.typicode.com/todos/'
urls = []
for i in range(1, itr):
    urls.append(tag + str(i))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;synchronous&quot;&gt;Synchronous&lt;/h3&gt;

&lt;p&gt;This is what some typical Python code for requests would look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def synchronous(urls):
    for url in urls:
        r = requests.get(url)
        print(r.json())
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;asynchronous&quot;&gt;Asynchronous&lt;/h3&gt;

&lt;h4 id=&quot;incorrect-alteration&quot;&gt;Incorrect Alteration&lt;/h4&gt;
&lt;p&gt;The following is an understandable but bad alteration to the synchronous code. The runtime for this is the same as the runtime for the synchronous method, because you have not created a list of tasks that the program knows it needs to execute together, thus you essentially still have synchronous code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;async def asynchronous_fail(urls):
    for url in urls:
        r = await requests_async.get(url)
        print(r.json())
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;correct-alteration&quot;&gt;Correct Alteration&lt;/h4&gt;
&lt;p&gt;Create a list of tasks, and run all of them together using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.gather()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;async def asynchronous(urls):
    tasks = []
    for url in urls:
        task = asyncio.create_task(requests_async.get(url))
        tasks.append(task)
    responses = await asyncio.gather(*tasks)
    for response in responses:
        print(response.json())
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;running-the-code&quot;&gt;Running the Code&lt;/h2&gt;

&lt;h3 id=&quot;python&quot;&gt;Python&lt;/h3&gt;
&lt;p&gt;Simply add these three lines to the bottom of your Python file and run it.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;asyncio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asynchronous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you try to run this same code in Jupyter Notebook, you will get this error:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RuntimeError: asyncio.run&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; cannot be called from a running event loop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This happens because Jupyter is already running an event loop. More info &lt;a href=&quot;https://stackoverflow.com/questions/55409641/asyncio-run-cannot-be-called-from-a-running-event-loop&quot;&gt;here&lt;/a&gt;. You need to use the following:&lt;/p&gt;

&lt;h3 id=&quot;jupyter-notebook&quot;&gt;Jupyter Notebook&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asynchronous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;starttime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;ordering&quot;&gt;Ordering&lt;/h2&gt;

&lt;p&gt;Asynchronous running can cause your responses to be out of order. If this is an issue, create your own responses list and fill it up, rather than receiving the output from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio.gather()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;async def asynchronous_ordered(urls):
    responses = [None] * len(urls) # create own responses list
    tasks = []
    for i in range(len(urls)):
        url = urls[i]
        task = asyncio.create_task(fetch(url, responses, i))
        tasks.append(task)
    await asyncio.gather(*tasks) # responses is not set to equal this
    for response in responses:
        print(response.json())

async def fetch(url, responses, i):
    response = await requests.get(url)
    responses[i] = response # fill up responses list
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;batching&quot;&gt;Batching&lt;/h2&gt;

&lt;p&gt;Sometimes running too many requests concurrently can cause timeout errors in your resource. This is when you need to create tasks in batches and gather them separately to avoid the issue. Find the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size&lt;/code&gt; that best fits your code by experimenting with a smaller portion of requests. Requests that take longer to process (long server delay) are more likely to cause errors than others. In my own experience with NLMatic’s engine, MongoDB had timeout errors whenever I ran batches of size greater than 10.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;asynchronous_ordered_batched&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kiterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kiterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asyncio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asyncio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;runtime-results&quot;&gt;Runtime Results&lt;/h2&gt;

&lt;p&gt;|&lt;img src=&quot;../site_files/connie_post_images/table.png&quot; width=&quot;500&quot; /&gt;|&lt;img src=&quot;../site_files/connie_post_images/chart.png&quot; width=&quot;500&quot; /&gt;|
|—|—|&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;synchronous&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_fail&lt;/code&gt; have similar runtimes because the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_fail&lt;/code&gt; method was not implemented correctly and is in reality synchronous code.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered_batched&lt;/code&gt; have noticeably better runtimes in comparison to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;synchronous&lt;/code&gt; - up to 4 times as fast.&lt;/p&gt;

&lt;p&gt;In general, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered_batched&lt;/code&gt; gives fast and stable code, so use that if you are going for consistency. However, the runtimes of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered&lt;/code&gt; can sometimes be better than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous_ordered_batched&lt;/code&gt;, depending on your database and servers. So, I recommend using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asynchronous&lt;/code&gt; first and then adding extra things (order and batch) as necessary.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;As you have seen, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asyncio&lt;/code&gt; is a helpful tool that can greatly boost your runtime if you are running a lot of independent API requests. It’s also very easy to implement when compared to threading, so definitely try it out. Of course, make sure that your requests are independent.&lt;/p&gt;

&lt;p&gt;Now I must conclude by saying that using asyncio improperly implemented can cause many bugs, so sometimes it is not worth the hassle. If you really must, use my guide and use it  s p a r i n g l y.&lt;/p&gt;

&lt;p&gt;And that’s a wrap!&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d&quot;&gt;Concurrency, parallelism, threads, sync, and async&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackify.com/when-to-use-asynchronous-programming/&quot;&gt;When/when not to use asynchronous programming&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://realpython.com/async-io-python/&quot;&gt;Async IO in Python: A Complete Walkthrough&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/library/asyncio.html&quot;&gt;asyncio: Python Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Connie Xu</name><email>rob@sor.com</email></author><summary type="html">by Connie Xu</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/Asynciothumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/Asynciothumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SQUAD 2.0 and Google Natural Questions: A Comparison and Investigation into Model Performance</title><link href="http://localhost:4000/category-1/category-2/2020/08/06/SQUAD-2.0-and-Google-Natural-Questions-A-Comparison-and-Investigation-into-Model-Performance.html" rel="alternate" type="text/html" title="SQUAD 2.0 and Google Natural Questions: A Comparison and Investigation into Model Performance" /><published>2020-08-06T09:00:00-04:00</published><updated>2020-08-06T09:00:00-04:00</updated><id>http://localhost:4000/category-1/category-2/2020/08/06/SQUAD%202.0%20and%20Google%20Natural%20Questions-%20A%20Comparison%20and%20Investigation%20into%20Model%20Performance</id><content type="html" xml:base="http://localhost:4000/category-1/category-2/2020/08/06/SQUAD-2.0-and-Google-Natural-Questions-A-Comparison-and-Investigation-into-Model-Performance.html">&lt;h4 id=&quot;by-nick-greenspan&quot;&gt;by Nick Greenspan&lt;/h4&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt; 
Nicholas Greenspan lives in New York City and is a freshman at Rice University who is majoring in computer science. Nicholas is interested in Machine Learning, Natural Language Processing, and their applications to various fields. Nicholas recently worked at the UTHealth School of Biomedical Informatics working on a Natural Language Processing project to help doctors find relevant treatments for their patients, and is excited to work on more interesting and meaningful problems at NLMatics. Outside of CS, Nicholas likes to read, play ice hockey, and listen to many genres of music including indie rock and electronic. Nicholas was one of NLMatics’ 2020 summer interns
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt; SQUAD 2.0 and Google Natural Questions: &lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; A Comparison and Investigation into Model Performance &lt;/h2&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;SQuAD 2.0 and Google Natural Questions are two of the most prominent datasets in NLP Questioning Answering today. Both include tens of thousands of training examples which consist of a question, context, and an answer span. Though they both have the same general structure, there are many differences, both major and nuanced, that distinguish the two datasets. Whether you want to learn about the datasets for research purposes, or are trying to decide which one to use to train a QA model for your business, this is an in-depth guide to understanding the two datasets, their differences, and their relationships.&lt;/p&gt;

&lt;h2 id=&quot;overviews-from-the-dataset-websites&quot;&gt;Overviews from the Dataset Websites&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick_post/squad_image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD 2.0&lt;/a&gt;: “SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick_post/nq_image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.google.com/research/NaturalQuestions&quot;&gt;Google Natural Questions&lt;/a&gt;: “The NQ corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.”&lt;/p&gt;

&lt;h2 id=&quot;important-takeaways-for-each-dataset&quot;&gt;Important takeaways for each dataset&lt;/h2&gt;

&lt;p&gt;SQuAD 2.0:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Questions generated by hired workers whose task was to write questions about a given article.&lt;/li&gt;
  &lt;li&gt;Sets of questions are from paragraphs in a wikipedia article.
    &lt;ul&gt;
      &lt;li&gt;442 different articles.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are unanswerable questions (33.4% of the dataset is unanswerable), which forces a model to “know what it doesn’t know”.
    &lt;ul&gt;
      &lt;li&gt;Ensures there are plausible answers in the paragraph if the question is unanswerable so that model can’t just use superficial clues to determine if an answer is in the paragraph.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;All questions end with a question mark.&lt;/li&gt;
  &lt;li&gt;Since there are series of questions that refer to the same article, some of the questions use pronouns such as “where did she grow up?”.&lt;/li&gt;
  &lt;li&gt;There are some ungrammatical sentences such as: “Why political movement was named for Joseph McCarthy?”.&lt;/li&gt;
  &lt;li&gt;There are occasional misspellings.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Google Natural Questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Real, user generated questions by people who are actually seeking information, not people who were hired for the explicit purpose of writing questions.&lt;/li&gt;
  &lt;li&gt;Requires a model to read a whole Wikipedia article, not just a paragraph, and has two different tasks: identifying a long answer, which is the paragraph or table that the contains the information to answer the question, and the short answer, which is the exact text that provides the answer to the question.
    &lt;ul&gt;
      &lt;li&gt;Note that since the article is so long BERT models are unequipped to handle them as they have a max sequence length capped at 512.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;All questions aren’t necessarily “questions” per se, some of them are phrase searches like  “benefits of colonial life for single celled organisms”.&lt;/li&gt;
  &lt;li&gt;Questions don’t necessarily have a long answer or short answer, so there is some of the unanswerable functionality that is also present in SQuAD 2.0.&lt;/li&gt;
  &lt;li&gt;If a question has a short answer, it definitely has a long answer, but not the other way around.&lt;/li&gt;
  &lt;li&gt;There is a “Yes or No” answer field which is “Yes” if the answer is yes, “No” if the answer is no, and “None” if there is not a yes or no answer to the question. If the “Yes or No” answer is not “None” then there is no short answer.&lt;/li&gt;
  &lt;li&gt;All questions don’t end with a question mark.&lt;/li&gt;
  &lt;li&gt;Since only one question per article there is more variety of topics of questions.&lt;/li&gt;
  &lt;li&gt;There seem to be less ungrammatical or misspelled sentences.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datapoint-examples&quot;&gt;Datapoint Examples&lt;/h2&gt;

&lt;p&gt;In version 2 of the dataset, SQuAD 2.0 examples have two formats –\
For answerable questions:
 	 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;question&quot;: &quot;In what country is Normandy located?&quot;, &quot;id&quot;: &quot;56ddde6b9a695914005b9628&quot;, &quot;answers&quot;: [ { &quot;text&quot;: &quot;France&quot;, &quot;answer_start&quot;: 159 } ], &quot;is_impossible&quot;: false }&lt;/code&gt; \
For impossible questions:
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;plausible_answers&quot;: [ { &quot;text&quot;: &quot;Normans&quot;, &quot;answer_start&quot;: 4 } ], &quot;question&quot;: &quot;Who gave their name to Normandy in the 1000's and 1100's&quot;, &quot;id&quot;: &quot;5ad39d53604f3c001a3fe8d1&quot;, &quot;answers&quot;: [], &quot;is_impossible&quot;: true }&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Google Natural Questions (abbreviated due to length of document html and tokens):\
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{&quot;annotations&quot;:[{&quot;annotation_id&quot;:6782080525527814293,&quot;long_answer&quot;:{&quot;candidate_index&quot;:92,&quot;end_byte&quot;:96948,&quot;end_token&quot;:3538,&quot;start_byte&quot;:82798,&quot;start_token&quot;:2114},&quot;short_answers&quot;:[{&quot;end_byte&quot;:96731,&quot;end_token&quot;:3525,&quot;start_byte&quot;:96715,&quot;start_token&quot;:3521}],&quot;yes_no_answer&quot;:&quot;NONE&quot;}],&quot;document_html&quot;:&amp;lt;/HTML&amp;gt;\n&quot;,&quot;document_title&quot;:&quot;The Walking Dead (season 8)&quot;,&quot;document_tokens&quot;:[{&quot;end_byte&quot;:95,&quot;html_token&quot;:false,&quot;start_byte&quot;:92,&quot;token&quot;:&quot;The&quot;},{&quot;end_byte&quot;:103,&quot;html_token&quot;:false,&quot;start_byte&quot;:96,&quot;token&quot;:&quot;Walking&quot;},{&quot;end_byte&quot;:108,&quot;html_token&quot;:false,&quot;start_byte&quot;:104,&quot;token&quot;:&quot;Dead&quot;},
…], 
document_url&quot;:&quot;https://en.wikipedia.org//w/index.php?title=The_Walking_Dead_(season_8)&amp;amp;amp;oldid=828222625&quot;,&quot;example_id&quot;:4549465242785278785,&quot;long_answer_candidates&quot;:[{&quot;end_byte&quot;:57620,&quot;end_token&quot;:216,&quot;start_byte&quot;:53609,&quot;start_token&quot;:24,&quot;top_level&quot;:true},{&quot;end_byte&quot;:53883,&quot;end_token&quot;:36,&quot;start_byte&quot;:53666,&quot;start_token&quot;:25,&quot;top_level&quot;:false},{&quot;end_byte&quot;:54388,&quot;end_token&quot;:42,&quot;start_byte&quot;:53884,&quot;start_token&quot;:36,&quot;top_level&quot;:false},…],
&quot;question_text&quot;:&quot;when is the last episode of season 8 of the walking dead&quot;,&quot;question_tokens&quot;:[&quot;when&quot;,&quot;is&quot;,&quot;the&quot;,&quot;last&quot;,&quot;episode&quot;,&quot;of&quot;,&quot;season&quot;,&quot;8&quot;,&quot;of&quot;,&quot;the&quot;,&quot;walking&quot;,&quot;dead&quot;]}&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Papers for more info on the datasets:\
&lt;a href=&quot;https://arxiv.org/pdf/1806.03822.pdf&quot;&gt;SQuAD 2.0&lt;/a&gt;\
&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf&quot;&gt;Google Natural Questions&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataset-statistics&quot;&gt;Dataset Statistics&lt;/h2&gt;

&lt;p&gt;To find the best dataset for your use case, here are some statistics about the different types of questions in the datasets. Note that this data is approximate, as general rules were used to separate questions into these categories and some may have fallen through the cracks mostly due to the fact that some question start with context and don’t lead or end with the question word such as “in september 1849 where did chopin take up residence?”. The “other” category in the table is made up of all questions that didn’t fall into our general descriptions of the question types, and the fact there are many more questions in the other category for Google Natural Questions vs SQuAD 2.0 is due to both the relative size of the dataset and the increased prevalence of phrase like “questions” such as “benefits of colonial life for single celled organisms”.&lt;/p&gt;

&lt;h3 id=&quot;question-type-distribution-of-train-sets&quot;&gt;Question Type Distribution of Train Sets&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Google Natural Questions&lt;/th&gt;
      &lt;th&gt;SQuAD 2.0&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Total Questions&lt;/td&gt;
      &lt;td&gt;307373&lt;/td&gt;
      &lt;td&gt;130319&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;What (%, total num)&lt;/td&gt;
      &lt;td&gt;17.1%, 52535&lt;/td&gt;
      &lt;td&gt;59.6%, 77701&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Where (%, total num)&lt;/td&gt;
      &lt;td&gt;10.3%, 31776&lt;/td&gt;
      &lt;td&gt;4.07%, 5303&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;When (%, total num)&lt;/td&gt;
      &lt;td&gt;13.6%, 41725&lt;/td&gt;
      &lt;td&gt;6.38%, 8308&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Who (%, total num)&lt;/td&gt;
      &lt;td&gt;25.1%, 77281&lt;/td&gt;
      &lt;td&gt;10.4%, 13533&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Why (%, total num)&lt;/td&gt;
      &lt;td&gt;1.31%, 4041&lt;/td&gt;
      &lt;td&gt;1.44%, 1881&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Which (%, total num)&lt;/td&gt;
      &lt;td&gt;2.83%, 8721&lt;/td&gt;
      &lt;td&gt;6.21%, 8088&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Whom (%, total num)&lt;/td&gt;
      &lt;td&gt;0%, 6&lt;/td&gt;
      &lt;td&gt;0.343%, 447&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;How (%, total num)&lt;/td&gt;
      &lt;td&gt;5.87%, 18041&lt;/td&gt;
      &lt;td&gt;9.95%, 12969&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Other (%, total num)&lt;/td&gt;
      &lt;td&gt;22.8%, 70157&lt;/td&gt;
      &lt;td&gt;1.50%, 1954&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Side Note:
The fact that question distribution is much more balanced for Google Natural Questions vs SQuAD 2.0, is an interesting comment on the types of questions people naturally vs artificially come up with.&lt;/p&gt;

&lt;h2 id=&quot;cross-training-experiment&quot;&gt;Cross Training Experiment&lt;/h2&gt;

&lt;p&gt;An interesting research question that we wanted to explore was how a model trained on one dataset would perform when tested on the other dataset. This would possibly allow us to see if one dataset allowed for better generalization to out of domain data, which could help inform a decision of the best dataset for one’s needs.&lt;/p&gt;

&lt;p&gt;Since the contexts in the Google Natural Questions (GNQ) dataset consist of whole wikipedia articles, which are much too long for the bert models we wanted to use, we decided to make use of the long answer as context for our training, and use the short answer as the target answer. We used data points which had long answers but not short answers to act as “unanswerable” questions. As there are 152148 data points with long answers in GNQ, our reformatted dataset had 152148 data points, as none of the data points without long answers were used in our reformatted dataset. 29.7% of the GNQ questions that have a long answer don’t have a short answer, which makes it comparable to the 33.4% of SQuAD 2.0 questions in the train data that are unanswerable.&lt;/p&gt;

&lt;p&gt;For maximum convenience and fair comparibility, we decided to use Huggingface’s run_squad.py script to train our model on our modified GNQ Dataset, which meant converting the GNQ dataset into the SQuAD 2.0 format, which involved stripping the text of html, finding the correct answer span in the the long answer context, among other things. If you want to do this yourself, or are just curious about the specifics check out part 2 of this guide to model training I helped write:&lt;/p&gt;

&lt;p&gt;One limitation we came across that caused us to change our training methods slightly was the amount of time it takes to train a model. Since there are a number of SQuAD 2.0 pretrained models available for &lt;a href=&quot;https://huggingface.co/ktrapeznikov/albert-xlarge-v2-squad-v2&quot;&gt;public use&lt;/a&gt;, we were able to use a model trained on the full SQuAD 2.0 dataset in this comparison. Since the GNQ dataset is very large, we initially choose to just run the training on 1/10th of the reformatted dataset, and then later on decided to train the model that was trained on the first 1/10th on the next 4/10th so that the model ended up being trained on 1/2th of the reformatted GNQ dataset.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Dataset model was trained on as the rows, and evaluation dataset as the columns.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;GNQ&lt;/th&gt;
      &lt;th&gt;SQuAD&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;GNQ 1/10th Data&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 53.137003841229195, 'f1': 57.66917657734894, 'total': 781, 'HasAns_exact': 40.0, 'HasAns_f1': 46.67854133379154, 'HasAns_total': 530, 'NoAns_exact': 80.87649402390439, 'NoAns_f1': 80.87649402390439, 'NoAns_total': 251, 'best_exact': 53.137003841229195, 'best_exact_thresh': 0.0, 'best_f1': 57.66917657734895, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 50.61062915859513, 'f1': 51.25986732355112, 'total': 11873, 'HasAns_exact': 12.179487179487179, 'HasAns_f1': 13.479825359737246, 'HasAns_total': 5928, 'NoAns_exact': 88.9318755256518, 'NoAns_f1': 88.9318755256518, 'NoAns_total': 5945, 'best_exact': 50.720121283584604, 'best_exact_thresh': 0.0, 'best_f1': 51.30938713471512, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GNQ ½ Data&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 57.01398285972034, 'f1': 63.63002288066548, 'total': 2217, 'HasAns_exact': 50.23380093520374, 'HasAns_f1': 60.03190429287585, 'HasAns_total': 1497, 'NoAns_exact': 71.11111111111111, 'NoAns_f1': 71.11111111111111, 'NoAns_total': 720, 'best_exact': 57.01398285972034, 'best_exact_thresh': 0.0, 'best_f1': 63.63002288066556, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 47.23321822622758, 'f1': 50.06099675107974, 'total': 11873, 'HasAns_exact': 35.27327935222672, 'HasAns_f1': 40.93694575330119, 'HasAns_total': 5928, 'NoAns_exact': 59.158957106812444, 'NoAns_f1': 59.158957106812444, 'NoAns_total': 5945, 'best_exact': 50.08843594710688, 'best_exact_thresh': 0.0, 'best_f1': 50.75645811811136, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SQuAD 2.0&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{'exact': 30.985915492957748, 'f1': 35.324918748038996, 'total': 781, 'HasAns_exact': 15.283018867924529, 'HasAns_f1': 21.67690857022349, 'HasAns_total': 530, 'NoAns_exact': 64.14342629482071, 'NoAns_f1': 64.14342629482071, 'NoAns_total': 251, 'best_exact': 32.394366197183096, 'best_exact_thresh': 0.0, 'best_f1': 35.43007732875682, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{‘exact': 75.90331003116314, 'f1': 79.23560349162027, 'total': 11873, 'HasAns_exact': 64.97975708502024, 'HasAns_f1': 71.65390017813893, 'HasAns_total': 5928, 'NoAns_exact': 86.79562657695543, 'NoAns_f1': 86.79562657695543, 'NoAns_total': 5945, 'best_exact': 75.90331003116314, 'best_exact_thresh': 0.0, 'best_f1': 79.23560349162024, 'best_f1_thresh': 0.0}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;A number of interesting observations and plausible conclusions can be made by looking at the data, and the eval data from the model trained on 1/10th and ½ of the reformatted GNQ dataset gives insight into how the performance of the model varies as it is fed more data.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;One initial comparison to make is to look at the model’s performance on the dev set of the same dataset of the train set it was trained on. The SQuAD 2.0 trained model has a better f1 of 79.2 than either the GNQ trained model trained on 1/10th or ½ of the data with f1s of 57.7 and 63.6 respectively.&lt;/li&gt;
  &lt;li&gt;Note that GNQ is a much harder task, as state of the art f1 for short answer identification is .64 vs the around .93 for SQuAD 2.0. Much of the difficulty may have been offset by the fact we are using the long answer for context instead of the whole wikipedia article, but the fact that GNQ’s questions are naturally generated, and don’t tend to borrow pieces of the paragraph, is one way in which GNQ is still a harder task than SQuAD 2.0.&lt;/li&gt;
  &lt;li&gt;The fact that the GNQ trained model was only trained on half of the dataset whereas the SQuAD 2.0 trained model was trained on the whole thing also probably had an effect on the model performance.&lt;/li&gt;
  &lt;li&gt;Though the SQuAD 2.0 model did better on its own dev set than the GNQ model did, the GNQ model did better on the SQuAD 2.0 dev set than the SQuAD 2.0 model did on the GNQ dev set when the overall and the has_ans exact and f1 scores are compared. This implies that the GNQ dataset does a better job of instilling a general purpose language understanding in the model that can generalize to other domains.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing to keep track of which can yield interesting insights into a model’s behavior is a model’s tendency to answer questions or refrain from answering them.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Looking at how the no_ans scores decrease and the has_ans score increases for the GNQ model on GNQ eval on differing amounts of data likely demonstrates how as the model is fed more data it tends to guess that the question has an answer more often.&lt;/li&gt;
  &lt;li&gt;This can be seen even more clearly by looking at the GNQ model on the SQuAD 2.0 eval, where for the 1/10th data model the has_ans score are very low and the no_ans score is very high, likely demonstrating that the model does not have much understanding and just tends to say that the question is unanswerable. For the 1/2th data model, the overall exact and f1 scores are lower, but it is clear the model has better understanding and not just relying on guessing that the question is unanswerable all the time as the has_ans scores are much higher, and the no_ans scores are lower.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A couple of differences between the datasets that may have affected the experimental results are as follows.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There is no explicit plausible yet wrong answer in the long answer of questions that don’t have a short answer in the GNQ dataset, unlike the plausible answers in the SQuAD 2.0 dataset. Though there is no plausible answer per se, the long answer paragraph is necessarily related to the question, so there might be spans in the GNQ paragraphs that don’t have a short answer that are similar to the plausible answers found in the SQuAD 2.0 dataset.&lt;/li&gt;
  &lt;li&gt;Also, in the reformatting of the dataset we counted questions that had Yes or No answer as unanswerable as they don’t have a short answer span.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Hopefully this article gave you further insight into whether SQuAD 2.0 or Google Natural Questions is right to use to train your model, or just gave insight into the nature of the datasets. NLP Question Answering is a very exciting area of active research, and there are a number of different interesting datasets for it out there beyond the two mentioned in the article such as &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/YangYihMeek_EMNLP-15_WikiQA.pdf&quot;&gt;WikiQA&lt;/a&gt;, &lt;a href=&quot;https://hotpotqa.github.io/&quot;&gt;HotpotQA&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/1712.07040.pdf&quot;&gt;NarrativeQA&lt;/a&gt;. In the future, I hope to explore these other datasets and run more experiments to understand their strengths, weaknesses, and relations.&lt;/p&gt;

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD Website&lt;/a&gt;\
&lt;a href=&quot;https://ai.google.com/research/NaturalQuestions&quot;&gt;SQuAD Paper&lt;/a&gt;\
&lt;a href=&quot;https://arxiv.org/pdf/1806.03822.pdf&quot;&gt;Google Natural Questions Website&lt;/a&gt;\
&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf&quot;&gt;Google Natural Questions Paper&lt;/a&gt;&lt;/p&gt;</content><author><name>Nick Greenspan</name></author><summary type="html">by Nick Greenspan</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/SIFthumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/SIFthumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Comprehensive Guide to Training a Machine Learning Model for Question Answering</title><link href="http://localhost:4000/category-1/category-2/2020/08/06/A-Comprehensive-Guide-to-Training-a-Machine-Learning-Model-for-Question-Answering_.html" rel="alternate" type="text/html" title="A Comprehensive Guide to Training a Machine Learning Model for Question Answering" /><published>2020-08-06T06:30:00-04:00</published><updated>2020-08-06T06:30:00-04:00</updated><id>http://localhost:4000/category-1/category-2/2020/08/06/A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_</id><content type="html" xml:base="http://localhost:4000/category-1/category-2/2020/08/06/A-Comprehensive-Guide-to-Training-a-Machine-Learning-Model-for-Question-Answering_.html">&lt;h4 id=&quot;by-batya-stein--nicholas-greenspan&quot;&gt;by Batya Stein &amp;amp; Nicholas Greenspan&lt;/h4&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;b&gt;Batya Stein&lt;/b&gt; is a rising junior at Princeton University studying Computer Science and English Literature. Her research interests include Natural Language Processing and Software Engineering and Design. Previously, she has interned at a non-profit, designing a project to interface between its CRM softwares. When not coding, she enjoys drawing, swimming, and reading fiction. Batya was one of NLMatics’ 2020 summer interns. 
&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
&lt;b&gt;Nicholas Greenspan&lt;/b&gt; lives in New York City and is a freshman at Rice University who is majoring in computer science. Nicholas is interested in Machine Learning, Natural Language Processing, and their applications to various fields. Nicholas recently worked at the UTHealth School of Biomedical Informatics working on a Natural Language Processing project to help doctors find relevant treatments for their patients, and is excited to work on more interesting and meaningful problems at NLMatics. Outside of CS, Nicholas likes to read, play ice hockey, and listen to many genres of music including indie rock and electronic. Nicholas was one of NLMatics’ 2020 summer interns.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt; A Comprehensive Guide to Training a Machine Learning Model for Question Answering: &lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Fine-tuning ALBERT on Google Natural Questions &lt;/h2&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#part-1-setting-up-the-ec2-instance&quot;&gt;Setting up the EC2 Instance&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#choosing-and-launching-an-ec2-instance&quot;&gt;Choosing and launching EC2 Instance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#preparing-the-instance-for-training&quot;&gt;Preparing instance for training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#setting-up-wandb-logging&quot;&gt;Setting up Wandb logging&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#part-2-downloading-and-formatting-data&quot;&gt;Downloading and Formatting Data&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#about-the-datasets&quot;&gt;About the datasets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#downloading-the-data&quot;&gt;Downloading the data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#processing-data-from-a-zip-file&quot;&gt;Processing from zip file&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#reformatting-google-nq-data-to-squad-format&quot;&gt;Reformatting Google Natural Questions to SQuAD format&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#part-3-training-the-model&quot;&gt;Training the Model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#model-comparisons&quot;&gt;Model comparisons&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#understanding-the-parameters-of-run_squadpy&quot;&gt;Understanding run_squad.py parameters&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#common-errors-we-encountered-and-how-to-fix-them&quot;&gt;Common errors encountered during training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#restoring-training-from-a-checkpoint&quot;&gt;Restoring from a checkpoint&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#understanding-your-model-outputs&quot;&gt;Understanding model outputs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/nlmatics/nlmatics.github.io/blob/gh-pages/docs/_posts/2020-08-06-A%20Comprehensive%20Guide%20to%20Training%20a%20Machine%20Learning%20Model%20for%20Question%20Answering_.md#downloading-your-model-uploading-to-s3-bucket&quot;&gt;Downloading your model and uploading to an S3 bucket&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro:&lt;/h2&gt;

&lt;p&gt;Feel like a machine learning question answering system could give your business a boost? Just interested in trying out revolutionary AI tech? You’ve come to the right place. Training your first deep learning model can feel like navigating a labyrinth with no clear start or finish, so we’ve made a highly detailed guide to save you time, energy, and a lot of stackoverflow searches. This is a comprehensive guide to training a ML question answering model that will walk you through every step of the process from cloud computing to model training. We will use AWS for our cloud computing necessities, and Huggingface’s transformers repository to access our ALBERT model and run_squad.py script for training.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why do I care? What AI can do for you:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Extracting information from long text documents can be a time consuming, monotonous endeavour. A revolutionary cutting edge technology, NLP Question Answering systems can automate this process, freeing you up to use that information to make decisions to further your business. Given a piece of text and a question, a Question Answering system can tell you where the answer lies in the text, and even tell you if the answer isn’t present at all! If this technology interests you, and you want to apply it at a business-level scale, you should definitely check out NLMatics’ product at https://www.nlmatics.com/.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://static.wixstatic.com/media/97e2b2_df3bf50a5add420faa4f298c5c232584~mv2.png/v1/fill/w_336,h_64,al_c,q_85,usm_0.66_1.00_0.01/logo.webp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We ran the model training described here during our summer engineering internship at NLMatics. Currently, one of the most popular Question Answering transformer models is trained on the &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD&lt;/a&gt; dataset, and we were curious to see if training on &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions/&quot;&gt;Google Natural Questions&lt;/a&gt; could give us more effective results. (As of this post, there are no ALBERT models pretrained on Google Natural Questions available for use online.) Along the way, we learned a lot about the ins and outs of model training, memory management, and data-logging.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Huggingface’s &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;transformers github repository&lt;/a&gt;, an open source resource for training language models, greatly simplifies the training process and puts many essential materials all in one place. Huggingface provides scripts for training models on different datasets, which saves you the labor of writing your own. In order to train on a new dataset while still making use of Huggingface’s &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py&quot;&gt;run_squad.py script&lt;/a&gt; and functions for evaluating SQuAD training metrics, we converted the Google Natural Questions dataset into the same format as SQuAD. For this reason, this guide can help you whether you want to use Google Natural Questions, SQuAD, or any other question answering dataset to train a transformer model.&lt;/p&gt;

&lt;p&gt;To learn more about machine learning or language models, see the links below.\
&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/ml-intro&quot;&gt;Machine Learning&lt;/a&gt;\
&lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;Transformer architecture (BERT)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To learn more about the datasets we worked with, see the links above or our blog post comparing the characteristics of both sets.&lt;/p&gt;

&lt;h2 id=&quot;part-1-setting-up-the-ec2-instance&quot;&gt;PART 1: SETTING UP THE EC2 INSTANCE&lt;/h2&gt;

&lt;h2 id=&quot;choosing-and-launching-an-ec2-instance&quot;&gt;Choosing And Launching An EC2 Instance:&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;To start our model training, we have to choose the right platform (one with a bit more computing power than a personal computer)&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://www.kodyaz.com/images/aws/aws-ec2-dashboard-to-list-all-ec2-instances.png&quot; alt=&quot;ec2 instance list&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;List of instances in EC2 dashboard&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;What is AWS EC2?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Amazon Web Services’ EC2 service allows users to utilize cloud computing power from their local machines. An On-Demand EC2 Instance provides a virtual server that charges per hour that instance is running.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instance types and costs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To choose an instance for your use case, consider its GPU and vCPU capacities, storage space, and &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand/&quot;&gt;cost&lt;/a&gt;. For our training, we considered &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types#Accelerated_Computing&quot;&gt;P and G- type instances&lt;/a&gt;, since both classes come with GPU power. P2/P3 instances have up to 16 GPUs (P3 is the latest generation and comes at a slightly higher cost). G4 instances have up to 4 GPUs, but are more cost effective for larger amounts of memory. Note that if you’re using CUDA, a platform for computing with GPU power often used in machine learning applications, the maximum number of GPUs it will run on is 8, &lt;a href=&quot;https://forums.developer.nvidia.com/t/cuda-peer-resources-error-when-running-on-more-than-8-k80s-aws-p2-16xlarge/45351&quot;&gt;due to limitations in the “peer-to-peer” system&lt;/a&gt;. We chose the p2.8xl instance, which has 8 NVIDIA K80 GPUs and 96GB of storage.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Instance&lt;/th&gt;
      &lt;th&gt;GPUs&lt;/th&gt;
      &lt;th&gt;GPU Type and total GPU memory&lt;/th&gt;
      &lt;th&gt;vCPUs&lt;/th&gt;
      &lt;th&gt;RAM&lt;/th&gt;
      &lt;th&gt;Linux Pricing (USD/HR)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p2.xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA K80 GPU (12GiB)&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;61 GiB&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p2.8xlarge&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;NVIDIA K80 GPU(96GiB)&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;488 GiB&lt;/td&gt;
      &lt;td&gt;7.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p2.16xlarge&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;NVIDIA K80 GPU (192GiB)&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;732 GiB&lt;/td&gt;
      &lt;td&gt;14.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p3.2xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA Tesla V100 GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
      &lt;td&gt;3.06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p3.8xlarge&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;NVIDIA Tesla V100 GPU (64GiB)&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;244&lt;/td&gt;
      &lt;td&gt;12.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;p3.16xlarge&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;NVIDIA Tesla V100 GPU (128GiB)&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;488&lt;/td&gt;
      &lt;td&gt;24.48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0.526&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.8xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;2.176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.12xlarge&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (64GiB)&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;192&lt;/td&gt;
      &lt;td&gt;3.912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g4dn.16xlarge&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NVIDIA T4 Tensor Core GPU (16GiB)&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
      &lt;td&gt;4.352&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Account limits&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before launching an instance, be aware that each type has a specific vCPU capacity that it needs to run, which can be found in the chart above. However, all AWS accounts have automatic limitations on the number of vCPUs that can be allocated per instance class. Current account limits can be viewed in the EC2 section of the “Service Quotas” tab in the AWS console. Make sure you are in the same region you plan on starting your instance in, then check the “applied quota value” for “Running On-Demand P instances” (or instance class of your choice). If the current vCPU limit isn’t large enough to launch your instance, use the “Request Limit Increase” button to request the necessary number of vCPUs. In our experience, requests can take up to 48 hours to be filled, so request increases in advance of when you plan to start your training.&lt;/p&gt;

&lt;p&gt;Once the limit is increased, you can launch your instance from the AWS EC2 console. Choose a platform with deep learning capabilities - we needed CUDA and a PyTorch environment, so we chose the Deep Learning AMI (Amazon Linux) Version 30.0. Next, choose instance type, and add additional storage if desired. Then configure the security group to allow from incoming connections from your IP address to the instance, by adding a rule of Type “SSH” with Source “My IP Address”, which automatically fills in your IP for you. You can also add additional rules to the security group later if needed to give others access to the instance. Do so by choosing your
instance from the console instance list. Under its description, click its security group, then from the pulldown menu in the security groups list choose actions -&amp;gt; edit inbound rules -&amp;gt; add SSH rule, with IP of the person who needs access as the source.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key-pair security file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To secure connections to your instance, create a key-pair file. In order to work, the key must not have public security permissions, so once you’ve downloaded the file, go to your terminal, navigate to the directory that the file is stored in, and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chmod 400 keypair-file-name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can then connect to the instance from your terminal/ command-line by selecting the instance in the console list and running the example command given by the connect button, which will look like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -i”xxx.pem” EC2-user@EC2-xx-xx-xxx-xxx.us-region.compute.amazonaws.com&lt;/code&gt;.
 (When logging onto the server for the first time, you may get a warning that “The authenticity of host… can’t be established” but this should not be a security issue since you are instantiating the connection directly from AWS.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IAM roles (sharing AWS resource access)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to give someone else access to the instance without giving them your account login information, you can create an IAM role in the IAM console. In the Users menu, add a User with AWS Management Console access, give user all EC2 permissions and then share the role’s login information.
Once you start using your instance, you can monitor the costs or credits used, with the AWS cost explorer service. Make sure to stop the instance when not in use to avoid extra charges!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/TLZQDUX.jpg&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;preparing-the-instance-for-training&quot;&gt;Preparing The Instance For Training&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Before we can get started with training, we’ll need to load all the necessary libraries and data onto our instance.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upload training data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prepare for training by connecting to your EC2 instance from the terminal of your choice and creating a directory inside it to store the data files in. (See section II, on downloading and reformatting data, to learn how to get your data into the right format before you start working with it.) Upload your training and dev sets into the directory (in json format). Upload the training script, which can be &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py&quot;&gt;found in the Transformers github repo&lt;/a&gt;, onto the instance. Lastly, create a new directory that will be used to store the evaluations and model checkpoints from the script’s output.&lt;/p&gt;

&lt;p&gt;Note that if you’ve previously trained a model of the same type on your EC2 instance, there will be a cached datafile with a name like “cached_dev_albert-xlarge-v2_384”. If you’re training on a different dataset than you used in the previous training, make sure to delete the cached file, otherwise the script will automatically load the data from it and you’ll end up training on different data then you meant to.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File transferring&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To upload and transfer files between an EC2 instance and local computer you can use the following terminal commands. (Run these commands while the instance is running, but in a separate terminal window from where the instance is logged in.)&lt;/p&gt;

&lt;p&gt;From &lt;strong&gt;local -&amp;gt; EC2&lt;/strong&gt; – run the command
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp -i /directory/to/abc.pem /your/local/file/to/copy user@EC2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;From &lt;strong&gt;EC2 -&amp;gt; local&lt;/strong&gt; (for downloading your model checkpoints, etc.) run
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp -i /directory/to/abc.pem user@EC2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file /your/local/directory/files/to/download&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Another option is to use &lt;a href=&quot;https://filezilla-project.org/&quot;&gt;FileZilla&lt;/a&gt; software for transferring files. This &lt;a href=&quot;https://stackoverflow.com/questions/16744863/connect-to-amazon-ec2-file-directory-using-filezilla-and-sftp&quot;&gt;post&lt;/a&gt; explains how to set up a connection to the EC2 instance through Filezilla with your pem key.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Library installations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prepare your environment by downloading the libraries needed for the run_squad script. Run the following commands in the terminal of your choice, while logged into your instance:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source activate env-name&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;This command activates a &lt;strong&gt;virtual environment&lt;/strong&gt;. Choose one of the environments that come pre-loaded onto the instance, listed when you first log into the instance from your terminal. 
Since run_squad.py relies on PyTorch, we used the environment “pytorch_latest_p36”. &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; is a python machine learning library that can carry out accelerated tensor computations using GPU power.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install transformers&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Transformers&lt;/strong&gt; is the Huggingface library that provides transformer model architectures that can be used with PyTorch or tensorflow. The transformers library also has processors for training on SQuAD data, which are used in the run_squad.py script.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install wandb&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Wandb&lt;/strong&gt; is web app that allows for easy visualization of training progress and evaluation graphs, lets you see logging output from terminal, keeps track of different runs within you project, and allows you to share results with others (We’ll talk more about how to use Wandb to view your results in the next section!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install tensorboardX&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The run_squad.py script uses the &lt;strong&gt;tensorboard&lt;/strong&gt; library to graph evaluations that are run as training progresses. We’ll sync wandb with tensorboard so the graphs can be seen directly from the wandb dashboard.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git clone https://github.com/NVIDIA/apex&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd apex&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -v --no-cache-dir ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Apex&lt;/strong&gt; is a pytorch extension that allows scripts to run with mixed-floating point precision (using 16 bit floats instead of 32 bit in order to decrease RAM usage)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setting-up-wandb-logging&quot;&gt;Setting Up Wandb Logging&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;You’re going to want to keep track of your training as it happens - here’s where we’ll set that up.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Wandb (Weights &amp;amp; Biases) provides a web-based platform for visualizing training loss and evaluation metrics as your training runs. Checking up on metrics periodically throughout training lets you visualize your progress, and course-correct if your loss suddenly swings upwards, or your metrics take a dramatic turn down.&lt;/p&gt;

&lt;p&gt;First you’ll want to create a free account on &lt;a href=&quot;https://www.wandb.com/&quot;&gt;wandb’s website&lt;/a&gt;. 
Then, to connect your wandb account to your EC2 instance, go to the wandb website settings section and copy your api key. Log into your EC2 instance from your terminal and run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb login your-api-key&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next, we’ll edit the run_squad.py script to integrate wandb logging. (You can edit it on your computer before uploading to the instance in the terminal editor of your choice after uploading it.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import wandb&lt;/code&gt; to the library imports.&lt;/li&gt;
  &lt;li&gt;Before line 742 (sending model to cuda), add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb.init(project=&quot;full_data_gnq_run&quot;, sync_tensorboard=True))&lt;/code&gt;. This will create a wandb project with the given name. Each time the script is rerun with the same project name will show up as a new run in the same project. Setting sync with tensorboard to true automatically logs the tensorboard graphs created during training to your project page.&lt;/li&gt;
  &lt;li&gt;In the next line, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb.config.update(args)&lt;/code&gt; to store all the arguments and model parameters inputted to the script&lt;/li&gt;
  &lt;li&gt;On line 759, after sending model to device, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wandb.watch(model, log='all')&lt;/code&gt;, which saves the weights of the model to wandb as it trains.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that you’ve added wandb statements into your script, you can watch your training progress from the wandb project dashboard. Two particularly helpful sections are the charts, where you can see loss and evaluation metrics, and the logs, where you can see the output of your script as it runs to track what percent complete your training is.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/checkpoints.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Wandb charts from our Google Natural Questions training&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;part-2-downloading-and-formatting-data&quot;&gt;PART 2: DOWNLOADING AND FORMATTING DATA&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;We’re going to download the data for the model to train on, and tweak it a bit to get it into the right format.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(Note that a lot of the information in this section is about Google Natural Questions, but the same tips for handling and reformatting large datasets could apply to any dataset of your choice.)&lt;/p&gt;

&lt;h2 id=&quot;about-the-datasets&quot;&gt;About the Datasets&lt;/h2&gt;

&lt;p&gt;Both &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD&lt;/a&gt; and &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions/&quot;&gt;Google Natural Questions&lt;/a&gt; are datasets containing pairs of questions and answers based on Wikipedia articles. Fundamentally, they serve the same purpose - to teach a model reading comprehension by having it locate answers within a large amount of text.&lt;/p&gt;

&lt;p&gt;However, there are also some interesting differences between the two datasets that made us want to experiment with training a model on Google Natural Questions instead of SQuAD. To start, Google Natural questions has about twice the amount of training data as SQuAD does. SQuAD gives paragraphs from Wikipedia articles as context for its answers and has multiple questions per article, while Google Natural Questions gives entire Wikipedia articles for context, and only has one question per article. Additionally, all of SQuAD’s answers are short (about a sentence long), but Google Natural Questions has some questions with short answers, some with long answers, some with yes-no answers, and some with combinations of all of the above.&lt;/p&gt;

&lt;p&gt;Example of a SQuAD 2.0 datapoint:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/squad_datapoint.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Answer_start indicates the character count where the answer is located in the context.
Some questions in SQuAD 2.0 are flagged is_impossible, if answers to them cannot be found in the given context.&lt;/p&gt;

&lt;p&gt;To learn more about the differences between the datasets, their compositions and the decisions we made when reformatting, please see Nick’s SQuAD vs. Google Natural Questions blog post.&lt;/p&gt;

&lt;h2 id=&quot;downloading-the-data&quot;&gt;Downloading the Data&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Or, how to actually access 40GB worth of question-answer pairs for training&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our goal was to train on the Google Natural Questions dataset using Huggingface’s run_squad.py script. To do so, we first had to convert the Google Natural Questions examples into SQuAD format. (This might seem like a convoluted approach but it’s actually one of the easiest ways to train with a new question answer dataset, since the Huggingface transformers library has functions specifically designed to process and evaluate SQuAD-formatted examples.)&lt;/p&gt;

&lt;p&gt;To get the data for reformatting, you can either download it directly to your computer or you can upload it to an Amazon S3 bucket and stream it into your reformatting script from there. S3 buckets are an AWS service that allow you to store data in the cloud and share it with others.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Downloading Google Natural Questions to your local computer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you have 40 gb of room on your local computer, you can download all the GNQ files using the google command line interface gsutil &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions/download&quot;&gt;as described on the GNQ homepage&lt;/a&gt;. (We didn’t want to use the simplified train data that is directly linked to on the homepage because it didn’t give all the information we needed for our reformatting script.&lt;/p&gt;

&lt;p&gt;Alternatively, you can download the files directly within your reformatting script, as demonstrated in &lt;a href=&quot;https://github.com/nyrnick/NLM-LM-Devel/blob/master/convert_and_download_las_as_context.py&quot;&gt;our code&lt;/a&gt;, by using the python request library to download the files from the urls of &lt;a href=&quot;https://console.cloud.google.com/storage/browser/natural_questions/v1.0/dev?authuser=0&quot;&gt;the google bucket where they are publicly stored&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streaming Google Natural Questions from an S3 Bucket&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you don’t have enough storage to download the files to your local computer, you can use gsutil to upload the data to an Amazon S3 bucket and then stream the data into your python reformatting code from the bucket without needing to download it first. To do so, follow these steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Make an AWS S3 bucket to hold the data&lt;/li&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://cloud.google.com/storage/docs/gsutil_install&quot;&gt;gustil&lt;/a&gt; and &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html&quot;&gt;AWS CLI&lt;/a&gt;, which are command line interaces for Google and AWS, respectively&lt;/li&gt;
  &lt;li&gt;Configure your AWS cli following &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html&quot;&gt;these instructions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Check that everything works by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 ls&lt;/code&gt; in your terminal, which displays the list of buckets associated with your AWS account&lt;/li&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil -m cp -R gs://natural_questions/v1.0 s3://your_bucket_name&lt;/code&gt; to download all data, or to download individual files replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://natural_questions/v1.0&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://natural_questions/v1.0/dev/nq-dev-00.jsonl.gz&lt;/code&gt; (there are 5 dev files altogether, so repeat this command through “dev-04”) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://natural_questions/v1.0/train/nq-train-01.jsonl.gz&lt;/code&gt; (there are 50 train files altogether, so repeat the command through “train-49”)&lt;/li&gt;
  &lt;li&gt;To stream the data from the s3 bucket in a python script, follow &lt;a href=&quot;https://www.slsmk.com/use-boto3-to-open-an-aws-s3-file-directly/&quot;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Downloading SQuAD Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to use the SQuAD dataset instead of Google Natural Questions to follow our tutorial, the data can be downloaded from https://rajpurkar.github.io/SQuAD-explorer/ using the links “Training Set v2.0 (40MB)” and “Dev Set v2.0 (4MB)” for the training and dev sets respectively on the left hand side bar.&lt;/p&gt;

&lt;h2 id=&quot;processing-data-from-a-zip-file&quot;&gt;Processing Data from a Zip File&lt;/h2&gt;

&lt;p&gt;The Google Natural Questions training files take up 40 GB even when contained in gzip (similar to zip) format. When unzipped into json format, any one of the files is large enough to likely crash whatever program you try to open them with. Therefore, when processing Natural Questions or any similarly large dataset in python, it is easiest to do so straight from a zip file.&lt;/p&gt;

&lt;p&gt;To process a &lt;strong&gt;gzip&lt;/strong&gt; in python, import gzip library and use a command such as&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gzip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'v1.0_train_nq-train-00.jsonl.gz'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
	    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;To process a &lt;strong&gt;zip&lt;/strong&gt; file, import zipfile library and use a command like&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zipfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ZipFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unzippedfilename&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   	    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   	        &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;reformatting-google-nq-data-to-squad-format&quot;&gt;Reformatting Google NQ Data to SQuAD Format&lt;/h2&gt;

&lt;p&gt;We wrote a python script that can be found &lt;a href=&quot;https://github.com/nyrnick/NLM-LM-Devel/blob/master/convert_and_download_las_as_context.py&quot;&gt;here&lt;/a&gt; for reformatting the data on our local computers before uploading the data to our EC2 instance. Since the way boths datasets are formatted is so different, we had to make certain decisions about how to reformat Google Natural Questions, like using the long answers as context and the short answers as answers. (Entire Wikipedia articles took way too long to process.)&lt;/p&gt;

&lt;p&gt;To hear more about the decisions we made when reformatting, see our code, linked above, and the SQuAD vs. Google Natural Questions blogpost.&lt;/p&gt;

&lt;h2 id=&quot;part-3-training-the-model&quot;&gt;PART 3: TRAINING THE MODEL&lt;/h2&gt;

&lt;h2 id=&quot;model-comparisons&quot;&gt;Model Comparisons&lt;/h2&gt;

&lt;p&gt;There are a number of different deep learning language models out there, many of which are based on BERT. For our purposes we choose ALBERT, which is a smaller and more computationally manageable version of BERT. For more detailed information on the different language models see here: (https://github.com/huggingface/transformers#model-architectures).&lt;/p&gt;

&lt;h2 id=&quot;understanding-the-parameters-of-run_squadpy&quot;&gt;Understanding the Parameters of run_squad.py&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Almost time to launch the training!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Script parameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Below is a list of parameters that we used for running run_squad.py. Note that not all of the parameters are necessary (only model_type, model_name_or_path and output_dir are needed for the script to run).&lt;/p&gt;

&lt;p&gt;--_model_type_: This is the type of model you are using. The main types are (‘distilbert’, ‘albert’, ‘bart’, ‘longformer’, ‘xlm-roberta’, ‘roberta’, ‘bert’, ‘xlnet’, ‘flaubert’, ‘mobilebert’, ‘xlm’, ‘electra’, ‘reformer’). We used ‘albertxlv2’.&lt;/p&gt;

&lt;p&gt;--_model_name_or_path_: This is where you indicate the specific pretrained model you want to use from https://huggingface.co/models, we used albert-xlarge-v2. A list of Huggingface provided pretrained models is here https://huggingface.co/transformers/pretrained_models.html. If you want to load a model from a checkpoint, which we will explain how to do later, you would provide the path to the checkpoint here.&lt;/p&gt;

&lt;p&gt;--_output dir_: This is where you specify the path to the directory you want the model outputs to go. The directory should be empty.&lt;/p&gt;

&lt;p&gt;--_do_train_: This indicates that you want to train your model.&lt;/p&gt;

&lt;p&gt;--_do_eval_: This indicates that you want to evaluate the performance of your model.&lt;/p&gt;

&lt;p&gt;--_train_file_: This is the path to the file with your training data.&lt;/p&gt;

&lt;p&gt;--_predict_file_: This is the path to the file with your evaluation data.&lt;/p&gt;

&lt;p&gt;--_learning_rate_: Determines the step size when moving towards minimizing the loss. We used 3e-5.&lt;/p&gt;

&lt;p&gt;--_num_train_epochs_: Determines the number of times you want to go through the dataset. For our training the metrics started to plateau around the end of the third epoch. We would recommend 3 to 5 epochs, but you can stop training prematurely if you see your metrics plateauing before the training has finished.&lt;/p&gt;

&lt;p&gt;--_max_seq_length_: The maximum length of a sequence after tokenization. We used 384, which is the default.&lt;/p&gt;

&lt;p&gt;--_threads_: The number of vCPU threads you want to use for the process of converting examples to features. Check how many vCPU threads you have, it should be the number of vCPU cores times 2. We used 128 threads.&lt;/p&gt;

&lt;p&gt;--_per_gpu_train_batch_size_: The batch size for training you use per gpu, aka the number of examples the model will look at in one iteration. That is if you have 4 gpus and your --_per_gpu_train_batch_size_ is 4, your total batch size will be 16. A larger batch size will result in a more accurate gradient, but a smaller batch size will make the training go faster and use less memory. We used 1.&lt;/p&gt;

&lt;p&gt;--_per_gpu_eval_batch_size_: Same thing as &lt;em&gt;per_gpu_train_batch_size&lt;/em&gt; except for evaluation. We used 8.&lt;/p&gt;

&lt;p&gt;--_version_2&lt;em&gt;with_negative&lt;/em&gt;: If you are using the SQuAD V2 dataset with negative examples use this flag.&lt;/p&gt;

&lt;p&gt;--_evaluate_during_training_: Determines if you want to evaluate during training, but note that this will only work if you are only using 1 gpu due to averaging concerns.&lt;/p&gt;

&lt;p&gt;--_fp_16_: Determines if you want to use 16 point precision instead of the default 32 point precision. This will decrease the amount of ram you use and decrease the training time. Note if you want to use this flag you must have Nvidia apex installed from https://www.github.com/nvidia/apex (as described above).&lt;/p&gt;

&lt;p&gt;--_gradient_accumulation_steps_: Makes gradient descent more stable (in a similar training to the one described here, we used 4)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screen command&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We know you’re probably excited to start training, but here are a few helpful commands before jumping in.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before you start training your model there is one important terminal functionality you should use. Since you don’t want the training process to stop every time your terminal disconnects from the EC2 instance, which will happen after periods of inactivity, you need to disconnect the process of training the model from the EC2 terminal. To do this, go to the terminal logged into your EC2 instance and run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen&lt;/code&gt; which will create a blank terminal screen. Once you have done that you can run the command to start the training.&lt;/p&gt;

&lt;p&gt;(Note that in order for a virtual environment to work within the screen, you must be running the base environment outside of the screen. Before activating the screen, if you are in a virtual environment, run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt; to return to the base environment, and only activate your virtual environment once you have entered the screen).&lt;/p&gt;

&lt;p&gt;The command consists of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python run_squad.py&lt;/code&gt; followed by all of the flags you want to use for your training.&lt;/p&gt;

&lt;p&gt;Here is the command we ran:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python run_squad.py &lt;span class=&quot;nt&quot;&gt;--model_type&lt;/span&gt; ALBERTxlv2 &lt;span class=&quot;nt&quot;&gt;--model_name_or_path&lt;/span&gt; ALBERT-xlarge-v2 &lt;span class=&quot;nt&quot;&gt;--do_train&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--do_eval&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--evaluate_during_training&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--train_file&lt;/span&gt; ~/real_data/modified_GNQ_las_as_context_1_10th_train.json &lt;span class=&quot;nt&quot;&gt;--predict_file&lt;/span&gt; ~/real_data/modified_GNQ_las_as_context_1_10th_dev.json &lt;span class=&quot;nt&quot;&gt;--learning_rate&lt;/span&gt; 3e-5 &lt;span class=&quot;nt&quot;&gt;--num_train_epochs&lt;/span&gt; 7 &lt;span class=&quot;nt&quot;&gt;--max_seq_length&lt;/span&gt; 384 &lt;span class=&quot;nt&quot;&gt;--doc_stride&lt;/span&gt; 128 &lt;span class=&quot;nt&quot;&gt;--output_dir&lt;/span&gt; ~/reformatted_outputs &lt;span class=&quot;nt&quot;&gt;--per_gpu_eval_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8 &lt;span class=&quot;nt&quot;&gt;--per_gpu_train_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nt&quot;&gt;--fp16&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--threads&lt;/span&gt; 128 &lt;span class=&quot;nt&quot;&gt;--version_2_with_negative&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you start the training and see that it has started without any initial errors you can hit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;control a&lt;/code&gt; and then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;control d&lt;/code&gt; which will return you to your previous terminal screen with the training running in the background. You can now disconnect from the EC2 instance without affecting the training. Please note that you must not actually turn the instance off during training or the training will be halted. To reconnect to the screen where the training is running, run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen -r&lt;/code&gt;. It is a good idea to check in on the training once in a while to make sure it has not halted, which can be easily accomplished by looking at the wandb logging, and you can even set your wandb account to send you email alerts if the training fails.&lt;/p&gt;

&lt;h2 id=&quot;common-errors-we-encountered-and-how-to-fix-them&quot;&gt;Common Errors We Encountered and How to Fix Them&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;To save you much of the frustration we endured, here are some errors we encountered and information on how to deal with them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you want to be proactive about your training you could look through this section before you start the training and take measures to prevent any possible errors you may feel are likely to come up.&lt;/p&gt;

&lt;p&gt;The two main errors we encountered were running out of memory (RAM) and computer storage within our EC2 instance. (The commands presented here are meant to be run while logged into your instance, and assume that your instance runs on a linux operating system.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to manage storage:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To see your current storage usage, run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df -h&lt;/code&gt;. To find the largest files on your instance, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo du -x -h / | sort -h | tail -40&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/file-sizes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you find yourself running out of storage there are a number of ways to get around it.&lt;/p&gt;

&lt;p&gt;Firstly, you could simply delete unneeded files on your amazon aws instance. To do that, navigate to the directory the files or folder you want to delete are located. Then run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm 'filename'&lt;/code&gt; to delete a single file, or the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm -R ‘foldername’&lt;/code&gt; if you want to delete a folder and all files in it. You can also delete virtual environments you aren’t using that come preinstalled on the EC2 instance. (For running run_squad.py, you only need the pytorch_latest_p36.) Do so by running the command ‘conda remove –name ‘myenv’ –all’ where myenv is the name of the environment you’d like to delete. A list of all installed environments is shown when starting up the instance.&lt;/p&gt;

&lt;p&gt;Another method is to simply add more storage to your machine through AWS. You can do this by simply going to the “volumes” section under the elastic block store header in the EC2 section of AWS. Once you are there select the instance you want to add more storage to, click the actions drop down menu, and then select modify volume. From here you can add as much storage as you like, but keep in mind you will be charged for it. See pricing info here: https://aws.amazon.com/ebs/pricing/. Note that you can only increase the volume size and not decrease it, so once you add more storage there is no going back.&lt;/p&gt;

&lt;p&gt;There are other methods for dealing with lack of storage issues, such as saving checkpoints to an s3 bucket as training runs, but we did not end up needing to use them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to manage memory (RAM, different than storage space):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To figure out your free memory in your instance at a given moment run the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;free -h&lt;/code&gt;. A good way to keep track of your memory usage throughout the run is to look at the wandb logging in the system section. If you keep encountering out of memory issues, there are a few ways to get around it.&lt;/p&gt;

&lt;p&gt;A way of increasing your overall memory is to use something called swap space. If you are not familiar with swap space, it basically involves partitioning part of your storage to be used for memory. A detailed guide can be found here: https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04.&lt;/p&gt;

&lt;p&gt;One method to decrease memory usage is to decrease your batch size. Note that if you are having out of memory issues in the convert examples to features part of the training process this will not help.&lt;/p&gt;

&lt;p&gt;Another method to decrease memory usage is to run the run_sqad.py script with the –fp_16 flag on if you are not already.&lt;/p&gt;

&lt;p&gt;If you are getting out of memory issues in the convert examples to features part of the training process, it might be necessary to decrease the size of your dataset.&lt;/p&gt;

&lt;p&gt;One final word of advice to avoid random errors is to make sure that you have the most recent version of the transformers library installed, and you are using the most recent run_squad.py script. We encountered some errors related to mismatched versions.&lt;/p&gt;

&lt;p&gt;Note:
Besides looking at the Wandb logging, you can use the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvidia-smi&lt;/code&gt; to view available gpus and current gpu usage on your instance, and the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top&lt;/code&gt; to see your current cpu usage, as well as your memory usage.&lt;/p&gt;

&lt;h2 id=&quot;restoring-training-from-a-checkpoint&quot;&gt;Restoring Training From a Checkpoint&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Annoyed you have to start from the beginning after your model training failed? Well, you don’t have to! Here’s how you can resume training from a checkpoint.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;After every 500 examples that the model trains on, it will output a “model checkpoint” that is a representation of the model at that point in training. The model checkpoints can be found in the output folder, and each one is stored in a folder called “checkpoint-x” where x is the number of examples the model had looked at when that checkpoint was generated. Resuming training from a checkpoint can save you a lot of time and convenience.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/site_files/nick-batya-post-imgs/checkpoints.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you encounter an error and want to pick up the training where you left off, or just want to resume training from a checkpoint for any other reason, here’s how to do it.&lt;/p&gt;

&lt;p&gt;All that is necessary is to run the same command you ran to start the training except &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--model_name_or_path&lt;/code&gt; must be set to the path to the folder for the checkpoint you want to resume training from. For example if your output folder is called outputs-2 and you want to resume training from checkpoint-1500 you should have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--model_name_or_path /path_to_outputs_folder/outputs-2/checkpoint-1500&lt;/code&gt;. For consistency, if you’re restarting training from the middle, you may want to see what the learning rate was at the checkpoint you’re using (on wandb), and use that as the starting learning rate for your new training.&lt;/p&gt;

&lt;p&gt;Note that if you do end up stopping your training early due to the metrics leveling off, you will have stopped the script before it reached the evaluation section, so if you want the evaluation results you will have to run the script again with only the –do_eval flag, and not the –do_train flag, and –model_name_or_path should be set to the path to the checkpoint you want to evaluate.&lt;/p&gt;

&lt;p&gt;Also note that if you previously ran an evaluation on a checkpoint, and now you want to evaluate that checkpoint using a different data file than you originally used, be sure to delete the any cached dataset files associated with that checkpoint (which will have names like “cached_dev_checkpoint-7000-v2_384”), so that your evaluation runs on the new data file instead of the old cached data.&lt;/p&gt;

&lt;h2 id=&quot;understanding-your-model-outputs&quot;&gt;Understanding Your Model Outputs&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What’s the point in training a model if we can’t understand what it’s telling us? Here is a primer on comprehending model terms.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Understanding some run_squad evaluation metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These metrics allow you to judge how well your model learned the dataset that you gave to it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‘Exact’ - compares answers that the model predicts for the dev set questions with the real answers, returns percentage of answers that model predicted correctly (eg answer spans match exactly) out of total dev set examples&lt;/li&gt;
  &lt;li&gt;‘F1’ - f1 is an accuracy measure that equals 2&lt;em&gt;((precision&lt;/em&gt;recall)/(precision+recall)), where &lt;strong&gt;precision&lt;/strong&gt; is the number of true positives over all positive results that the test returns, and &lt;strong&gt;recall&lt;/strong&gt; is the number of true true positives over everything that should have been identified as positive.
Here, precision and recall are computed by splitting the correct answers and the predicted answers into tokens by word. The number of tokens that appear in both answers is counted. This intersecting token count is divided by the number of tokens in the predicted answer to find the precision, and divided by number of tokens in the real answer to find recall.
The final f1 score returned is the sum of f1 scores for each example in the dev set divided by the total number of examples.&lt;/li&gt;
  &lt;li&gt;‘Total’ - total number of examples in dev set&lt;/li&gt;
  &lt;li&gt;‘HasAns_exact’ - average exact score (computed as above) but only taking the average for all examples in the dev set that have answers (e.g. not impossible)&lt;/li&gt;
  &lt;li&gt;‘HasAns_f1’ - average f1 score over all examples in dev set that have answers&lt;/li&gt;
  &lt;li&gt;‘HasAns_total’ - total # of examples in dev set that have answers&lt;/li&gt;
  &lt;li&gt;‘NoAns_exact’ - average exact score over all examples in dev set that have no answer (e.g. are impossible)
‘NoAns_f1’ - average f1 score over all examples in dev set that have no answer&lt;/li&gt;
  &lt;li&gt;‘NoAns_total’ - total number of examples in dev set that have no answer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Understanding the final model outputs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every checkpoint folder has files containing information about the model, optimizer, tokenizer, scheduler, tokens, and the training arguments. When the model has finished evaluating, it will output a predictions_.json file, a nbest_predections_.json file, and a null_odds_.json if you are using the SQuAD V2 dataset. The predictions file contains the model prediction for each example in the evaluation file, while the nbest_predictions file contains the n best predictions for each example in the evaluation file, where n is by default 20.&lt;/p&gt;

&lt;h2 id=&quot;downloading-your-model-uploading-to-s3-bucket&quot;&gt;Downloading Your Model/ Uploading to S3 Bucket&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Congratulations! You’ve officially trained a ML model for Question Answering! Now that we have our model, where should we store it for future use?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once your model is trained, the last step is to download it from the EC2 instance so that you and others can access it. (Once an EC2 instance is deleted the information on it also is, so be sure to save the model information you want before terminating your instance for good.) You can save all checkpoints that were stored throughout training, or you can choose to save only the best checkpoint based on evaluation metrics (be aware that the best checkpoint is not necessarily the last.)&lt;/p&gt;

&lt;p&gt;To download the checkpoints to your local computer, use the scp command described above for file transfers from SSH. In order to share the model with others, or to upload it to Huggingface for public use, you will need to upload it to an Amazon s3 bucket. 
You can upload locally downloaded files to the s3 bucket using the s3 section of the AWS console.&lt;/p&gt;

&lt;p&gt;Alternatively, you can download the AWS client software onto your EC2 instance and upload the model files directly to a bucket without first downloading to your local computer. Do this by creating an s3 bucket from the s3 web console. Then connect the s3 bucket to your instance using an IAM role and the AWS cli software by following the steps outlined in &lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/&quot;&gt;this post&lt;/a&gt;. Once you have downloaded AWS cli, you can use the command “aws s3 cp filetocopy.txt s3://mybucketpath” to copy files into the bucket. Set permissions for public access using the command flags described &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html#using-s3-commands-managing-objects&quot;&gt;here&lt;/a&gt;, or set permissions directly from the s3 web console.&lt;/p&gt;

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;
&lt;p&gt;https://medium.com/@dearsikandarkhan/files-copying-between-aws-EC2-and-local-d07ed205eefa – file transfer between EC2 and local\
https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04 – adding swap space\
https://medium.com/@arnab.k/how-to-keep-processes-running-after-ending-ssh-session-c836010b26a3 – screen command\
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html – authorizing inbound traffic to instance\
https://github.com/huggingface/transformers#run_squadpy-fine-tuning-on-squad-for-question-answering
https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ – meaning of f1 score\
http://www.kodyaz.com/aws/list-all-amazon-EC2-instances-using-aws-gui-tools.aspx – EC2 instance dashboard image&lt;/p&gt;</content><author><name>Batya Stein &amp; Nicholas Greenspan</name></author><summary type="html">by Batya Stein &amp;amp; Nicholas Greenspan</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/site_files/MLQAthumb.png" /><media:content medium="image" url="http://localhost:4000/site_files/MLQAthumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>